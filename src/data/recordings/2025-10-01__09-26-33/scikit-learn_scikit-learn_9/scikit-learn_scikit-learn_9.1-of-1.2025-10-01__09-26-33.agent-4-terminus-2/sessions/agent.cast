{"version": 2, "width": 160, "height": 40, "timestamp": 1762942281, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[0.002, "i", "asciinema rec --stdin /logs/agent.cast\r"]
[0.004, "o", "asciinema rec --stdin /logs/agent.cast\r\n"]
[0.006, "o", "\u001b[?2004l\r\n\u001b[0;31masciinema: /logs/agent.cast already exists, aborting\u001b[0m\r\n\u001b[0;31masciinema: use --overwrite option if you want to overwrite existing recording\u001b[0m\r\n\u001b[0;31masciinema: use --append option if you want to append to existing recording\u001b[0m\r\n"]
[5.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[5.002, "i", "clear\r"]
[5.004, "o", "clear\r\n"]
[5.006, "o", "\u001b[?2004l\r\n\u001b[H\u001b[J\r\n"]
[10.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[10.002, "i", "ls -la\r"]
[10.004, "o", "ls -la\r\n"]
[10.006, "o", "\u001b[?2004l\r\ntotal 176\r\ndrwxr-xr-x 1 root root  4096 Oct  1 14:47 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:25 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.binder\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.circleci\u001b[0m\r\n-rw-r--r-- 1 root root  1392 Sep 13 19:09 .cirrus.star\r\n-rw-r--r-- 1 root root   999 Sep 13 19:09 .codecov.yml\r\n-rw-r--r-- 1 root root   150 Sep 13 19:09 .coveragerc\r\ndrwxr-xr-x 1 root root  4096 Sep 15 17:27 \u001b[01;34m.git\u001b[0m\r\n-rw-r--r-- 1 root root  1000 Sep 13 19:09 .git-blame-ignore-revs\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34m.github\u001b[0m\r\n-rw-r--r-- 1 root root  2004 Sep 13 19:09 .gitignore\r\n-rw-r--r-- 1 root root  7263 Sep 13 19:08 .mailmap\r\n-rw-r--r-- 1 root root   872 Sep 13 19:09 .pre-commit-config.yaml\r\n-rw-r--r-- 1 root root   645 Sep 13 19:09 CODE_OF_CONDUCT.md\r\n-rw-r--r-- 1 root root  2109 Sep 13 19:09 CONTRIBUTING.md\r\n-rw-r--r-- 1 root root  1532 Sep 13 19:09 COPYING\r\n-rw-r--r-- 1 root root   971 Sep 13 19:09 MANIFEST.in\r\n-rw-r--r-- 1 root root  1491 Sep 13 19:09 Makefile\r\n-rw-r--r-- 1 root root  7604 Sep 13 19:09 README.rst\r\n-rw-r--r-- 1 root root   692 Sep 13 19:09 SECURITY.md\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34masv_benchmarks\u001b[0m\r\n-rw-r--r-- 1 root root 11978 Sep 13 19:09 azure-pipelines.yml\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\ndrwxr-xr-x 4 root root  4096 Sep 15 14:32 \u001b[01;34mbuild\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mbuild_tools\u001b[0m\r\n-rw-r--r-- 1 root root   388 Sep 13 19:09 conftest.py\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mdoc\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mexamples\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[01;34mmaint_tools\u001b[0m\r\n-rw-r--r-- 1 root root  3838 Sep 13 19:09 pyproject.toml\r\ndrwxr-xr-x 1 root root  4096 Sep 15 14:32 \u001b[01;34mscikit_learn.egg-info\u001b[0m\r\n-rw-r--r-- 1 root root  2113 Sep 13 19:09 setup.cfg\r\n-rwxr-xr-x 1 root root 22566 Sep 13 19:09 \u001b[01;32msetup.py\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Sep 15 14:38 \u001b[01;34msklearn\u001b[0m\r\n"]
[15.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[15.002, "i", "git status -sb\r"]
[15.004, "o", "git status -sb\r\n"]
[15.006, "o", "\u001b[?2004l\r\n## \u001b[32mmain\u001b[m\r\n"]
[20.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[20.002, "i", "ls -la benchmarks || ls -la asv_bench || ls -la asv\r"]
[20.004, "o", "ls -la benchmarks || ls -la asv_bench || ls -la asv\r\n"]
[20.006, "o", "\u001b[?2004l\r\ntotal 312\r\ndrwxr-xr-x 1 root root  4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root  4096 Oct  1 14:47 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root    41 Sep 13 19:08 .gitignore\r\n-rw-r--r-- 1 root root  3121 Sep 13 19:08 bench_20newsgroups.py\r\n-rw-r--r-- 1 root root  7338 Sep 13 19:09 bench_covertype.py\r\n-rw-r--r-- 1 root root  1774 Sep 13 19:08 bench_feature_expansions.py\r\n-rw-r--r-- 1 root root  1478 Sep 13 19:09 bench_glm.py\r\n-rw-r--r-- 1 root root  3967 Sep 13 19:09 bench_glmnet.py\r\n-rw-r--r-- 1 root root  9516 Sep 13 19:08 bench_hist_gradient_boosting.py\r\n-rw-r--r-- 1 root root  3525 Sep 13 19:09 bench_hist_gradient_boosting_adult.py\r\n-rw-r--r-- 1 root root  2622 Sep 13 19:08 bench_hist_gradient_boosting_categorical_only.py\r\n-rw-r--r-- 1 root root  4120 Sep 13 19:09 bench_hist_gradient_boosting_higgsboson.py\r\n-rw-r--r-- 1 root root 11031 Sep 13 19:08 bench_hist_gradient_boosting_threading.py\r\n-rw-r--r-- 1 root root  5513 Sep 13 19:09 bench_isolation_forest.py\r\n-rw-r--r-- 1 root root  3274 Sep 13 19:09 bench_isotonic.py\r\n-rw-r--r-- 1 root root  5461 Sep 13 19:09 bench_kernel_pca_solvers_time_vs_n_components.py\r\n-rw-r--r-- 1 root root  5729 Sep 13 19:09 bench_kernel_pca_solvers_time_vs_n_samples.py\r\n-rw-r--r-- 1 root root  3097 Sep 13 19:09 bench_lasso.py\r\n-rw-r--r-- 1 root root  3543 Sep 13 19:09 bench_lof.py\r\n-rw-r--r-- 1 root root  6994 Sep 13 19:09 bench_mnist.py\r\n-rwxr-xr-x 1 root root  6769 Sep 13 19:08 \u001b[01;32mbench_multilabel_metrics.py\u001b[0m\r\n-rw-r--r-- 1 root root  9419 Sep 13 19:08 bench_online_ocsvm.py\r\n-rw-r--r-- 1 root root  4419 Sep 13 19:09 bench_plot_fastkmeans.py\r\n-rw-r--r-- 1 root root  2555 Sep 13 19:08 bench_plot_hierarchical.py\r\n-rw-r--r-- 1 root root  5561 Sep 13 19:08 bench_plot_incremental_pca.py\r\n-rw-r--r-- 1 root root  3909 Sep 13 19:09 bench_plot_lasso_path.py\r\n-rw-r--r-- 1 root root  5719 Sep 13 19:09 bench_plot_neighbors.py\r\n-rw-r--r-- 1 root root 15557 Sep 13 19:09 bench_plot_nmf.py\r\n-rw-r--r-- 1 root root  4415 Sep 13 19:09 bench_plot_omp_lars.py\r\n-rw-r--r-- 1 root root  1236 Sep 13 19:09 bench_plot_parallel_pairwise.py\r\n-rw-r--r-- 1 root root  5995 Sep 13 19:09 bench_plot_polynomial_kernel_approximation.py\r\n-rw-r--r-- 1 root root 18086 Sep 13 19:09 bench_plot_randomized_svd.py\r\n-rw-r--r-- 1 root root  2749 Sep 13 19:09 bench_plot_svd.py\r\n-rw-r--r-- 1 root root  1270 Sep 13 19:08 bench_plot_ward.py\r\n-rw-r--r-- 1 root root  8566 Sep 13 19:09 bench_random_projections.py\r\n-rw-r--r-- 1 root root  7802 Sep 13 19:09 bench_rcv1_logreg_convergence.py\r\n-rw-r--r-- 1 root root 10868 Sep 13 19:09 bench_saga.py\r\n-rw-r--r-- 1 root root  7430 Sep 13 19:09 bench_sample_without_replacement.py\r\n-rw-r--r-- 1 root root  5339 Sep 13 19:09 bench_sgd_regression.py\r\n-rw-r--r-- 1 root root  3357 Sep 13 19:08 bench_sparsify.py\r\n-rw-r--r-- 1 root root  1919 Sep 13 19:09 bench_text_vectorizers.py\r\n-rw-r--r-- 1 root root  3623 Sep 13 19:09 bench_tree.py\r\n-rw-r--r-- 1 root root  6387 Sep 13 19:09 bench_tsne_mnist.py\r\n-rw-r--r-- 1 root root   857 Sep 13 19:08 plot_tsne_mnist.py\r\n"]
[25.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[25.002, "i", "grep -R \"MiniBatchDictionaryLearning\" -n . || true\r"]
[25.004, "o", "grep -R \"MiniBatchDictionaryLearning\" -n . || true\r\n"]
[25.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[K./examples/decomposition/plot_image_denoising.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K118\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./examples/decomposition/plot_image_denoising.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K122\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K167\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# By default, :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` divides the data into\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K172\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kbatch_dict_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K242\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` implements a faster, but less accurate\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K244\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# datasets. Read more in the :ref:`User Guide <\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K>`.\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K255\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K# `\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` estimator on all images. Generally,\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K272\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_dict_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_code_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./examples/decomposition/plot_faces_decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K318\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdict_pos_estimator = decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K592\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K.. _\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K:\r\n\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K597\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K:class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` implements a faster, but less accurate\r\n\u001b[35m\u001b[K./doc/modules/decomposition.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K601\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[KBy default, :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` divides the data into\r\n\u001b[35m\u001b[K./doc/modules/classes.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K324\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K   decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K542\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`\r\n\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K551\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Enhancement| The :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K562\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` serve internal purpose and are\r\n\u001b[35m\u001b[K./doc/whats_new/v1.1.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K585\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K41\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`, :class:`decomposition.SparsePCA`\r\n\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K595\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| Fixed a bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K602\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n\u001b[35m\u001b[K./doc/whats_new/v1.0.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K610\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n\u001b[35m\u001b[K./doc/whats_new/v0.23.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K55\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :func:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K.partial_fit` which should\r\n\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K121\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| The fitted components in :class:`\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` might differ. The\r\n\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K175\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Fix| Fixed a bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` where the\r\n\u001b[35m\u001b[K./doc/whats_new/v1.2.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K533\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`. `n_iter` will be removed\r\n\u001b[35m\u001b[K./doc/whats_new/v1.3.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K311\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- |Efficiency| :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n\u001b[35m\u001b[K./doc/whats_new/v0.22.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K241\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` |Fix|\r\n\u001b[35m\u001b[K./doc/whats_new/v0.22.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K404\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` now take a\r\n\u001b[35m\u001b[K./doc/whats_new/v0.15.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K333\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K- Fixed bug in :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` :\r\n\u001b[35m\u001b[K./doc/whats_new/v0.13.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K220\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`,\r\n\u001b[35m\u001b[K./doc/whats_new/v0.13.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K  :class:`decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K` and\r\n\u001b[35m\u001b[K./doc/computing/scaling_strategies.rst\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K80\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K      + :class:`sklearn.decomposition.\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K`\r\n\u001b[35m\u001b[K./sklearn/utils/estimator_checks.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K680\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        # \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./sklearn/utils/estimator_checks.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K681\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if name == \"\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\":\r\n\u001b[35m\u001b[K./sklearn/decomposition/_nmf.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1981\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : Finds a dictionary that can best be used to represent\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K12\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K,\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K328\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K354\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K407\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K411\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K421\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K451\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K460\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K470\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dico = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K481\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K496\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict1 = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K507\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict2 = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K831\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    dict_learner = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K978\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(batch_size=4, max_iter=10),\r\n\u001b[35m\u001b[K./sklearn/decomposition/tests/test_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1025\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        model = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(batch_size=256, n_iter=2, max_iter=2).fit(X)\r\n\u001b[35m\u001b[K./sklearn/decomposition/_sparse_pca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K20\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom ._dict_learning import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K, dict_learning\r\n\u001b[35m\u001b[K./sklearn/decomposition/_sparse_pca.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K532\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/__init__.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K11\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K,\r\n\u001b[35m\u001b[K./sklearn/decomposition/__init__.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K36\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \"\u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\",\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K858\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate, version of the dictionary\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K884\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \"'n_steps_' attributes of the \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K object instead.\"\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K891\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        est = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1209\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate version\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1405\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K : A faster, less accurate, version of the\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1675\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K: A faster, less accurate, version of the\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K2062\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    >>> from sklearn.decomposition import \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./sklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K2066\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    >>> dict_learner = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\ngrep: ./sklearn/decomposition/__pycache__/_sparse_pca.cpython-310.pyc: binary file matches\r\ngrep: ./sklearn/decomposition/__pycache__/__init__.cpython-310.pyc: binary file matches\r\ngrep: ./sklearn/decomposition/__pycache__/_nmf.cpython-310.pyc: binary file matches\r\ngrep: ./sklearn/decomposition/__pycache__/_dict_learning.cpython-310.pyc: binary file matches\r\n\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import PCA, DictionaryLearning, \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K67\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[KBenchmark(Transformer, Estimator, Benchmark):\r\n\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K69\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    Benchmarks for \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[K./asv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K84\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        estimator = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[30.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[30.002, "i", "grep -R \"asv\" -n . | head -n 50\r"]
[30.004, "o", "grep -R \"asv\" -n . | head -n 50\r\n"]
[30.006, "o", "\u001b[?2004l\r\ngrep: ./sklearn/metrics/cluster/_expected_mutual_info_fast.cpython-310-x86_64-linux-gnu.so: binary file matches\r\n./doc/developers/contributing.rst:1051:`asv benchmarks <https://github.com/airspeed-velocity/asv>`_ to monitor the\r\n./doc/developers/contributing.rst:1054:The corresponding benchmark suite can be found in the `scikit-learn/asv_benchmarks` directory.\r\n./doc/developers/contributing.rst:1056:To use all features of asv, you will need either `conda` or `virtualenv`. For\r\n./doc/developers/contributing.rst:1057:more details please check the `asv installation webpage\r\n./doc/developers/contributing.rst:1058:<https://asv.readthedocs.io/en/latest/installing.html>`_.\r\n./doc/developers/contributing.rst:1060:First of all you need to install the development version of asv:\r\n./doc/developers/contributing.rst:1064:    pip install git+https://github.com/airspeed-velocity/asv\r\n./doc/developers/contributing.rst:1066:and change your directory to `asv_benchmarks/`:\r\n./doc/developers/contributing.rst:1070:  cd asv_benchmarks/\r\n./doc/developers/contributing.rst:1085:  asv continuous -b LogisticRegression upstream/main HEAD\r\n./doc/developers/contributing.rst:1092:  asv continuous -E virtualenv -b LogisticRegression upstream/main HEAD\r\n./doc/developers/contributing.rst:1098:  asv continuous -b linear_model upstream/main HEAD\r\n./doc/developers/contributing.rst:1108:  asv continuous upstream/main HEAD\r\n./doc/developers/contributing.rst:1118:  asv run -b linear_model HEAD^!\r\n./doc/developers/contributing.rst:1125:  asv run --python=same\r\n./doc/developers/contributing.rst:1134:  asv run --python=same --set-commit-hash=<commit hash>\r\n./doc/developers/contributing.rst:1141:  asv show\r\n./doc/developers/contributing.rst:1147:  asv show <commit hash>\r\n./doc/developers/contributing.rst:1156:More information on how to write a benchmark and how to use asv can be found in\r\n./doc/developers/contributing.rst:1157:the `asv documentation <https://asv.readthedocs.io/en/latest/index.html>`_.\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:7:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h\",\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:8:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h\",\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:9:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h\",\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:10:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h\",\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:11:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h\"\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:18:            \"/opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/core/include\"\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1134:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":688\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1143:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":689\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1152:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":690\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1161:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":691\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1170:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":695\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1179:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":696\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1188:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":697\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1197:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":698\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1206:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":702\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1215:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":703\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1224:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":712\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1233:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":713\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1242:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":715\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1251:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":716\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1260:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":718\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1269:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":719\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1278:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":721\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1287:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":722\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1296:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":723\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1335:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":725\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1344:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":726\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1353:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":727\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:1362:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":729\r\n./sklearn/metrics/cluster/_expected_mutual_info_fast.c:3788:/* \"../../opt/conda/envs/asv_3.10/lib/python3.10/site-packages/numpy/__init__.pxd\":731\r\n"]
[35.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[35.002, "i", "fd -a asv.conf.json || find . -maxdepth 3 -name 'asv.conf.json'\r"]
[35.004, "o", "fd -a asv.conf.json || find . -maxdepth 3 -name 'asv.conf.json'\r\n"]
[35.006, "o", "\u001b[?2004l\r\n-bash: fd: command not found\r\n./asv_benchmarks/asv.conf.json\r\n"]
[40.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[40.002, "i", "cat asv_benchmarks.txt 2>/dev/null || true\r"]
[40.004, "o", "cat asv_benchmarks.txt 2>/dev/null || true\r\n"]
[40.006, "o", "\u001b[?2004l\r\n"]
[45.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[45.002, "i", "rg -n \"class .*MiniBatchDictionaryLearningBenchmark|MiniBatchDictionaryLearningBenchmark\" -S benchmarks 2>/dev/null\r"]
[45.004, "o", "rg -n \"class .*MiniBatchDictionaryLearningBenchmark|MiniBatchDictionaryLearningBenchmark\" -S benchmarks 2>/dev/null\r\n"]
[45.006, "o", "l || rg -n \"MiniBatchDictionaryLearningBenchmark\" -S .\r\n\u001b[?2004l\r\n-bash: rg: command not found\r\n"]
[50.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[50.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[50.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[50.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[55.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[55.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[55.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[55.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[60.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[60.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[60.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[60.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[65.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[65.002, "i", "ls -la asv_benchmarks\r"]
[65.004, "o", "ls -la asv_benchmarks\r\n"]
[65.006, "o", "\u001b[?2004l\r\ntotal 28\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Oct  1 14:47 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root   66 Sep 13 19:08 .gitignore\r\n-rw-r--r-- 1 root root 6754 Sep 13 19:09 asv.conf.json\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34mbenchmarks\u001b[0m\r\n"]
[70.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[70.002, "i", "ls -la asv_benchmarks/benchmarks\r"]
[70.004, "o", "ls -la asv_benchmarks/benchmarks\r\n"]
[70.006, "o", "\u001b[?2004l\r\ntotal 76\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 root root 4096 Sep 13 19:09 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root   49 Sep 13 19:08 __init__.py\r\n-rw-r--r-- 1 root root 2925 Sep 13 19:08 cluster.py\r\n-rw-r--r-- 1 root root 7341 Sep 13 19:09 common.py\r\n-rw-r--r-- 1 root root 1544 Sep 13 19:09 config.json\r\n-rw-r--r-- 1 root root 5176 Sep 13 19:09 datasets.py\r\n-rw-r--r-- 1 root root 2406 Sep 13 19:08 decomposition.py\r\n-rw-r--r-- 1 root root 2995 Sep 13 19:08 ensemble.py\r\n-rw-r--r-- 1 root root 6648 Sep 13 19:09 linear_model.py\r\n-rw-r--r-- 1 root root  820 Sep 13 19:08 manifold.py\r\n-rw-r--r-- 1 root root 1363 Sep 13 19:08 metrics.py\r\n-rw-r--r-- 1 root root 2371 Sep 13 19:08 model_selection.py\r\n-rw-r--r-- 1 root root 1140 Sep 13 19:08 neighbors.py\r\n-rw-r--r-- 1 root root  762 Sep 13 19:08 svm.py\r\n-rw-r--r-- 1 root root 1310 Sep 13 19:08 utils.py\r\n"]
[75.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[75.002, "i", "grep -R \"MiniBatchDictionaryLearningBenchmark\" -n asv_benchmarks/benchmarks || true\r"]
[75.004, "o", "grep -R \"MiniBatchDictionaryLearningBenchmark\" -n asv_benchmarks/benchmarks || true\r\n"]
[75.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K67\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearningBenchmark\u001b[m\u001b[K(Transformer, Estimator, Benchmark):\r\n"]
[80.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[80.002, "i", "grep -R \"DictionaryLearningBenchmark\\|MiniBatchDictionaryLearning\" -n asv_benchmarks/benchmarks || true\r"]
[80.004, "o", "grep -R \"DictionaryLearningBenchmark\\|MiniBatchDictionaryLearning\" -n asv_benchmarks/benchmarks || true\r\n"]
[80.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kfrom sklearn.decomposition import PCA, DictionaryLearning, \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K33\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KDictionaryLearningBenchmark\u001b[m\u001b[K(Transformer, Estimator, Benchmark):\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K67\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kclass \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[KBenchmark(Transformer, Estimator, Benchmark):\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K69\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    Benchmarks for \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K\r\n\u001b[35m\u001b[Kasv_benchmarks/benchmarks/decomposition.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K84\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        estimator = \u001b[01;31m\u001b[KMiniBatchDictionaryLearning\u001b[m\u001b[K(\r\n"]
[85.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[85.002, "i", "asv run --config asv_benchmarks/asv.conf.json --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBench\r"]
[85.004, "o", "asv run --config asv_benchmarks/asv.conf.json --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBench\r\n"]
[85.006, "o", "hmark.*\" --show-stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_benchmarks/asv.conf.json not found.\u001b[0m\r\n"]
[90.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[90.002, "i", "asv show --config asv_benchmarks/asv.conf.json --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenc\r"]
[90.004, "o", "asv show --config asv_benchmarks/asv.conf.json --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenc\r\n"]
[90.006, "o", "chmark.*\"\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_benchmarks/asv.conf.json not found.\u001b[0m\r\n"]
[95.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[95.002, "i", "asv profile --config asv_benchmarks/asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.\r"]
[95.004, "o", "asv profile --config asv_benchmarks/asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.\r\n"]
[95.006, "o", ".time_fit\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;31mConfig file asv_benchmarks/asv.conf.json not found.\u001b[0m\r\n"]
[100.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[100.002, "i", "grep -R \"class MiniBatchDictionaryLearning\\|def .*MiniBatchDictionaryLearning\" -n sklearn/decomposition || true\r"]
[100.004, "o", "grep -R \"class MiniBatchDictionaryLearning\\|def .*MiniBatchDictionaryLearning\" -n sklearn/decomposition || true\r\n"]
[100.006, "o", "\u001b[?2004l\r\n\u001b[35m\u001b[Ksklearn/decomposition/_dict_learning.py\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[105.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[105.002, "i", "sed -n '1,220p' sklearn/decomposition/_dict_learning.py\r"]
[105.004, "o", "sed -n '1,220p' sklearn/decomposition/_dict_learning.py\r\n"]
[105.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n"]
[110.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[110.002, "i", "sed -n '1,200p' asv_benchmarks/asv.conf.json\r"]
[110.004, "o", "sed -n '1,200p' asv_benchmarks/asv.conf.json\r\n"]
[110.006, "o", "\u001b[?2004l\r\n{\r\n    // The version of the config file format.  Do not change, unless\r\n    // you know what you are doing.\r\n    \"version\": 1,\r\n\r\n    // The name of the project being benchmarked\r\n    \"project\": \"scikit-learn\",\r\n\r\n    // The project's homepage\r\n    \"project_url\": \"scikit-learn.org/\",\r\n\r\n    // The URL or local path of the source code repository for the\r\n    // project being benchmarked\r\n    \"repo\": \"..\",\r\n\r\n    // The Python project's subdirectory in your repo.  If missing or\r\n    // the empty string, the project is assumed to be located at the root\r\n    // of the repository.\r\n    // \"repo_subdir\": \"\",\r\n\r\n    // Customizable commands for building, installing, and\r\n    // uninstalling the project. See asv.conf.json documentation.\r\n    //\r\n    // \"install_command\": [\"python -mpip install {wheel_file}\"],\r\n    // \"uninstall_command\": [\"return-code=any python -mpip uninstall -y {project}\"],\r\n    // \"build_command\": [\r\n    //     \"python setup.py build\",\r\n    //     \"PIP_NO_BUILD_ISOLATION=false python -mpip wheel --no-deps --no-index -w {build_cache_dir} {build_dir}\"\r\n    // ],\r\n\r\n    // List of branches to benchmark. If not provided, defaults to \"master\r\n    // (for git) or \"default\" (for mercurial).\r\n    \"branches\": [\"main\"],\r\n    // \"branches\": [\"default\"],    // for mercurial\r\n\r\n    // The DVCS being used.  If not set, it will be automatically\r\n    // determined from \"repo\" by looking at the protocol in the URL\r\n    // (if remote), or by looking for special directories, such as\r\n    // \".git\" (if local).\r\n    // \"dvcs\": \"git\",\r\n\r\n    // The tool to use to create environments.  May be \"conda\",\r\n    // \"virtualenv\" or other value depending on the plugins in use.\r\n    // If missing or the empty string, the tool will be automatically\r\n    // determined by looking for tools on the PATH environment\r\n    // variable.\r\n    \"environment_type\": \"conda\",\r\n\r\n    // timeout in seconds for installing any dependencies in environment\r\n    // defaults to 10 min\r\n    //\"install_timeout\": 600,\r\n\r\n    // the base URL to show a commit for the project.\r\n    \"show_commit_url\": \"https://github.com/scikit-learn/scikit-learn/commit/\",\r\n\r\n    // The Pythons you'd like to test against. If not provided, defaults\r\n    // to the current version of Python used to run `asv`.\r\n    // \"pythons\": [\"3.6\"],\r\n\r\n    // The list of conda channel names to be searched for benchmark\r\n    // dependency packages in the specified order\r\n    // \"conda_channels\": [\"conda-forge\", \"defaults\"]\r\n\r\n    // The matrix of dependencies to test. Each key is the name of a\r\n    // package (in PyPI) and the values are version numbers. An empty\r\n    // list or empty string indicates to just test against the default\r\n    // (latest) version. null indicates that the package is to not be\r\n    // installed. If the package to be tested is only available from\r\n    // PyPi, and the 'environment_type' is conda, then you can preface\r\n    // the package name by 'pip+', and the package will be installed via\r\n    // pip (with all the conda available packages installed first,\r\n    // followed by the pip installed packages).\r\n    //\r\n    \"matrix\": {\r\n        \"numpy\": [],\r\n        \"scipy\": [],\r\n        \"cython\": [],\r\n        \"joblib\": [],\r\n        \"threadpoolctl\": [],\r\n        \"pandas\": []\r\n    },\r\n\r\n    // Combinations of libraries/python versions can be excluded/included\r\n    // from the set to test. Each entry is a dictionary containing additional\r\n    // key-value pairs to include/exclude.\r\n    //\r\n    // An exclude entry excludes entries where all values match. The\r\n    // values are regexps that should match the whole string.\r\n    //\r\n    // An include entry adds an environment. Only the packages listed\r\n    // are installed. The 'python' key is required. The exclude rules\r\n    // do not apply to includes.\r\n    //\r\n    // In addition to package names, the following keys are available:\r\n    //\r\n    // - python\r\n    //     Python version, as in the *pythons* variable above.\r\n    // - environment_type\r\n    //     Environment type, as above.\r\n    // - sys_platform\r\n    //     Platform, as in sys.platform. Possible values for the common\r\n    //     cases: 'linux2', 'win32', 'cygwin', 'darwin'.\r\n    //\r\n    // \"exclude\": [\r\n    //     {\"python\": \"3.2\", \"sys_platform\": \"win32\"}, // skip py3.2 on windows\r\n    //     {\"environment_type\": \"conda\", \"six\": null}, // don't run without six on conda\r\n    // ],\r\n    //\r\n    // \"include\": [\r\n    //     // additional env for python2.7\r\n    //     {\"python\": \"2.7\", \"numpy\": \"1.8\"},\r\n    //     // additional env if run on windows+conda\r\n    //     {\"platform\": \"win32\", \"environment_type\": \"conda\", \"python\": \"2.7\", \"libpython\": \"\"},\r\n    // ],\r\n\r\n    // The directory (relative to the current directory) that benchmarks are\r\n    // stored in.  If not provided, defaults to \"benchmarks\"\r\n    // \"benchmark_dir\": \"benchmarks\",\r\n\r\n    // The directory (relative to the current directory) to cache the Python\r\n    // environments in.  If not provided, defaults to \"env\"\r\n    // \"env_dir\": \"env\",\r\n\r\n    // The directory (relative to the current directory) that raw benchmark\r\n    // results are stored in.  If not provided, defaults to \"results\".\r\n    // \"results_dir\": \"results\",\r\n\r\n    // The directory (relative to the current directory) that the html tree\r\n    // should be written to.  If not provided, defaults to \"html\".\r\n    // \"html_dir\": \"html\",\r\n\r\n    // The number of characters to retain in the commit hashes.\r\n    // \"hash_length\": 8,\r\n\r\n    // `asv` will cache results of the recent builds in each\r\n    // environment, making them faster to install next time.  This is\r\n    // the number of builds to keep, per environment.\r\n    // \"build_cache_size\": 2,\r\n\r\n    // The commits after which the regression search in `asv publish`\r\n    // should start looking for regressions. Dictionary whose keys are\r\n    // regexps matching to benchmark names, and values corresponding to\r\n    // the commit (exclusive) after which to start looking for\r\n    // regressions.  The default is to start from the first commit\r\n    // with results. If the commit is `null`, regression detection is\r\n    // skipped for the matching benchmark.\r\n    //\r\n    // \"regressions_first_commits\": {\r\n    //    \"some_benchmark\": \"352cdf\",  // Consider regressions only after this commit\r\n    //    \"another_benchmark\": null,   // Skip regression detection altogether\r\n    // },\r\n\r\n    // The thresholds for relative change in results, after which `asv\r\n    // publish` starts reporting regressions. Dictionary of the same\r\n    // form as in ``regressions_first_commits``, with values\r\n    // indicating the thresholds.  If multiple entries match, the\r\n    // maximum is taken. If no entry matches, the default is 5%.\r\n    //\r\n    // \"regressions_thresholds\": {\r\n    //    \"some_benchmark\": 0.01,     // Threshold of 1%\r\n    //    \"another_benchmark\": 0.5,   // Threshold of 50%\r\n    // },\r\n}\r\n"]
[115.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[115.002, "i", "sed -n '1,200p' asv_benchmarks/benchmarks/decomposition.py\r"]
[115.004, "o", "sed -n '1,200p' asv_benchmarks/benchmarks/decomposition.py\r\n"]
[115.006, "o", "\u001b[?2004l\r\nfrom sklearn.decomposition import PCA, DictionaryLearning, MiniBatchDictionaryLearning\r\n\r\nfrom .common import Benchmark, Estimator, Transformer\r\nfrom .datasets import _mnist_dataset, _olivetti_faces_dataset\r\nfrom .utils import make_dict_learning_scorers, make_pca_scorers\r\n\r\n\r\nclass PCABenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for PCA.\r\n    \"\"\"\r\n\r\n    param_names = [\"svd_solver\"]\r\n    params = ([\"full\", \"arpack\", \"randomized\"],)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _mnist_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        (svd_solver,) = params\r\n\r\n        estimator = PCA(n_components=32, svd_solver=svd_solver, random_state=0)\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_pca_scorers(self)\r\n\r\n\r\nclass DictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for DictionaryLearning.\r\n    \"\"\"\r\n\r\n    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _olivetti_faces_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        fit_algorithm, n_jobs = params\r\n\r\n        estimator = DictionaryLearning(\r\n            n_components=15,\r\n            fit_algorithm=fit_algorithm,\r\n            alpha=0.1,\r\n            transform_alpha=1,\r\n            max_iter=20,\r\n            tol=1e-16,\r\n            random_state=0,\r\n            n_jobs=n_jobs,\r\n        )\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_dict_learning_scorers(self)\r\n\r\n\r\nclass MiniBatchDictionaryLearningBenchmark(Transformer, Estimator, Benchmark):\r\n    \"\"\"\r\n    Benchmarks for MiniBatchDictionaryLearning\r\n    \"\"\"\r\n\r\n    param_names = [\"fit_algorithm\", \"n_jobs\"]\r\n    params = ([\"lars\", \"cd\"], Benchmark.n_jobs_vals)\r\n\r\n    def setup_cache(self):\r\n        super().setup_cache()\r\n\r\n    def make_data(self, params):\r\n        return _olivetti_faces_dataset()\r\n\r\n    def make_estimator(self, params):\r\n        fit_algorithm, n_jobs = params\r\n\r\n        estimator = MiniBatchDictionaryLearning(\r\n            n_components=15,\r\n            fit_algorithm=fit_algorithm,\r\n            alpha=0.1,\r\n            batch_size=3,\r\n            random_state=0,\r\n            n_jobs=n_jobs,\r\n        )\r\n\r\n        return estimator\r\n\r\n    def make_scorers(self):\r\n        make_dict_learning_scorers(self)\r\n"]
[120.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[120.002, "i", "pwd\r"]
[120.004, "o", "pwd\r\n"]
[120.006, "o", "\u001b[?2004l\r\n/workspace/repo\r\n"]
[125.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[125.002, "i", "ls -l asv_benchmarks/asv.conf.json\r"]
[125.004, "o", "ls -l asv_benchmarks/asv.conf.json\r\n"]
[125.006, "o", "\u001b[?2004l\r\n-rw-r--r-- 1 root root 6754 Sep 13 19:09 asv_benchmarks/asv.conf.json\r\n"]
[130.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[130.002, "i", "cd asv_benchmarks\r"]
[130.004, "o", "cd asv_benchmarks\r\n"]
[130.006, "o", "\u001b[?2004l\r\n"]
[135.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[135.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[135.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[135.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars        94M  \r\n                    cd        93.4M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       84.5M \r\n                    cd        84.3M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\nasv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\ncd ..\r\nsed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r\n                                                                                     ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n             --                 n_jobs   \r\n             --------------- ------------\r\n              fit_algorithm       1      \r\n             =============== ============\r\n                   lars       6.73\u00b10.04s \r\n                    cd        1.64\u00b10.03s \r\n             =============== ============\r\n\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       161\u00b11ms \r\n                    cd        166\u00b12ms \r\n             =============== =========\r\n\r\n[66.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n             --                      n_jobs        \r\n             --------------- ----------------------\r\n              fit_algorithm            1           \r\n             =============== ======================\r\n                   lars       -0.07507291436195374 \r\n                    cd        -0.07509346306324005 \r\n             =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n              --                      n_jobs        \r\n              --------------- ----------------------\r\n               fit_algorithm            1           \r\n              =============== ======================\r\n                    lars       -0.07244297116994858 \r\n                     cd        -0.07244537025690079 \r\n              =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n\r\n"]
[140.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[140.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[140.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[140.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[145.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[145.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[145.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[145.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[150.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[150.002, "i", "cd ..\r"]
[150.004, "o", "cd ..\r\n"]
[150.006, "o", "\u001b[?2004l\r\n"]
[155.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[155.002, "i", "sed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r"]
[155.004, "o", "sed -n '1800,2100p' sklearn/decomposition/_dict_learning.py\r\n"]
[155.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n"]
[160.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[160.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r"]
[160.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-stderr\r\n"]
[160.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[165.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[165.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[165.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[165.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[170.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[170.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[170.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[170.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;31mNo `asv.conf` file found for valid extensions: ['.json', '.jsonc'].\u001b[0m\r\n"]
[175.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[175.002, "i", "cd ..\r"]
[175.004, "o", "cd ..\r\n"]
[175.006, "o", "\u001b[?2004l\r\n"]
[180.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[180.002, "i", "grep -n \"def dict_learning_online\\|class MiniBatchDictionaryLearning\\|def _mini_batch_dictionary_learning\" -n sklearn/de\r"]
[180.004, "o", "grep -n \"def dict_learning_online\\|class MiniBatchDictionaryLearning\\|def _mini_batch_dictionary_learning\" -n sklearn/de\r\n"]
[180.006, "o", "ecomposition/_dict_learning.py || true\r\n\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[185.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[185.002, "i", "sed -n '1500,2200p' sklearn/decomposition/_dict_learning.py\r"]
[185.004, "o", "sed -n '1500,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[185.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[190.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace# "]
[190.002, "i", "cd /workspace/repo\r"]
[190.004, "o", "cd /workspace/repo\r\n"]
[190.006, "o", "\u001b[?2004l\r\n"]
[195.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[195.002, "i", "cd asv_benchmarks\r"]
[195.004, "o", "cd asv_benchmarks\r\n"]
[195.006, "o", "\u001b[?2004l\r\n"]
[200.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[200.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[200.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[200.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[205.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[205.002, "i", "cd ..\r"]
[205.004, "o", "cd ..\r\n"]
[205.006, "o", "\u001b[?2004l\r\n"]
[210.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[210.002, "i", "grep -n \"def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionaryLearning\" sklearn/\r"]
[210.004, "o", "grep -n \"def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionaryLearning\" sklearn/\r\n"]
[210.006, "o", "/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[215.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[215.002, "i", "sed -n '1,320p' sklearn/decomposition/_dict_learning.py\r"]
[215.004, "o", "sed -n '1,320p' sklearn/decomposition/_dict_learning.py\r\n"]
[215.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n"]
[220.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[220.002, "i", "sed -n '320,900p' sklearn/decomposition/_dict_learning.py\r"]
[220.004, "o", "sed -n '320,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[220.006, "o", "\u001b[?2004l\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n"]
[225.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[225.002, "i", "sed -n '900,1600p' sklearn/decomposition/_dict_learning.py\r"]
[225.004, "o", "sed -n '900,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[225.006, "o", "\u001b[?2004l\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[230.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[230.002, "i", "sed -n '1600,2200p' sklearn/decomposition/_dict_learning.py\r"]
[230.004, "o", "sed -n '1600,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[230.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n"]
[235.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[235.002, "i", "cd asv_benchmarks\r"]
[235.004, "o", "cd asv_benchmarks\r\n"]
[235.006, "o", "\u001b[?2004l\r\n"]
[240.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[240.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[240.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[240.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[245.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[245.002, "i", "cd ..\r"]
[245.004, "o", "cd ..\r\n"]
[245.006, "o", "\u001b[?2004l\r\n"]
[250.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[250.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionary\r"]
[250.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\\|class MiniBatchDictionary\r\n"]
[250.006, "o", "yLearning\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K458\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _update_dict\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n"]
[255.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[255.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[255.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[255.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[260.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[260.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[260.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[260.006, "o", "\u001b[?2004l\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[265.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[265.002, "i", "cd asv_benchmarks\r"]
[265.004, "o", "cd asv_benchmarks\r\n"]
[265.006, "o", "\u001b[?2004l\r\n"]
[270.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[270.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[270.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[270.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[275.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[275.002, "i", "cd ..\r"]
[275.004, "o", "cd ..\r\n"]
[275.006, "o", "\u001b[?2004l\r\n"]
[280.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[280.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r"]
[280.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r\n"]
[280.006, "o", "naryLearning\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[285.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[285.002, "i", "sed -n '1100,1800p' sklearn/decomposition/_dict_learning.py\r"]
[285.004, "o", "sed -n '1100,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[285.006, "o", "\u001b[?2004l\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[290.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[290.002, "i", "sed -n '1800,2400p' sklearn/decomposition/_dict_learning.py\r"]
[290.004, "o", "sed -n '1800,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[290.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[295.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[295.002, "i", "cd asv_benchmarks\r"]
[295.004, "o", "cd asv_benchmarks\r\n"]
[295.006, "o", "\u001b[?2004l\r\n"]
[300.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[300.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[300.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[300.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[305.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[305.002, "i", "cd ..\r"]
[305.004, "o", "cd ..\r\n"]
[305.006, "o", "\u001b[?2004l\r\n"]
[310.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[310.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r"]
[310.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|class MiniBatchDiction\r\n"]
[310.006, "o", "naryLearning\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[315.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[315.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[315.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[315.006, "o", "\u001b[?2004l\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[320.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[320.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[320.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[320.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[325.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[325.002, "i", "cd asv_benchmarks\r"]
[325.004, "o", "cd asv_benchmarks\r\n"]
[325.006, "o", "\u001b[?2004l\r\n"]
[330.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[330.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[330.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[330.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[335.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[335.002, "i", "cd ..\r"]
[335.004, "o", "cd ..\r\n"]
[335.006, "o", "\u001b[?2004l\r\n"]
[340.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[340.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/\r"]
[340.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/\r\n"]
[340.006, "o", "/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[345.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[345.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[345.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[345.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[350.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[350.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[350.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[350.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[355.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[355.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[355.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[355.006, "o", "\u001b[?2004l\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[360.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[360.002, "i", "cd asv_benchmarks\r"]
[360.004, "o", "cd asv_benchmarks\r\n"]
[360.006, "o", "\u001b[?2004l\r\n"]
[365.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[365.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[365.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[365.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[370.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[370.002, "i", "cd ..\r"]
[370.004, "o", "cd ..\r\n"]
[370.006, "o", "\u001b[?2004l\r\n"]
[375.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[375.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _block_coordinate_\r"]
[375.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _block_coordinate_\r\n"]
[375.006, "o", "_descent\\|class MiniBatchDictionaryLearning\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K1867\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kclass MiniBatchDictionaryLearning\u001b[m\u001b[K(_BaseSparseCoding, BaseEstimator):\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[380.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[380.002, "i", "sed -n '1,1000p' sklearn/decomposition/_dict_learning.py\r"]
[380.004, "o", "sed -n '1,1000p' sklearn/decomposition/_dict_learning.py\r\n"]
[380.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n"]
[385.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[385.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[385.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[385.006, "o", "\u001b[?2004l\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[390.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[390.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[390.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[390.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[395.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[395.002, "i", "cd asv_benchmarks\r"]
[395.004, "o", "cd asv_benchmarks\r\n"]
[395.006, "o", "\u001b[?2004l\r\n"]
[400.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[400.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[400.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[400.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[405.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[405.002, "i", "cd ..\r"]
[405.004, "o", "cd ..\r\n"]
[405.006, "o", "\u001b[?2004l\r\n"]
[410.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[410.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[410.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[410.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n\u001b[32m\u001b[K2479\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[415.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[415.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/_di\r"]
[415.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _mini_batch_dictionary_learning\" sklearn/decomposition/_di\r\n"]
[415.006, "o", "ict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K458\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _update_dict\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n"]
[420.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[420.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[420.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[420.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[425.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[425.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[425.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[425.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[430.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[430.002, "i", "sed -n '2000,3000p' sklearn/decomposition/_dict_learning.py\r"]
[430.004, "o", "sed -n '2000,3000p' sklearn/decomposition/_dict_learning.py\r\n"]
[430.006, "o", "\u001b[?2004l\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[435.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[435.002, "i", "cd asv_benchmarks\r"]
[435.004, "o", "cd asv_benchmarks\r\n"]
[435.006, "o", "\u001b[?2004l\r\n"]
[440.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[440.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[440.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[440.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[445.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[445.002, "i", "cd ..\r"]
[445.004, "o", "cd ..\r\n"]
[445.006, "o", "\u001b[?2004l\r\n"]
[450.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[450.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r"]
[450.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r\n"]
[450.006, "o", "ecomputed\\|gram=\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K42\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n\u001b[32m\u001b[K231\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n\u001b[32m\u001b[K364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n\u001b[32m\u001b[K382\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[KNone,\r\n\u001b[32m\u001b[K419\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n\u001b[32m\u001b[K441\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kgram=\u001b[m\u001b[Kgram,\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[455.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[455.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[455.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[455.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n"]
[460.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[460.002, "i", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r"]
[460.004, "o", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[460.006, "o", "\u001b[?2004l\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[465.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[465.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[465.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[465.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[470.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[470.002, "i", "cd asv_benchmarks\r"]
[470.004, "o", "cd asv_benchmarks\r\n"]
[470.006, "o", "\u001b[?2004l\r\n"]
[475.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[475.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[475.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[475.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[480.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[480.002, "i", "cd ..\r"]
[480.004, "o", "cd ..\r\n"]
[480.006, "o", "\u001b[?2004l\r\n"]
[485.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[485.002, "i", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r"]
[485.004, "o", "grep -n \"def _minibatch_step\\|def dict_learning_online\\|def _mini_batch_dictionary_learning\\|def _sparse_encode_pre\r\n"]
[485.006, "o", "ecomputed\" sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef _sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[490.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[490.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[490.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[490.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[495.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[495.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[495.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[495.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[500.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[500.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[500.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[500.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[505.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[505.002, "i", "cd asv_benchmarks\r"]
[505.004, "o", "cd asv_benchmarks\r\n"]
[505.006, "o", "\u001b[?2004l\r\n"]
[510.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[510.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[510.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[510.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[515.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[515.002, "i", "cd ..\r"]
[515.004, "o", "cd ..\r\n"]
[515.006, "o", "\u001b[?2004l\r\n"]
[520.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[520.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|def dict_learning_online\" sklearn/decomposition/_dict_lea\r"]
[520.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|def dict_learning_online\" sklearn/decomposition/_dict_lea\r\n"]
[520.006, "o", "arning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n\u001b[32m\u001b[K664\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K\u001b[01;31m\u001b[Kdef dict_learning_online\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[525.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[525.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[525.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[525.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n"]
[530.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[530.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[530.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[530.006, "o", "\u001b[?2004l\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[535.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[535.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[535.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[535.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[540.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[540.002, "i", "cd asv_benchmarks\r"]
[540.004, "o", "cd asv_benchmarks\r\n"]
[540.006, "o", "\u001b[?2004l\r\n"]
[545.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[545.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[545.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[545.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[550.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[550.002, "i", "cd ..\r"]
[550.004, "o", "cd ..\r\n"]
[550.006, "o", "\u001b[?2004l\r\n"]
[555.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[555.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[555.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[555.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n\u001b[32m\u001b[K2479\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[560.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[560.002, "i", "grep -n \"_sparse_encode_precomputed\\|copy_cov=\" sklearn/decomposition/_dict_learning.py\r"]
[560.004, "o", "grep -n \"_sparse_encode_precomputed\\|copy_cov=\" sklearn/decomposition/_dict_learning.py\r\n"]
[560.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K46\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n\u001b[32m\u001b[K236\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n\u001b[32m\u001b[K369\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[Kcopy_cov,\r\n\u001b[32m\u001b[K387\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[KTrue,\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K423\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[Kcopy_cov,\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n\u001b[32m\u001b[K445\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kcopy_cov=\u001b[m\u001b[Kcopy_cov,\r\n"]
[565.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[565.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[565.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[565.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n"]
[570.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[570.002, "i", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r"]
[570.004, "o", "sed -n '600,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[570.006, "o", "\u001b[?2004l\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[575.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[575.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[575.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[575.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[580.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[580.002, "i", "cd asv_benchmarks\r"]
[580.004, "o", "cd asv_benchmarks\r\n"]
[580.006, "o", "\u001b[?2004l\r\n"]
[585.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[585.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[585.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[585.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[590.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[590.002, "i", "cd ..\r"]
[590.004, "o", "cd ..\r\n"]
[590.006, "o", "\u001b[?2004l\r\n"]
[595.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[595.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r"]
[595.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r\n"]
[595.006, "o", "ng.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[600.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[600.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[600.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[600.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[605.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[605.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[605.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[605.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[610.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[610.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[610.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[610.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[615.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[615.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[615.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[615.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[620.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[620.002, "i", "cd asv_benchmarks\r"]
[620.004, "o", "cd asv_benchmarks\r\n"]
[620.006, "o", "\u001b[?2004l\r\n"]
[625.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[625.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[625.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[625.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[630.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[630.002, "i", "cd ..\r"]
[630.004, "o", "cd ..\r\n"]
[630.006, "o", "\u001b[?2004l\r\n"]
[635.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[635.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[635.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[635.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[640.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[640.002, "i", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[640.004, "o", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[640.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K412\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        copy_\u001b[01;31m\u001b[Kcov = \u001b[m\u001b[KFalse\r\n\u001b[32m\u001b[K413\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kcov = \u001b[m\u001b[Knp.dot(dictionary, X.T)\r\n"]
[645.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[645.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[645.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[645.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[650.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[650.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[650.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[650.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[655.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[655.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[655.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[655.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[660.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[660.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[660.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[660.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[665.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[665.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[665.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[665.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[670.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[670.002, "i", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[670.004, "o", "grep -n \"cov\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[670.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K412\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        copy_\u001b[01;31m\u001b[Kcov = \u001b[m\u001b[KFalse\r\n\u001b[32m\u001b[K413\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kcov = \u001b[m\u001b[Knp.dot(dictionary, X.T)\r\n"]
[675.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[675.002, "i", "grep -n \"gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r"]
[675.004, "o", "grep -n \"gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py\r\n"]
[675.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K409\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kgram = \u001b[m\u001b[Knp.dot(dictionary, dictionary.T)\r\n"]
[680.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[680.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[680.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[680.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n"]
[685.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[685.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[685.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[685.006, "o", "\u001b[?2004l\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[690.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[690.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[690.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[690.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[695.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[695.002, "i", "cd asv_benchmarks\r"]
[695.004, "o", "cd asv_benchmarks\r\n"]
[695.006, "o", "\u001b[?2004l\r\n"]
[700.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[700.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r"]
[700.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n"]
[700.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[705.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[705.002, "i", "cd ..\r"]
[705.004, "o", "cd ..\r\n"]
[705.006, "o", "\u001b[?2004l\r\n"]
[710.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[710.002, "i", "cd asv_benchmarks\r"]
[710.004, "o", "cd asv_benchmarks\r\n"]
[710.006, "o", "\u001b[?2004l\r\n"]
[715.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[715.002, "i", "asv run --python=same --bench \".*MiniBatchDictionaryLearningBenchmark.*\" --quick\r"]
[715.004, "o", "asv run --python=same --bench \".*MiniBatchDictionaryLearningBenchmark.*\" --quick\r\n"]
[715.006, "o", "\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       93.7M \r\n                    cd        93.3M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       84.2M \r\n                    cd         84M  \r\n             =============== =======\r\n\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv dev -l | sed -n '1,200p'\r\nasv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r\n                                                                                     ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       6.62\u00b10s \r\n                    cd        1.56\u00b10s \r\n             =============== =========\r\n\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       157\u00b10ms \r\n                    cd        157\u00b10ms \r\n             =============== =========\r\n\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n             --                      n_jobs        \r\n             --------------- ----------------------\r\n              fit_algorithm            1           \r\n             =============== ======================\r\n                   lars       -0.07507291436195374 \r\n                    cd        -0.07509346306324005 \r\n             =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n              --                      n_jobs        \r\n              --------------- ----------------------\r\n               fit_algorithm            1           \r\n              =============== ======================\r\n                    lars       -0.07244297116994858 \r\n                     cd        -0.07244537025690079 \r\n              =============== ======================\r\n\r\n"]
[720.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[720.002, "i", "asv dev -l | sed -n '1,200p'\r"]
[720.004, "o", "asv dev -l | sed -n '1,200p'\r\n"]
[720.006, "o", "\u001b[?2004l\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[725.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[725.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r"]
[725.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || true\r\n"]
[725.006, "o", "\u001b[?2004l\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[730.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[730.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[730.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[730.006, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[735.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[735.002, "i", "cd ..\r"]
[735.004, "o", "cd ..\r\n"]
[735.006, "o", "\u001b[?2004l\r\n"]
[740.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[740.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r"]
[740.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learnin\r\n"]
[740.006, "o", "ng.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[745.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[745.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[745.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[745.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[750.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[750.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[750.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[750.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[755.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[755.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[755.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[755.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[760.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[760.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[760.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[760.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[765.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[765.002, "i", "cd asv_benchmarks\r"]
[765.004, "o", "cd asv_benchmarks\r\n"]
[765.006, "o", "\u001b[?2004l\r\n"]
[770.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[770.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | head -n 200\r"]
[770.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | head -n 200\r\n"]
[770.006, "o", "\u001b[?2004l\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[775.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[775.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[775.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[775.006, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[780.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[780.002, "i", "cd ..\r"]
[780.004, "o", "cd ..\r\n"]
[780.006, "o", "\u001b[?2004l\r\n"]
[785.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[785.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[785.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[785.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[790.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[790.002, "i", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r"]
[790.004, "o", "grep -n \"_sparse_encode_precomputed\" sklearn/decomposition/_dict_learning.py\r\n"]
[790.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K438\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        delayed(\u001b[01;31m\u001b[K_sparse_encode_precomputed\u001b[m\u001b[K)(\r\n"]
[795.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[795.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[795.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[795.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n"]
[800.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[800.002, "i", "sed -n '400,1000p' sklearn/decomposition/_dict_learning.py\r"]
[800.004, "o", "sed -n '400,1000p' sklearn/decomposition/_dict_learning.py\r\n"]
[800.006, "o", "\u001b[?2004l\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n"]
[805.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[805.002, "i", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r"]
[805.004, "o", "sed -n '1000,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[805.006, "o", "\u001b[?2004l\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[810.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[810.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[810.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[810.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[815.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[815.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[815.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[815.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[820.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[820.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[820.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[820.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[825.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[825.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[825.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[825.006, "o", "\u001b[?2004l\r\n"]
[830.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[830.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[830.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[830.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n"]
[835.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[835.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[835.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[835.006, "o", "\u001b[?2004l\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[840.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[840.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[840.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[840.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[845.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[845.002, "i", "cd asv_benchmarks\r"]
[845.004, "o", "cd asv_benchmarks\r\n"]
[845.006, "o", "\u001b[?2004l\r\n"]
[850.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[850.002, "i", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | sed -n '1,200p'\r"]
[850.004, "o", "asv dev -l | grep -n \"MiniBatchDictionaryLearningBenchmark\" || asv dev -l | sed -n '1,200p'\r\n"]
[850.006, "o", "\u001b[?2004l\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\nusage: asv [-h] [--verbose] [--config CONFIG] [--version]\r\n           {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}\r\n           ...\r\nasv: error: argument {help,quickstart,machine,setup,run,continuous,find,rm,publish,preview,profile,update,show,compare,check,gh-pages}: invalid choice: 'dev' (choose from 'help', 'quickstart', 'machine', 'setup', 'run', 'continuous', 'find', 'rm', 'publish', 'preview', 'profile', 'update', 'show', 'compare', 'check', 'gh-pages')\r\n"]
[855.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[855.002, "i", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r"]
[855.004, "o", "asv profile --python=same benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || \r\n"]
[855.006, "o", " asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || true\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'benchmarks.decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[860.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[860.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[860.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[860.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[865.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[865.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" -n sklearn/decomposition/_dict_learning.py || true\r"]
[865.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" -n sklearn/decomposition/_dict_learning.py || true\r\n"]
[865.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[870.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[870.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[870.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[870.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[875.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[875.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[875.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[875.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[880.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[880.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[880.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[880.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[885.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[885.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[885.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[885.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[890.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[890.002, "i", "cd asv_benchmarks\r"]
[890.004, "o", "cd asv_benchmarks\r\n"]
[890.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_benchmarks: No such file or directory\r\n"]
[895.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[895.002, "i", "asv run --python=same --quick --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-\r"]
[895.004, "o", "asv run --python=same --quick --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.*\" --show-\r\n"]
[895.006, "o", "-stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 6 total benchmarks (1 commits * 1 environments * 6 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit\u001b[0m                                                                                  ok\r\n[16.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       93.8M \r\n                    cd        93.1M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform\u001b[0m                                                                            ok\r\n[33.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =======\u001b[0m\r\n             --               n_jobs\r\n             --------------- -------\r\n              fit_algorithm     1   \r\n             =============== =======\r\n                   lars       84.4M \r\n                    cd        84.2M \r\n             =============== =======\r\n\r\n[33.33%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n                                                                                     ok\r\n[50.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       6.93\u00b10s \r\n                    cd        1.57\u00b10s \r\n             =============== =========\r\n\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0m                                                                               ok\r\n[66.67%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n             --                n_jobs \r\n             --------------- ---------\r\n              fit_algorithm      1    \r\n             =============== =========\r\n                   lars       161\u00b10ms \r\n                    cd        159\u00b10ms \r\n             =============== =========\r\n\r\n[66.67%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n              /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                new_code = orthogonal_mp_gram(\r\n\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_test_score\u001b[0m                                                                             ok\r\n[83.33%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n             --                      n_jobs        \r\n             --------------- ----------------------\r\n              fit_algorithm            1           \r\n             =============== ======================\r\n                   lars       -0.07507291436195374 \r\n                    cd        -0.07509346306324005 \r\n             =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.track_train_score\u001b[0m                                                                            ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ======================\u001b[0m\r\n              --                      n_jobs        \r\n              --------------- ----------------------\r\n               fit_algorithm            1           \r\n              =============== ======================\r\n                    lars       -0.07244297116994858 \r\n                     cd        -0.07244537025690079 \r\n              =============== ======================\r\n\r\n[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n\r\n"]
[900.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[900.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r"]
[900.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.*\"\r\n"]
[900.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[905.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[905.002, "i", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)\" || a\r"]
[905.004, "o", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)\" || a\r\n"]
[905.006, "o", "asv profile --python=same \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(cd,1)\" || true\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(lars,1)' benchmark not found\u001b[0m\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit(cd,1)' benchmark not found\u001b[0m\r\n"]
[910.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[910.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[910.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[910.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[915.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[915.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[915.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[915.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[920.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[920.002, "i", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[920.004, "o", "grep -n \"cov\\s*=\\s|gram\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[920.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[925.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[925.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[925.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[925.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[930.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[930.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[930.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[930.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[935.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[935.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[935.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[935.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[940.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[940.002, "i", "cd asv_benchmarks\r"]
[940.004, "o", "cd asv_benchmarks\r\n"]
[940.006, "o", "\u001b[?2004l\r\n-bash: cd: asv_benchmarks: No such file or directory\r\n"]
[945.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[945.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[945.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[945.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile --config asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.84\u00b10.03s \r\n                     cd        1.58\u00b10.04s \r\n              =============== ============\r\n\r\n"]
[950.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[950.002, "i", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile\r"]
[950.004, "o", "asv profile --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit || asv profile\r\n"]
[950.006, "o", "e --config asv.conf.json --python=same decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;31m'decomposition.MiniBatchDictionaryLearningBenchmark.time_fit' benchmark not found\u001b[0m\r\n"]
[955.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[955.002, "i", "cd ..\r"]
[955.004, "o", "cd ..\r\n"]
[955.006, "o", "\u001b[?2004l\r\n"]
[960.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[960.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[960.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[960.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[965.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[965.002, "i", "grep -n \"sparse_encode\\(|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r"]
[965.004, "o", "grep -n \"sparse_encode\\(|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[965.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[970.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[970.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[970.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[970.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n"]
[975.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[975.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[975.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[975.006, "o", "\u001b[?2004l\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[980.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[980.002, "i", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r"]
[980.004, "o", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[980.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n"]
[985.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[985.002, "i", "applypatch << 'PATCH'\r"]
[985.004, "o", "applypatch << 'PATCH'\r\n"]
[985.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> *** Begin Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        new_code = orthogonal_mp_gram(\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # Avoid unnecessary copies of the covariance / Xy array in the\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # hot path. The covariance is not reused after sparse coding within\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # the minibatch update, so we can safely allow in-place updates.\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # This reduces allocation and memory traffic in tight loops.\r\n\u001b[?2004l\r\n\u001b[?2004h> +        new_code = orthogonal_mp_gram(\r\n\u001b[?2004l\r\n\u001b[?2004h>              Gram=gram,\r\n\u001b[?2004l\r\n\u001b[?2004h>              Xy=cov,\r\n\u001b[?2004l\r\n\u001b[?2004h>              n_nonzero_coefs=int(regularization),\r\n\u001b[?2004l\r\n\u001b[?2004h>              tol=None,\r\n\u001b[?2004l\r\n\u001b[?2004h> -            norms_squared=row_norms(X, squared=True),\r\n\u001b[?2004l\r\n\u001b[?2004h> -            copy_Xy=copy_cov,\r\n\u001b[?2004l\r\n\u001b[?2004h> +            norms_squared=row_norms(X, squared=True),\r\n\u001b[?2004l\r\n\u001b[?2004h> +            copy_Xy=copy_cov,\r\n\u001b[?2004l\r\n\u001b[?2004h>          ).T\r\n\u001b[?2004l\r\n\u001b[?2004h> *** End Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> PATCH\r\n\u001b[?2004l\r\n-bash: applypatch: command not found\r\n"]
[990.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[990.002, "i", "applypatch << 'PATCH'\r"]
[990.004, "o", "applypatch << 'PATCH'\r\n"]
[990.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> *** Begin Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -        clf = Lasso(\r\n\u001b[?2004l\r\n\u001b[?2004h> +        # Reuse precomputed Gram and avoid redundant allocations when possible\r\n\u001b[?2004l\r\n\u001b[?2004h> +        clf = Lasso(\r\n\u001b[?2004l\r\n\u001b[?2004h>              alpha=alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>              fit_intercept=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>              precompute=gram,\r\n\u001b[?2004l\r\n\u001b[?2004h>              max_iter=max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>              warm_start=True,\r\n\u001b[?2004l\r\n\u001b[?2004h>              positive=positive,\r\n\u001b[?2004l\r\n\u001b[?2004h>          )\r\n\u001b[?2004l\r\n\u001b[?2004h> *** End Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> PATCH\r\n\u001b[?2004l\r\n-bash: applypatch: command not found\r\n"]
[995.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[995.002, "i", "applypatch << 'PATCH'\r"]
[995.004, "o", "applypatch << 'PATCH'\r\n"]
[995.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> *** Begin Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> *** Update File: sklearn/decomposition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[?2004h> @@\r\n\u001b[?2004l\r\n\u001b[?2004h> -            lasso_lars = LassoLars(\r\n\u001b[?2004l\r\n\u001b[?2004h> +            # Instantiate the estimator with precomputed Gram to avoid\r\n\u001b[?2004l\r\n\u001b[?2004h> +            # recomputations inside the solver.\r\n\u001b[?2004l\r\n\u001b[?2004h> +            lasso_lars = LassoLars(\r\n\u001b[?2004l\r\n\u001b[?2004h>                  alpha=alpha,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  fit_intercept=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  verbose=verbose,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  precompute=gram,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  fit_path=False,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  positive=positive,\r\n\u001b[?2004l\r\n\u001b[?2004h>                  max_iter=max_iter,\r\n\u001b[?2004l\r\n\u001b[?2004h>              )\r\n\u001b[?2004l\r\n\u001b[?2004h> *** End Patch\r\n\u001b[?2004l\r\n\u001b[?2004h> PATCH\r\n\u001b[?2004l\r\n-bash: applypatch: command not found\r\n"]
[1000.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1000.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1000.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1000.006, "o", "\u001b[?2004l\r\n"]
[1005.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1005.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q || true\r"]
[1005.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q || true\r\n"]
[1005.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1010.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1010.002, "i", "cd asv_benchmarks\r"]
[1010.004, "o", "cd asv_benchmarks\r\n"]
[1010.006, "o", "\u001b[?2004l\r\n"]
[1015.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1015.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1015.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1015.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --show-stderr\r\n                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.61\u00b10.06s \r\n                     cd        1.54\u00b10.01s \r\n              =============== ============\r\n\r\n"]
[1020.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1020.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --\r"]
[1020.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_transform\" --\r\n"]
[1020.006, "o", "-show-stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_transform\u001b[0mcd ..\r\ngrep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\nsed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\nsed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n                                                                               ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== =========\u001b[0m\r\n              --                n_jobs \r\n              --------------- ---------\r\n               fit_algorithm      1    \r\n              =============== =========\r\n                    lars       162\u00b11ms \r\n                     cd        155\u00b13ms \r\n              =============== =========\r\n\r\n[100.00%] \u00b7\u00b7\u00b7\u00b7 \u001b[0;31mFor parameters: 'lars', 1\u001b[0m\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n               /workspace/repo/sklearn/decomposition/_dict_learning.py:193: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\r\n                 new_code = orthogonal_mp_gram(\r\n\r\n"]
[1025.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1025.002, "i", "cd ..\r"]
[1025.004, "o", "cd ..\r\n"]
[1025.006, "o", "\u001b[?2004l\r\n"]
[1030.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1030.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py \r"]
[1030.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py \r\n"]
[1035.0, "o", "\u001bM(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1035.002, "i", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.p\u001b[Ky\r"]
[1035.004, "o", "grep -n \"def _minibatch_step\\|_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.p\u001b[Ky\r\n"]
[1035.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1040.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1040.002, "i", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r"]
[1040.004, "o", "sed -n '1,500p' sklearn/decomposition/_dict_learning.py\r\n"]
[1040.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n"]
[1045.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1045.002, "i", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1045.004, "o", "sed -n '500,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1045.006, "o", "\u001b[?2004l\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[1050.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1050.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1050.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1050.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[1055.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1055.002, "i", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1055.004, "o", "sed -n '2000,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1055.006, "o", "\u001b[?2004l\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1060.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1060.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1060.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1060.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1065.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1065.002, "i", "cd asv_benchmarks\r"]
[1065.004, "o", "cd asv_benchmarks\r\n"]
[1065.006, "o", "\u001b[?2004l\r\n"]
[1070.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1070.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1070.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1070.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.50\u00b10.03s \r\n                     cd        1.50\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1075.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1075.002, "i", "cd /workspace/repo\r"]
[1075.004, "o", "cd /workspace/repo\r\n"]
[1075.006, "o", "\u001b[?2004l\r\n"]
[1080.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1080.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r"]
[1080.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py\r\n"]
[1080.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[1085.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1085.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1085.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1085.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1090.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1090.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1090.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1090.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1095.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1095.002, "i", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[1095.004, "o", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1095.006, "o", "\u001b[?2004l\r\n"]
[1100.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1100.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1100.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1100.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n"]
[1105.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1105.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1105.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1105.006, "o", "\u001b[?2004l\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n"]
[1110.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1110.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1110.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1110.006, "o", "\u001b[?2004l\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n"]
[1115.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1115.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"_minibatch_step\\(\" -n sklearn/dec\r"]
[1115.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"_minibatch_step\\(\" -n sklearn/dec\r\n"]
[1115.006, "o", "composition/_dict_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    \u001b[01;31m\u001b[Kdef _minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n"]
[1120.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1120.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1120.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1120.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1125.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1125.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1125.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1125.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1130.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1130.002, "i", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r"]
[1130.004, "o", "grep -n \"\\bgram\\s*=\\s|\\bcov\\s*=\\s\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1130.006, "o", "\u001b[?2004l\r\n"]
[1135.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1135.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1135.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1135.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n"]
[1140.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1140.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1140.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1140.006, "o", "\u001b[?2004l\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n"]
[1145.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1145.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1145.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1145.006, "o", "\u001b[?2004l\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n"]
[1150.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1150.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1150.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1150.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1155.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1155.002, "i", "cd asv_benchmarks\r"]
[1155.004, "o", "cd asv_benchmarks\r\n"]
[1155.006, "o", "\u001b[?2004l\r\n"]
[1160.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1160.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1160.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1160.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.52\u00b10.04s \r\n                     cd        1.54\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1165.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1165.002, "i", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1165.004, "o", "grep -n \"def _minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1165.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1170.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1170.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1170.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1170.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1175.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1175.002, "i", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1175.004, "o", "grep -n \"sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1175.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1180.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1180.002, "i", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r"]
[1180.004, "o", "sed -n '1,600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1180.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1185.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1185.002, "i", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r"]
[1185.004, "o", "sed -n '600,1400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1185.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1190.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1190.002, "i", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1190.004, "o", "sed -n '1400,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1190.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1195.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1195.002, "i", "cd /workspace/repo\r"]
[1195.004, "o", "cd /workspace/repo\r\n"]
[1195.006, "o", "\u001b[?2004l\r\n"]
[1200.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1200.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1200.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1200.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1205.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1205.002, "i", "grep -n \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1205.004, "o", "grep -n \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1205.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1210.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1210.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1210.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1210.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1215.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1215.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1215.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1215.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[1220.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1220.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1220.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1220.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[1225.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1225.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1225.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1225.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1230.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1230.002, "i", "cd asv_benchmarks\r"]
[1230.004, "o", "cd asv_benchmarks\r\n"]
[1230.006, "o", "\u001b[?2004l\r\n"]
[1235.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1235.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1235.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1235.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.69\u00b10.06s \r\n                     cd        1.64\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1240.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1240.002, "i", "cd /workspace/repo\r"]
[1240.004, "o", "cd /workspace/repo\r\n"]
[1240.006, "o", "\u001b[?2004l\r\n"]
[1245.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1245.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1245.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1245.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1250.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1250.002, "i", "grep -n \"_sparse_encode_precomputed\\s*(\\|sparse_encode\\s*(\" sklearn/decomposition/_dict_learning.py\r"]
[1250.004, "o", "grep -n \"_sparse_encode_precomputed\\s*(\\|sparse_encode\\s*(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1250.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K38\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[K_sparse_encode_precomputed(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K227\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K361\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    return _\u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K378\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[Kdef _\u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K416\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[K_sparse_encode_precomputed(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K604\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K1001\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        this_code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K1052\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K1272\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = \u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n\u001b[32m\u001b[K2224\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        code = _\u001b[01;31m\u001b[Ksparse_encode(\u001b[m\u001b[K\r\n"]
[1255.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1255.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1255.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1255.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n"]
[1260.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1260.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1260.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1260.006, "o", "\u001b[?2004l\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[1265.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1265.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1265.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1265.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1270.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1270.002, "i", "python - << 'PY'\r"]
[1270.004, "o", "python - << 'PY'\r\n"]
[1270.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import re, sys\r\n\u001b[?2004l\r\n\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n\u001b[?2004l\r\n\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Add copy_cov=False in calls to _sparse_encode_precomputed if not already present\r\n\u001b[?2004l\r\n\u001b[?2004h> pat=r\"_sparse_encode_precomputed\\((?P<args>[^\\)]*)\\)\"\r\n\u001b[?2004l\r\n\u001b[?2004h> \r\n\u001b[?2004l\r\n\u001b[?2004h> def repl(m):\r\n\u001b[?2004l\r\n\u001b[?2004h>     args=m.group('args')\r\n\u001b[?2004l\r\n\u001b[?2004h>     if 'copy_cov' in args:\r\n\u001b[?2004l\r\n\u001b[?2004h>         return m.group(0)\r\n\u001b[?2004l\r\n\u001b[?2004h>     # insert copy_cov=False at end before closing\r\n\u001b[?2004l\r\n\u001b[?2004h>     new=f\"_sparse_encode_precomputed({args}, copy_cov=False)\"\r\n\u001b[?2004l\r\n\u001b[?2004h>     return new\r\n\u001b[?2004l\r\n\u001b[?2004h> ns=re.sub(pat,repl,s)\r\n\u001b[?2004l\r\n\u001b[?2004h> if ns!=s:\r\n\u001b[?2004l\r\n\u001b[?2004h>     open(p,'w',encoding='utf-8').write(ns)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Patched _sparse_encode_precomputed call(s) with copy_cov=False')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('No patch applied (already present or no call found)')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nNo patch applied (already present or no call found)\r\n"]
[1275.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1275.002, "i", "git status -sb\r"]
[1275.004, "o", "git status -sb\r\n"]
[1275.006, "o", "\u001b[?2004l\r\n## \u001b[32mmain\u001b[m\r\n"]
[1280.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1280.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1280.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1280.006, "o", "\u001b[?2004l\r\n"]
[1285.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1285.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1285.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1285.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1290.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1290.002, "i", "cd asv_benchmarks\r"]
[1290.004, "o", "cd asv_benchmarks\r\n"]
[1290.006, "o", "\u001b[?2004l\r\n"]
[1295.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1295.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1295.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1295.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.83\u00b10.02s \r\n                     cd        1.64\u00b10.03s \r\n              =============== ============\r\n\r\n"]
[1300.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1300.002, "i", "cd /workspace/repo\r"]
[1300.004, "o", "cd /workspace/repo\r\n"]
[1300.006, "o", "\u001b[?2004l\r\n"]
[1305.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1305.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"minibatch\" sklearn/decomposition/_dic\r"]
[1305.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || grep -n \"minibatch\" sklearn/decomposition/_dic\r\n"]
[1305.006, "o", "ct_learning.py\r\n\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n\u001b[32m\u001b[K2479\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1310.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1310.002, "i", "grep -n \"def sparse_encode\\|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1310.004, "o", "grep -n \"def sparse_encode\\|_sparse_encode_precomputed\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1310.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1315.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1315.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1315.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1315.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1320.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1320.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1320.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1320.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[1325.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1325.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1325.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1325.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[1330.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1330.002, "i", "cd /workspace/repo\r"]
[1330.004, "o", "cd /workspace/repo\r\n"]
[1330.006, "o", "\u001b[?2004l\r\n"]
[1335.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1335.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1335.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1335.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1340.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1340.002, "i", "grep -n \"def _update_dict\\|def update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1340.004, "o", "grep -n \"def _update_dict\\|def update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1340.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1345.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1345.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1345.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1345.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1350.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1350.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1350.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1350.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n"]
[1355.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1355.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1355.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1355.006, "o", "\u001b[?2004l\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[1360.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1360.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1360.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1360.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1365.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1365.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1365.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1365.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n\u001b[32m\u001b[K2479\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1370.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1370.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1370.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1370.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1375.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1375.002, "i", "grep -n \"def transform\\(|def fit\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1375.004, "o", "grep -n \"def transform\\(|def fit\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1375.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1380.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1380.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1380.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1380.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1385.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1385.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1385.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1385.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[1390.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1390.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r"]
[1390.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1390.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n"]
[1395.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1395.002, "i", "cd /workspace/repo\r"]
[1395.004, "o", "cd /workspace/repo\r\n"]
[1395.006, "o", "\u001b[?2004l\r\n"]
[1400.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1400.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r"]
[1400.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r\n"]
[1400.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1405.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1405.002, "i", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1405.004, "o", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1405.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1410.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1410.002, "i", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1410.004, "o", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1410.006, "o", "\u001b[?2004l\r\n     1\t\"\"\" Dictionary learning.\r\n     2\t\"\"\"\r\n     3\t# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n     4\t# License: BSD 3 clause\r\n     5\t\r\n     6\timport itertools\r\n     7\timport sys\r\n     8\timport time\r\n     9\timport warnings\r\n    10\tfrom math import ceil\r\n    11\tfrom numbers import Integral, Real\r\n    12\t\r\n    13\timport numpy as np\r\n    14\tfrom joblib import effective_n_jobs\r\n    15\tfrom scipy import linalg\r\n    16\t\r\n    17\tfrom ..base import (\r\n    18\t    BaseEstimator,\r\n    19\t    ClassNamePrefixFeaturesOutMixin,\r\n    20\t    TransformerMixin,\r\n    21\t    _fit_context,\r\n    22\t)\r\n    23\tfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\n    24\tfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\n    25\tfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\n    26\tfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\n    27\tfrom ..utils.parallel import Parallel, delayed\r\n    28\tfrom ..utils.validation import check_is_fitted\r\n    29\t\r\n    30\t\r\n    31\tdef _check_positive_coding(method, positive):\r\n    32\t    if positive and method in [\"omp\", \"lars\"]:\r\n    33\t        raise ValueError(\r\n    34\t            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n    35\t        )\r\n    36\t\r\n    37\t\r\n    38\tdef _sparse_encode_precomputed(\r\n    39\t    X,\r\n    40\t    dictionary,\r\n    41\t    *,\r\n    42\t    gram=None,\r\n    43\t    cov=None,\r\n    44\t    algorithm=\"lasso_lars\",\r\n    45\t    regularization=None,\r\n    46\t    copy_cov=True,\r\n    47\t    init=None,\r\n    48\t    max_iter=1000,\r\n    49\t    verbose=0,\r\n    50\t    positive=False,\r\n    51\t):\r\n    52\t    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n    53\t\r\n    54\t    Each row of the result is the solution to a Lasso problem.\r\n    55\t\r\n    56\t    Parameters\r\n    57\t    ----------\r\n    58\t    X : ndarray of shape (n_samples, n_features)\r\n    59\t        Data matrix.\r\n    60\t\r\n    61\t    dictionary : ndarray of shape (n_components, n_features)\r\n    62\t        The dictionary matrix against which to solve the sparse coding of\r\n    63\t        the data. Some of the algorithms assume normalized rows.\r\n    64\t\r\n    65\t    gram : ndarray of shape (n_components, n_components), default=None\r\n    66\t        Precomputed Gram matrix, `dictionary * dictionary'`\r\n    67\t        gram can be `None` if method is 'threshold'.\r\n    68\t\r\n    69\t    cov : ndarray of shape (n_components, n_samples), default=None\r\n    70\t        Precomputed covariance, `dictionary * X'`.\r\n    71\t\r\n    72\t    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n    73\t            default='lasso_lars'\r\n    74\t        The algorithm used:\r\n    75\t\r\n    76\t        * `'lars'`: uses the least angle regression method\r\n    77\t          (`linear_model.lars_path`);\r\n    78\t        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n    79\t        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n    80\t          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n    81\t          the estimated components are sparse;\r\n    82\t        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n    83\t          solution;\r\n    84\t        * `'threshold'`: squashes to zero all coefficients less than\r\n    85\t          regularization from the projection `dictionary * data'`.\r\n    86\t\r\n    87\t    regularization : int or float, default=None\r\n    88\t        The regularization parameter. It corresponds to alpha when\r\n    89\t        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n    90\t        Otherwise it corresponds to `n_nonzero_coefs`.\r\n    91\t\r\n    92\t    init : ndarray of shape (n_samples, n_components), default=None\r\n    93\t        Initialization value of the sparse code. Only used if\r\n    94\t        `algorithm='lasso_cd'`.\r\n    95\t\r\n    96\t    max_iter : int, default=1000\r\n    97\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n    98\t        `'lasso_lars'`.\r\n    99\t\r\n   100\t    copy_cov : bool, default=True\r\n   101\t        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n   102\t        be overwritten.\r\n   103\t\r\n   104\t    verbose : int, default=0\r\n   105\t        Controls the verbosity; the higher, the more messages.\r\n   106\t\r\n   107\t    positive: bool, default=False\r\n   108\t        Whether to enforce a positivity constraint on the sparse code.\r\n   109\t\r\n   110\t        .. versionadded:: 0.20\r\n   111\t\r\n   112\t    Returns\r\n   113\t    -------\r\n   114\t    code : ndarray of shape (n_components, n_features)\r\n   115\t        The sparse codes.\r\n   116\t    \"\"\"\r\n   117\t    n_samples, n_features = X.shape\r\n   118\t    n_components = dictionary.shape[0]\r\n   119\t\r\n   120\t    if algorithm == \"lasso_lars\":\r\n   121\t        alpha = float(regularization) / n_features  # account for scaling\r\n   122\t        try:\r\n   123\t            err_mgt = np.seterr(all=\"ignore\")\r\n   124\t\r\n   125\t            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n   126\t            # corrects the verbosity level.\r\n   127\t            lasso_lars = LassoLars(\r\n   128\t                alpha=alpha,\r\n   129\t                fit_intercept=False,\r\n   130\t                verbose=verbose,\r\n   131\t                precompute=gram,\r\n   132\t                fit_path=False,\r\n   133\t                positive=positive,\r\n   134\t                max_iter=max_iter,\r\n   135\t            )\r\n   136\t            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n   137\t            new_code = lasso_lars.coef_\r\n   138\t        finally:\r\n   139\t            np.seterr(**err_mgt)\r\n   140\t\r\n   141\t    elif algorithm == \"lasso_cd\":\r\n   142\t        alpha = float(regularization) / n_features  # account for scaling\r\n   143\t\r\n   144\t        # TODO: Make verbosity argument for Lasso?\r\n   145\t        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n   146\t        # argument that we could pass in from Lasso.\r\n   147\t        clf = Lasso(\r\n   148\t            alpha=alpha,\r\n   149\t            fit_intercept=False,\r\n   150\t            precompute=gram,\r\n   151\t            max_iter=max_iter,\r\n   152\t            warm_start=True,\r\n   153\t            positive=positive,\r\n   154\t        )\r\n   155\t\r\n   156\t        if init is not None:\r\n   157\t            # In some workflows using coordinate descent algorithms:\r\n   158\t            #  - users might provide NumPy arrays with read-only buffers\r\n   159\t            #  - `joblib` might memmap arrays making their buffer read-only\r\n   160\t            # TODO: move this handling (which is currently too broad)\r\n   161\t            # closer to the actual private function which need buffers to be writable.\r\n   162\t            if not init.flags[\"WRITEABLE\"]:\r\n   163\t                init = np.array(init)\r\n   164\t            clf.coef_ = init\r\n   165\t\r\n   166\t        clf.fit(dictionary.T, X.T, check_input=False)\r\n   167\t        new_code = clf.coef_\r\n   168\t\r\n   169\t    elif algorithm == \"lars\":\r\n   170\t        try:\r\n   171\t            err_mgt = np.seterr(all=\"ignore\")\r\n   172\t\r\n   173\t            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n   174\t            # corrects the verbosity level.\r\n   175\t            lars = Lars(\r\n   176\t                fit_intercept=False,\r\n   177\t                verbose=verbose,\r\n   178\t                precompute=gram,\r\n   179\t                n_nonzero_coefs=int(regularization),\r\n   180\t                fit_path=False,\r\n   181\t            )\r\n   182\t            lars.fit(dictionary.T, X.T, Xy=cov)\r\n   183\t            new_code = lars.coef_\r\n   184\t        finally:\r\n   185\t            np.seterr(**err_mgt)\r\n   186\t\r\n   187\t    elif algorithm == \"threshold\":\r\n   188\t        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n   189\t        if positive:\r\n   190\t            np.clip(new_code, 0, None, out=new_code)\r\n   191\t\r\n   192\t    elif algorithm == \"omp\":\r\n   193\t        new_code = orthogonal_mp_gram(\r\n   194\t            Gram=gram,\r\n   195\t            Xy=cov,\r\n   196\t            n_nonzero_coefs=int(regularization),\r\n   197\t            tol=None,\r\n   198\t            norms_squared=row_norms(X, squared=True),\r\n   199\t            copy_Xy=copy_cov,\r\n   200\t        ).T\r\n   201\t\r\n   202\t    return new_code.reshape(n_samples, n_components)\r\n   203\t\r\n   204\t\r\n   205\t@validate_params(\r\n   206\t    {\r\n   207\t        \"X\": [\"array-like\"],\r\n   208\t        \"dictionary\": [\"array-like\"],\r\n   209\t        \"gram\": [\"array-like\", None],\r\n   210\t        \"cov\": [\"array-like\", None],\r\n   211\t        \"algorithm\": [\r\n   212\t            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n   213\t        ],\r\n   214\t        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n   215\t        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n   216\t        \"copy_cov\": [\"boolean\"],\r\n   217\t        \"init\": [\"array-like\", None],\r\n   218\t        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n   219\t        \"n_jobs\": [Integral, None],\r\n   220\t        \"check_input\": [\"boolean\"],\r\n   221\t        \"verbose\": [\"verbose\"],\r\n   222\t        \"positive\": [\"boolean\"],\r\n   223\t    },\r\n   224\t    prefer_skip_nested_validation=True,\r\n   225\t)\r\n   226\t# XXX : could be moved to the linear_model module\r\n   227\tdef sparse_encode(\r\n   228\t    X,\r\n   229\t    dictionary,\r\n   230\t    *,\r\n   231\t    gram=None,\r\n   232\t    cov=None,\r\n   233\t    algorithm=\"lasso_lars\",\r\n   234\t    n_nonzero_coefs=None,\r\n   235\t    alpha=None,\r\n   236\t    copy_cov=True,\r\n   237\t    init=None,\r\n   238\t    max_iter=1000,\r\n   239\t    n_jobs=None,\r\n   240\t    check_input=True,\r\n   241\t    verbose=0,\r\n   242\t    positive=False,\r\n   243\t):\r\n   244\t    \"\"\"Sparse coding.\r\n   245\t\r\n   246\t    Each row of the result is the solution to a sparse coding problem.\r\n   247\t    The goal is to find a sparse array `code` such that::\r\n   248\t\r\n   249\t        X ~= code * dictionary\r\n   250\t\r\n   251\t    Read more in the :ref:`User Guide <SparseCoder>`.\r\n   252\t\r\n   253\t    Parameters\r\n   254\t    ----------\r\n   255\t    X : array-like of shape (n_samples, n_features)\r\n   256\t        Data matrix.\r\n   257\t\r\n   258\t    dictionary : array-like of shape (n_components, n_features)\r\n   259\t        The dictionary matrix against which to solve the sparse coding of\r\n   260\t        the data. Some of the algorithms assume normalized rows for meaningful\r\n"]
[1415.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1415.002, "i", "sed -n '260,800p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1415.004, "o", "sed -n '260,800p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1415.006, "o", "\u001b[?2004l\r\n     1\t        the data. Some of the algorithms assume normalized rows for meaningful\r\n     2\t        output.\r\n     3\t\r\n     4\t    gram : array-like of shape (n_components, n_components), default=None\r\n     5\t        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n     6\t\r\n     7\t    cov : array-like of shape (n_components, n_samples), default=None\r\n     8\t        Precomputed covariance, `dictionary' * X`.\r\n     9\t\r\n    10\t    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n    11\t            default='lasso_lars'\r\n    12\t        The algorithm used:\r\n    13\t\r\n    14\t        * `'lars'`: uses the least angle regression method\r\n    15\t          (`linear_model.lars_path`);\r\n    16\t        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n    17\t        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n    18\t          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n    19\t          the estimated components are sparse;\r\n    20\t        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n    21\t          solution;\r\n    22\t        * `'threshold'`: squashes to zero all coefficients less than\r\n    23\t          regularization from the projection `dictionary * data'`.\r\n    24\t\r\n    25\t    n_nonzero_coefs : int, default=None\r\n    26\t        Number of nonzero coefficients to target in each column of the\r\n    27\t        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n    28\t        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n    29\t        `n_nonzero_coefs=int(n_features / 10)`.\r\n    30\t\r\n    31\t    alpha : float, default=None\r\n    32\t        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n    33\t        penalty applied to the L1 norm.\r\n    34\t        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n    35\t        threshold below which coefficients will be squashed to zero.\r\n    36\t        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n    37\t        the reconstruction error targeted. In this case, it overrides\r\n    38\t        `n_nonzero_coefs`.\r\n    39\t        If `None`, default to 1.\r\n    40\t\r\n    41\t    copy_cov : bool, default=True\r\n    42\t        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n    43\t        be overwritten.\r\n    44\t\r\n    45\t    init : ndarray of shape (n_samples, n_components), default=None\r\n    46\t        Initialization value of the sparse codes. Only used if\r\n    47\t        `algorithm='lasso_cd'`.\r\n    48\t\r\n    49\t    max_iter : int, default=1000\r\n    50\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n    51\t        `'lasso_lars'`.\r\n    52\t\r\n    53\t    n_jobs : int, default=None\r\n    54\t        Number of parallel jobs to run.\r\n    55\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n    56\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n    57\t        for more details.\r\n    58\t\r\n    59\t    check_input : bool, default=True\r\n    60\t        If `False`, the input arrays X and dictionary will not be checked.\r\n    61\t\r\n    62\t    verbose : int, default=0\r\n    63\t        Controls the verbosity; the higher, the more messages.\r\n    64\t\r\n    65\t    positive : bool, default=False\r\n    66\t        Whether to enforce positivity when finding the encoding.\r\n    67\t\r\n    68\t        .. versionadded:: 0.20\r\n    69\t\r\n    70\t    Returns\r\n    71\t    -------\r\n    72\t    code : ndarray of shape (n_samples, n_components)\r\n    73\t        The sparse codes.\r\n    74\t\r\n    75\t    See Also\r\n    76\t    --------\r\n    77\t    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n    78\t        path using LARS algorithm.\r\n    79\t    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    80\t    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    81\t    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n    82\t        dictionary.\r\n    83\t    \"\"\"\r\n    84\t    if check_input:\r\n    85\t        if algorithm == \"lasso_cd\":\r\n    86\t            dictionary = check_array(\r\n    87\t                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n    88\t            )\r\n    89\t            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n    90\t        else:\r\n    91\t            dictionary = check_array(dictionary)\r\n    92\t            X = check_array(X)\r\n    93\t\r\n    94\t    if dictionary.shape[1] != X.shape[1]:\r\n    95\t        raise ValueError(\r\n    96\t            \"Dictionary and X have different numbers of features:\"\r\n    97\t            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n    98\t        )\r\n    99\t\r\n   100\t    _check_positive_coding(algorithm, positive)\r\n   101\t\r\n   102\t    return _sparse_encode(\r\n   103\t        X,\r\n   104\t        dictionary,\r\n   105\t        gram=gram,\r\n   106\t        cov=cov,\r\n   107\t        algorithm=algorithm,\r\n   108\t        n_nonzero_coefs=n_nonzero_coefs,\r\n   109\t        alpha=alpha,\r\n   110\t        copy_cov=copy_cov,\r\n   111\t        init=init,\r\n   112\t        max_iter=max_iter,\r\n   113\t        n_jobs=n_jobs,\r\n   114\t        verbose=verbose,\r\n   115\t        positive=positive,\r\n   116\t    )\r\n   117\t\r\n   118\t\r\n   119\tdef _sparse_encode(\r\n   120\t    X,\r\n   121\t    dictionary,\r\n   122\t    *,\r\n   123\t    gram=None,\r\n   124\t    cov=None,\r\n   125\t    algorithm=\"lasso_lars\",\r\n   126\t    n_nonzero_coefs=None,\r\n   127\t    alpha=None,\r\n   128\t    copy_cov=True,\r\n   129\t    init=None,\r\n   130\t    max_iter=1000,\r\n   131\t    n_jobs=None,\r\n   132\t    verbose=0,\r\n   133\t    positive=False,\r\n   134\t):\r\n   135\t    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n   136\t\r\n   137\t    n_samples, n_features = X.shape\r\n   138\t    n_components = dictionary.shape[0]\r\n   139\t\r\n   140\t    if algorithm in (\"lars\", \"omp\"):\r\n   141\t        regularization = n_nonzero_coefs\r\n   142\t        if regularization is None:\r\n   143\t            regularization = min(max(n_features / 10, 1), n_components)\r\n   144\t    else:\r\n   145\t        regularization = alpha\r\n   146\t        if regularization is None:\r\n   147\t            regularization = 1.0\r\n   148\t\r\n   149\t    if gram is None and algorithm != \"threshold\":\r\n   150\t        gram = np.dot(dictionary, dictionary.T)\r\n   151\t\r\n   152\t    if cov is None and algorithm != \"lasso_cd\":\r\n   153\t        copy_cov = False\r\n   154\t        cov = np.dot(dictionary, X.T)\r\n   155\t\r\n   156\t    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n   157\t        code = _sparse_encode_precomputed(\r\n   158\t            X,\r\n   159\t            dictionary,\r\n   160\t            gram=gram,\r\n   161\t            cov=cov,\r\n   162\t            algorithm=algorithm,\r\n   163\t            regularization=regularization,\r\n   164\t            copy_cov=copy_cov,\r\n   165\t            init=init,\r\n   166\t            max_iter=max_iter,\r\n   167\t            verbose=verbose,\r\n   168\t            positive=positive,\r\n   169\t        )\r\n   170\t        return code\r\n   171\t\r\n   172\t    # Enter parallel code block\r\n   173\t    n_samples = X.shape[0]\r\n   174\t    n_components = dictionary.shape[0]\r\n   175\t    code = np.empty((n_samples, n_components))\r\n   176\t    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n   177\t\r\n   178\t    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n   179\t        delayed(_sparse_encode_precomputed)(\r\n   180\t            X[this_slice],\r\n   181\t            dictionary,\r\n   182\t            gram=gram,\r\n   183\t            cov=cov[:, this_slice] if cov is not None else None,\r\n   184\t            algorithm=algorithm,\r\n   185\t            regularization=regularization,\r\n   186\t            copy_cov=copy_cov,\r\n   187\t            init=init[this_slice] if init is not None else None,\r\n   188\t            max_iter=max_iter,\r\n   189\t            verbose=verbose,\r\n   190\t            positive=positive,\r\n   191\t        )\r\n   192\t        for this_slice in slices\r\n   193\t    )\r\n   194\t    for this_slice, this_view in zip(slices, code_views):\r\n   195\t        code[this_slice] = this_view\r\n   196\t    return code\r\n   197\t\r\n   198\t\r\n   199\tdef _update_dict(\r\n   200\t    dictionary,\r\n   201\t    Y,\r\n   202\t    code,\r\n   203\t    A=None,\r\n   204\t    B=None,\r\n   205\t    verbose=False,\r\n   206\t    random_state=None,\r\n   207\t    positive=False,\r\n   208\t):\r\n   209\t    \"\"\"Update the dense dictionary factor in place.\r\n   210\t\r\n   211\t    Parameters\r\n   212\t    ----------\r\n   213\t    dictionary : ndarray of shape (n_components, n_features)\r\n   214\t        Value of the dictionary at the previous iteration.\r\n   215\t\r\n   216\t    Y : ndarray of shape (n_samples, n_features)\r\n   217\t        Data matrix.\r\n   218\t\r\n   219\t    code : ndarray of shape (n_samples, n_components)\r\n   220\t        Sparse coding of the data against which to optimize the dictionary.\r\n   221\t\r\n   222\t    A : ndarray of shape (n_components, n_components), default=None\r\n   223\t        Together with `B`, sufficient stats of the online model to update the\r\n   224\t        dictionary.\r\n   225\t\r\n   226\t    B : ndarray of shape (n_features, n_components), default=None\r\n   227\t        Together with `A`, sufficient stats of the online model to update the\r\n   228\t        dictionary.\r\n   229\t\r\n   230\t    verbose: bool, default=False\r\n   231\t        Degree of output the procedure will print.\r\n   232\t\r\n   233\t    random_state : int, RandomState instance or None, default=None\r\n   234\t        Used for randomly initializing the dictionary. Pass an int for\r\n   235\t        reproducible results across multiple function calls.\r\n   236\t        See :term:`Glossary <random_state>`.\r\n   237\t\r\n   238\t    positive : bool, default=False\r\n   239\t        Whether to enforce positivity when finding the dictionary.\r\n   240\t\r\n   241\t        .. versionadded:: 0.20\r\n   242\t    \"\"\"\r\n   243\t    n_samples, n_components = code.shape\r\n   244\t    random_state = check_random_state(random_state)\r\n   245\t\r\n   246\t    if A is None:\r\n   247\t        A = code.T @ code\r\n   248\t    if B is None:\r\n   249\t        B = Y.T @ code\r\n   250\t\r\n   251\t    n_unused = 0\r\n   252\t\r\n   253\t    for k in range(n_components):\r\n   254\t        if A[k, k] > 1e-6:\r\n   255\t            # 1e-6 is arbitrary but consistent with the spams implementation\r\n   256\t            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n   257\t        else:\r\n   258\t            # kth atom is almost never used -> sample a new one from the data\r\n   259\t            newd = Y[random_state.choice(n_samples)]\r\n   260\t\r\n"]
[1420.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1420.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1420.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1420.006, "o", "\u001b[?2004l\r\n     1\t           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n     2\t\r\n     3\t    return_n_iter : bool, default=False\r\n     4\t        Whether or not to return the number of iterations.\r\n     5\t\r\n     6\t        .. deprecated:: 1.1\r\n     7\t           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n     8\t\r\n     9\t    positive_dict : bool, default=False\r\n    10\t        Whether to enforce positivity when finding the dictionary.\r\n    11\t\r\n    12\t        .. versionadded:: 0.20\r\n    13\t\r\n    14\t    positive_code : bool, default=False\r\n    15\t        Whether to enforce positivity when finding the code.\r\n    16\t\r\n    17\t        .. versionadded:: 0.20\r\n    18\t\r\n    19\t    method_max_iter : int, default=1000\r\n    20\t        Maximum number of iterations to perform when solving the lasso problem.\r\n    21\t\r\n    22\t        .. versionadded:: 0.22\r\n    23\t\r\n    24\t    tol : float, default=1e-3\r\n    25\t        Control early stopping based on the norm of the differences in the\r\n    26\t        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n    27\t\r\n    28\t        To disable early stopping based on changes in the dictionary, set\r\n    29\t        `tol` to 0.0.\r\n    30\t\r\n    31\t        .. versionadded:: 1.1\r\n    32\t\r\n    33\t    max_no_improvement : int, default=10\r\n    34\t        Control early stopping based on the consecutive number of mini batches\r\n    35\t        that does not yield an improvement on the smoothed cost function. Used only if\r\n    36\t        `max_iter` is not None.\r\n    37\t\r\n    38\t        To disable convergence detection based on cost function, set\r\n    39\t        `max_no_improvement` to None.\r\n    40\t\r\n    41\t        .. versionadded:: 1.1\r\n    42\t\r\n    43\t    Returns\r\n    44\t    -------\r\n    45\t    code : ndarray of shape (n_samples, n_components),\r\n    46\t        The sparse code (only returned if `return_code=True`).\r\n    47\t\r\n    48\t    dictionary : ndarray of shape (n_components, n_features),\r\n    49\t        The solutions to the dictionary learning problem.\r\n    50\t\r\n    51\t    n_iter : int\r\n    52\t        Number of iterations run. Returned only if `return_n_iter` is\r\n    53\t        set to `True`.\r\n    54\t\r\n    55\t    See Also\r\n    56\t    --------\r\n    57\t    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    58\t    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    59\t    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n    60\t        learning algorithm.\r\n    61\t    SparsePCA : Sparse Principal Components Analysis.\r\n    62\t    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    63\t    \"\"\"\r\n    64\t    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    65\t    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n    66\t        raise ValueError(\r\n    67\t            \"The following arguments are incompatible with 'max_iter': \"\r\n    68\t            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n    69\t        )\r\n    70\t\r\n    71\t    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    72\t    return_inner_stats = _check_warn_deprecated(\r\n    73\t        return_inner_stats,\r\n    74\t        \"return_inner_stats\",\r\n    75\t        default=False,\r\n    76\t        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    77\t    )\r\n    78\t    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    79\t    return_n_iter = _check_warn_deprecated(\r\n    80\t        return_n_iter,\r\n    81\t        \"return_n_iter\",\r\n    82\t        default=False,\r\n    83\t        additional_message=(\r\n    84\t            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n    85\t            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n    86\t        ),\r\n    87\t    )\r\n    88\t\r\n    89\t    if max_iter is not None:\r\n    90\t        transform_algorithm = \"lasso_\" + method\r\n    91\t\r\n    92\t        est = MiniBatchDictionaryLearning(\r\n    93\t            n_components=n_components,\r\n    94\t            alpha=alpha,\r\n    95\t            n_iter=n_iter,\r\n    96\t            n_jobs=n_jobs,\r\n    97\t            fit_algorithm=method,\r\n    98\t            batch_size=batch_size,\r\n    99\t            shuffle=shuffle,\r\n   100\t            dict_init=dict_init,\r\n   101\t            random_state=random_state,\r\n   102\t            transform_algorithm=transform_algorithm,\r\n   103\t            transform_alpha=alpha,\r\n   104\t            positive_code=positive_code,\r\n   105\t            positive_dict=positive_dict,\r\n   106\t            transform_max_iter=method_max_iter,\r\n   107\t            verbose=verbose,\r\n   108\t            callback=callback,\r\n   109\t            tol=tol,\r\n   110\t            max_no_improvement=max_no_improvement,\r\n   111\t        ).fit(X)\r\n   112\t\r\n   113\t        if not return_code:\r\n   114\t            return est.components_\r\n   115\t        else:\r\n   116\t            code = est.transform(X)\r\n   117\t            return code, est.components_\r\n   118\t\r\n   119\t    # TODO(1.4) remove the whole old behavior\r\n   120\t    # Fallback to old behavior\r\n   121\t\r\n   122\t    n_iter = _check_warn_deprecated(\r\n   123\t        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n   124\t    )\r\n   125\t\r\n   126\t    if n_components is None:\r\n   127\t        n_components = X.shape[1]\r\n   128\t\r\n   129\t    if method not in (\"lars\", \"cd\"):\r\n   130\t        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n   131\t\r\n   132\t    _check_positive_coding(method, positive_code)\r\n   133\t\r\n   134\t    method = \"lasso_\" + method\r\n   135\t\r\n   136\t    t0 = time.time()\r\n   137\t    n_samples, n_features = X.shape\r\n   138\t    # Avoid integer division problems\r\n   139\t    alpha = float(alpha)\r\n   140\t    random_state = check_random_state(random_state)\r\n   141\t\r\n   142\t    # Init V with SVD of X\r\n   143\t    if dict_init is not None:\r\n   144\t        dictionary = dict_init\r\n   145\t    else:\r\n   146\t        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n   147\t        dictionary = S[:, np.newaxis] * dictionary\r\n   148\t    r = len(dictionary)\r\n   149\t    if n_components <= r:\r\n   150\t        dictionary = dictionary[:n_components, :]\r\n   151\t    else:\r\n   152\t        dictionary = np.r_[\r\n   153\t            dictionary,\r\n   154\t            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n   155\t        ]\r\n   156\t\r\n   157\t    if verbose == 1:\r\n   158\t        print(\"[dict_learning]\", end=\" \")\r\n   159\t\r\n   160\t    if shuffle:\r\n   161\t        X_train = X.copy()\r\n   162\t        random_state.shuffle(X_train)\r\n   163\t    else:\r\n   164\t        X_train = X\r\n   165\t\r\n   166\t    X_train = check_array(\r\n   167\t        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n   168\t    )\r\n   169\t\r\n   170\t    # Fortran-order dict better suited for the sparse coding which is the\r\n   171\t    # bottleneck of this algorithm.\r\n   172\t    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n   173\t    dictionary = np.require(dictionary, requirements=\"W\")\r\n   174\t\r\n   175\t    batches = gen_batches(n_samples, batch_size)\r\n   176\t    batches = itertools.cycle(batches)\r\n   177\t\r\n   178\t    # The covariance of the dictionary\r\n   179\t    if inner_stats is None:\r\n   180\t        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n   181\t        # The data approximation\r\n   182\t        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n   183\t    else:\r\n   184\t        A = inner_stats[0].copy()\r\n   185\t        B = inner_stats[1].copy()\r\n   186\t\r\n   187\t    # If n_iter is zero, we need to return zero.\r\n   188\t    ii = iter_offset - 1\r\n   189\t\r\n   190\t    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n   191\t        this_X = X_train[batch]\r\n   192\t        dt = time.time() - t0\r\n   193\t        if verbose == 1:\r\n   194\t            sys.stdout.write(\".\")\r\n   195\t            sys.stdout.flush()\r\n   196\t        elif verbose:\r\n   197\t            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n   198\t                print(\r\n   199\t                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n   200\t                )\r\n   201\t\r\n   202\t        this_code = sparse_encode(\r\n   203\t            this_X,\r\n   204\t            dictionary,\r\n   205\t            algorithm=method,\r\n   206\t            alpha=alpha,\r\n   207\t            n_jobs=n_jobs,\r\n   208\t            check_input=False,\r\n   209\t            positive=positive_code,\r\n   210\t            max_iter=method_max_iter,\r\n   211\t            verbose=verbose,\r\n   212\t        )\r\n   213\t\r\n   214\t        # Update the auxiliary variables\r\n   215\t        if ii < batch_size - 1:\r\n   216\t            theta = float((ii + 1) * batch_size)\r\n   217\t        else:\r\n   218\t            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n   219\t        beta = (theta + 1 - batch_size) / (theta + 1)\r\n   220\t\r\n   221\t        A *= beta\r\n   222\t        A += np.dot(this_code.T, this_code)\r\n   223\t        B *= beta\r\n   224\t        B += np.dot(this_X.T, this_code)\r\n   225\t\r\n   226\t        # Update dictionary in place\r\n   227\t        _update_dict(\r\n   228\t            dictionary,\r\n   229\t            this_X,\r\n   230\t            this_code,\r\n   231\t            A,\r\n   232\t            B,\r\n   233\t            verbose=verbose,\r\n   234\t            random_state=random_state,\r\n   235\t            positive=positive_dict,\r\n   236\t        )\r\n   237\t\r\n   238\t        # Maybe we need a stopping criteria based on the amount of\r\n   239\t        # modification in the dictionary\r\n   240\t        if callback is not None:\r\n   241\t            callback(locals())\r\n   242\t\r\n   243\t    if return_inner_stats:\r\n   244\t        if return_n_iter:\r\n   245\t            return dictionary, (A, B), ii - iter_offset + 1\r\n   246\t        else:\r\n   247\t            return dictionary, (A, B)\r\n   248\t    if return_code:\r\n   249\t        if verbose > 1:\r\n   250\t            print(\"Learning code...\", end=\" \")\r\n   251\t        elif verbose == 1:\r\n   252\t            print(\"|\", end=\" \")\r\n   253\t        code = sparse_encode(\r\n   254\t            X,\r\n   255\t            dictionary,\r\n   256\t            algorithm=method,\r\n   257\t            alpha=alpha,\r\n   258\t            n_jobs=n_jobs,\r\n   259\t            check_input=False,\r\n   260\t            positive=positive_code,\r\n"]
[1425.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1425.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r"]
[1425.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '1,260p'\r\n"]
[1425.006, "o", "\u001b[?2004l\r\n     1\t\r\n     2\t    n_jobs : int or None, default=None\r\n     3\t        Number of parallel jobs to run.\r\n     4\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n     5\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n     6\t        for more details.\r\n     7\t\r\n     8\t    code_init : ndarray of shape (n_samples, n_components), default=None\r\n     9\t        Initial value for the code, for warm restart. Only used if `code_init`\r\n    10\t        and `dict_init` are not None.\r\n    11\t\r\n    12\t    dict_init : ndarray of shape (n_components, n_features), default=None\r\n    13\t        Initial values for the dictionary, for warm restart. Only used if\r\n    14\t        `code_init` and `dict_init` are not None.\r\n    15\t\r\n    16\t    callback : callable, default=None\r\n    17\t        Callable that gets invoked every five iterations.\r\n    18\t\r\n    19\t        .. versionadded:: 1.3\r\n    20\t\r\n    21\t    verbose : bool, default=False\r\n    22\t        To control the verbosity of the procedure.\r\n    23\t\r\n    24\t    split_sign : bool, default=False\r\n    25\t        Whether to split the sparse feature vector into the concatenation of\r\n    26\t        its negative part and its positive part. This can improve the\r\n    27\t        performance of downstream classifiers.\r\n    28\t\r\n    29\t    random_state : int, RandomState instance or None, default=None\r\n    30\t        Used for initializing the dictionary when ``dict_init`` is not\r\n    31\t        specified, randomly shuffling the data when ``shuffle`` is set to\r\n    32\t        ``True``, and updating the dictionary. Pass an int for reproducible\r\n    33\t        results across multiple function calls.\r\n    34\t        See :term:`Glossary <random_state>`.\r\n    35\t\r\n    36\t    positive_code : bool, default=False\r\n    37\t        Whether to enforce positivity when finding the code.\r\n    38\t\r\n    39\t        .. versionadded:: 0.20\r\n    40\t\r\n    41\t    positive_dict : bool, default=False\r\n    42\t        Whether to enforce positivity when finding the dictionary.\r\n    43\t\r\n    44\t        .. versionadded:: 0.20\r\n    45\t\r\n    46\t    transform_max_iter : int, default=1000\r\n    47\t        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n    48\t        `'lasso_lars'`.\r\n    49\t\r\n    50\t        .. versionadded:: 0.22\r\n    51\t\r\n    52\t    Attributes\r\n    53\t    ----------\r\n    54\t    components_ : ndarray of shape (n_components, n_features)\r\n    55\t        dictionary atoms extracted from the data\r\n    56\t\r\n    57\t    error_ : array\r\n    58\t        vector of errors at each iteration\r\n    59\t\r\n    60\t    n_features_in_ : int\r\n    61\t        Number of features seen during :term:`fit`.\r\n    62\t\r\n    63\t        .. versionadded:: 0.24\r\n    64\t\r\n    65\t    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n    66\t        Names of features seen during :term:`fit`. Defined only when `X`\r\n    67\t        has feature names that are all strings.\r\n    68\t\r\n    69\t        .. versionadded:: 1.0\r\n    70\t\r\n    71\t    n_iter_ : int\r\n    72\t        Number of iterations run.\r\n    73\t\r\n    74\t    See Also\r\n    75\t    --------\r\n    76\t    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n    77\t        dictionary learning algorithm.\r\n    78\t    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    79\t    SparseCoder : Find a sparse representation of data from a fixed,\r\n    80\t        precomputed dictionary.\r\n    81\t    SparsePCA : Sparse Principal Components Analysis.\r\n    82\t\r\n    83\t    References\r\n    84\t    ----------\r\n    85\t\r\n    86\t    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    87\t    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n    88\t\r\n    89\t    Examples\r\n    90\t    --------\r\n    91\t    >>> import numpy as np\r\n    92\t    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    93\t    >>> from sklearn.decomposition import DictionaryLearning\r\n    94\t    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    95\t    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    96\t    ...     random_state=42,\r\n    97\t    ... )\r\n    98\t    >>> dict_learner = DictionaryLearning(\r\n    99\t    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n   100\t    ...     random_state=42,\r\n   101\t    ... )\r\n   102\t    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n   103\t\r\n   104\t    We can check the level of sparsity of `X_transformed`:\r\n   105\t\r\n   106\t    >>> np.mean(X_transformed == 0)\r\n   107\t    0.41...\r\n   108\t\r\n   109\t    We can compare the average squared euclidean norm of the reconstruction\r\n   110\t    error of the sparse coded signal relative to the squared euclidean norm of\r\n   111\t    the original signal:\r\n   112\t\r\n   113\t    >>> X_hat = X_transformed @ dict_learner.components_\r\n   114\t    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n   115\t    0.07...\r\n   116\t    \"\"\"\r\n   117\t\r\n   118\t    _parameter_constraints: dict = {\r\n   119\t        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n   120\t        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n   121\t        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n   122\t        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n   123\t        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n   124\t        \"transform_algorithm\": [\r\n   125\t            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n   126\t        ],\r\n   127\t        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n   128\t        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n   129\t        \"n_jobs\": [Integral, None],\r\n   130\t        \"code_init\": [np.ndarray, None],\r\n   131\t        \"dict_init\": [np.ndarray, None],\r\n   132\t        \"callback\": [callable, None],\r\n   133\t        \"verbose\": [\"verbose\"],\r\n   134\t        \"split_sign\": [\"boolean\"],\r\n   135\t        \"random_state\": [\"random_state\"],\r\n   136\t        \"positive_code\": [\"boolean\"],\r\n   137\t        \"positive_dict\": [\"boolean\"],\r\n   138\t        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n   139\t    }\r\n   140\t\r\n   141\t    def __init__(\r\n   142\t        self,\r\n   143\t        n_components=None,\r\n   144\t        *,\r\n   145\t        alpha=1,\r\n   146\t        max_iter=1000,\r\n   147\t        tol=1e-8,\r\n   148\t        fit_algorithm=\"lars\",\r\n   149\t        transform_algorithm=\"omp\",\r\n   150\t        transform_n_nonzero_coefs=None,\r\n   151\t        transform_alpha=None,\r\n   152\t        n_jobs=None,\r\n   153\t        code_init=None,\r\n   154\t        dict_init=None,\r\n   155\t        callback=None,\r\n   156\t        verbose=False,\r\n   157\t        split_sign=False,\r\n   158\t        random_state=None,\r\n   159\t        positive_code=False,\r\n   160\t        positive_dict=False,\r\n   161\t        transform_max_iter=1000,\r\n   162\t    ):\r\n   163\t        super().__init__(\r\n   164\t            transform_algorithm,\r\n   165\t            transform_n_nonzero_coefs,\r\n   166\t            transform_alpha,\r\n   167\t            split_sign,\r\n   168\t            n_jobs,\r\n   169\t            positive_code,\r\n   170\t            transform_max_iter,\r\n   171\t        )\r\n   172\t        self.n_components = n_components\r\n   173\t        self.alpha = alpha\r\n   174\t        self.max_iter = max_iter\r\n   175\t        self.tol = tol\r\n   176\t        self.fit_algorithm = fit_algorithm\r\n   177\t        self.code_init = code_init\r\n   178\t        self.dict_init = dict_init\r\n   179\t        self.callback = callback\r\n   180\t        self.verbose = verbose\r\n   181\t        self.random_state = random_state\r\n   182\t        self.positive_dict = positive_dict\r\n   183\t\r\n   184\t    def fit(self, X, y=None):\r\n   185\t        \"\"\"Fit the model from data in X.\r\n   186\t\r\n   187\t        Parameters\r\n   188\t        ----------\r\n   189\t        X : array-like of shape (n_samples, n_features)\r\n   190\t            Training vector, where `n_samples` is the number of samples\r\n   191\t            and `n_features` is the number of features.\r\n   192\t\r\n   193\t        y : Ignored\r\n   194\t            Not used, present for API consistency by convention.\r\n   195\t\r\n   196\t        Returns\r\n   197\t        -------\r\n   198\t        self : object\r\n   199\t            Returns the instance itself.\r\n   200\t        \"\"\"\r\n   201\t        self.fit_transform(X)\r\n   202\t        return self\r\n   203\t\r\n   204\t    @_fit_context(prefer_skip_nested_validation=True)\r\n   205\t    def fit_transform(self, X, y=None):\r\n   206\t        \"\"\"Fit the model from data in X and return the transformed data.\r\n   207\t\r\n   208\t        Parameters\r\n   209\t        ----------\r\n   210\t        X : array-like of shape (n_samples, n_features)\r\n   211\t            Training vector, where `n_samples` is the number of samples\r\n   212\t            and `n_features` is the number of features.\r\n   213\t\r\n   214\t        y : Ignored\r\n   215\t            Not used, present for API consistency by convention.\r\n   216\t\r\n   217\t        Returns\r\n   218\t        -------\r\n   219\t        V : ndarray of shape (n_samples, n_components)\r\n   220\t            Transformed data.\r\n   221\t        \"\"\"\r\n   222\t        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n   223\t\r\n   224\t        method = \"lasso_\" + self.fit_algorithm\r\n   225\t\r\n   226\t        random_state = check_random_state(self.random_state)\r\n   227\t        X = self._validate_data(X)\r\n   228\t\r\n   229\t        if self.n_components is None:\r\n   230\t            n_components = X.shape[1]\r\n   231\t        else:\r\n   232\t            n_components = self.n_components\r\n   233\t\r\n   234\t        V, U, E, self.n_iter_ = _dict_learning(\r\n   235\t            X,\r\n   236\t            n_components,\r\n   237\t            alpha=self.alpha,\r\n   238\t            tol=self.tol,\r\n   239\t            max_iter=self.max_iter,\r\n   240\t            method=method,\r\n   241\t            method_max_iter=self.transform_max_iter,\r\n   242\t            n_jobs=self.n_jobs,\r\n   243\t            code_init=self.code_init,\r\n   244\t            dict_init=self.dict_init,\r\n   245\t            callback=self.callback,\r\n   246\t            verbose=self.verbose,\r\n   247\t            random_state=random_state,\r\n   248\t            return_n_iter=True,\r\n   249\t            positive_dict=self.positive_dict,\r\n   250\t            positive_code=self.positive_code,\r\n   251\t        )\r\n   252\t        self.components_ = U\r\n   253\t        self.error_ = E\r\n   254\t\r\n   255\t        return V\r\n   256\t\r\n   257\t    @property\r\n   258\t    def _n_features_out(self):\r\n   259\t        \"\"\"Number of transformed output features.\"\"\"\r\n   260\t        return self.components_.shape[0]\r\n"]
[1430.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1430.002, "i", "python - << 'PY'\r"]
[1430.004, "o", "python - << 'PY'\r\n"]
[1430.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import io,sys,re\r\n\u001b[?2004l\r\n\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n\u001b[?2004l\r\n\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Add copy_cov=False at call sites of _sparse_encode_precomputed if not already present\r\n\u001b[?2004l\r\n\u001b[?2004h> pat=r\"(_sparse_encode_precomputed\\([^\\)]*)\\)\"\r\n\u001b[?2004l\r\n\u001b[?2004h> changed=False\r\n\u001b[?2004l\r\n\u001b[?2004h> out=[]\r\n\u001b[?2004l\r\n\u001b[?2004h> i=0\r\n\u001b[?2004l\r\n\u001b[?2004h> while True:\r\n\u001b[?2004l\r\n\u001b[?2004h>     m=re.search(pat,s[i:])\r\n\u001b[?2004l\r\n\u001b[?2004h>     if not m:\r\n\u001b[?2004l\r\n\u001b[?2004h>         break\r\n\u001b[?2004l\r\n\u001b[?2004h>     a=i+m.start(1); b=i+m.end(1)\r\n\u001b[?2004l\r\n\u001b[?2004h>     call=s[a:b]\r\n\u001b[?2004l\r\n\u001b[?2004h>     if 'copy_cov' not in call:\r\n\u001b[?2004l\r\n\u001b[?2004h>         call=call[:-1]+', copy_cov=False)'\r\n\u001b[?2004l\r\n\u001b[?2004h>         s=s[:a]+call+s[b:]\r\n\u001b[?2004l\r\n\u001b[?2004h>         changed=True\r\n\u001b[?2004l\r\n\u001b[?2004h>         i=a+len(call)\r\n\u001b[?2004l\r\n\u001b[?2004h>     else:\r\n\u001b[?2004l\r\n\u001b[?2004h>         i=b\r\n\u001b[?2004l\r\n\u001b[?2004h> if changed:\r\n\u001b[?2004l\r\n\u001b[?2004h>     open(p,'w',encoding='utf-8').write(s)\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('Patched call sites to add copy_cov=False')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('No patch applied (already present or no call found)')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nNo patch applied (already present or no call found)\r\n"]
[1435.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1435.002, "i", "git status -sb\r"]
[1435.004, "o", "git status -sb\r\n"]
[1435.006, "o", "\u001b[?2004l\r\n## \u001b[32mmain\u001b[m\r\n"]
[1440.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1440.002, "i", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1440.004, "o", "git diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1440.006, "o", "\u001b[?2004l\r\n"]
[1445.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1445.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1445.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1445.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1450.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1450.002, "i", "cd asv_benchmarks\r"]
[1450.004, "o", "cd asv_benchmarks\r\n"]
[1450.006, "o", "\u001b[?2004l\r\n"]
[1455.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1455.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1455.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1455.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.69\u00b10.07s \r\n                     cd        1.61\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1460.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1460.002, "i", "cd /workspace/repo\r"]
[1460.004, "o", "cd /workspace/repo\r\n"]
[1460.006, "o", "\u001b[?2004l\r\n"]
[1465.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1465.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1465.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1465.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1470.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1470.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1470.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1470.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1475.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1475.002, "i", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r"]
[1475.004, "o", "sed -n '1,900p' sklearn/decomposition/_dict_learning.py\r\n"]
[1475.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n"]
[1480.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1480.002, "i", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r"]
[1480.004, "o", "sed -n '900,1800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1480.006, "o", "\u001b[?2004l\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n"]
[1485.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1485.002, "i", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1485.004, "o", "sed -n '1800,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1485.006, "o", "\u001b[?2004l\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1490.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1490.002, "i", "grep -R \"_minibatch_step\\s*\\(\" -n sklearn || true\r"]
[1490.004, "o", "grep -R \"_minibatch_step\\s*\\(\" -n sklearn || true\r\n"]
[1490.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1495.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1495.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r"]
[1495.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(\" -n sklearn || true\r\n"]
[1495.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1500.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1500.002, "i", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1500.004, "o", "grep -R \"sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1500.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1505.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1505.002, "i", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r"]
[1505.004, "o", "sed -n '1,400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1505.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n"]
[1510.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1510.002, "i", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1510.004, "o", "sed -n '400,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1510.006, "o", "\u001b[?2004l\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[1515.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1515.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1515.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1515.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[1520.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1520.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1520.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1520.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1525.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1525.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1525.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1525.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1530.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1530.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1530.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1530.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1535.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1535.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1535.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1535.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[1540.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1540.002, "i", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1540.004, "o", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1540.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1545.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1545.002, "i", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r"]
[1545.004, "o", "grep -n \"_minibatch_step\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1545.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2219\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    def \u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(self, X, dictionary, random_state, step):\r\n\u001b[32m\u001b[K2399\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                batch_cost = self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(\r\n\u001b[32m\u001b[K2425\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X_train[batch], dictionary, self._random_state, i)\r\n\u001b[32m\u001b[K2479\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.\u001b[01;31m\u001b[K_minibatch_step\u001b[m\u001b[K(X, dictionary, self._random_state, self.n_steps_)\r\n"]
[1550.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1550.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r"]
[1550.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py || true\r\n"]
[1550.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1555.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1555.002, "i", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py ||\r"]
[1555.004, "o", "grep -n \"def dict_learning_online\\|def _update_dict\\|def _update_dict\\(\" sklearn/decomposition/_dict_learning.py ||\r\n"]
[1555.006, "o", "| true\r\n\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1560.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1560.002, "i", "python - << 'PY'\r"]
[1560.004, "o", "python - << 'PY'\r\n"]
[1560.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> import re\r\n\u001b[?2004l\r\n\u001b[?2004h> p='sklearn/decomposition/_dict_learning.py'\r\n\u001b[?2004l\r\n\u001b[?2004h> s=open(p,'r',encoding='utf-8').read()\r\n\u001b[?2004l\r\n\u001b[?2004h> for name in ['_minibatch_step', 'dict_learning_online', '_update_dict']:\r\n\u001b[?2004l\r\n\u001b[?2004h>     for m in re.finditer(r'\\n\\s*def\\s+'+name+r'\\s*\\(', s):\r\n\u001b[?2004l\r\n\u001b[?2004h>         start=max(0, s.rfind('\\n', 0, m.start())-200)\r\n\u001b[?2004l\r\n\u001b[?2004h>         print('\\nFound', name, 'at index', m.start())\r\n\u001b[?2004l\r\n\u001b[?2004h>         block=s[m.start()-200:m.start()+400]\r\n\u001b[?2004l\r\n\u001b[?2004h>         print(block)\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\n\r\nFound _minibatch_step at index 73956\r\ntch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            posi\r\n\r\nFound dict_learning_online at index 21019\r\ned in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    re\r\n\r\nFound _update_dict at index 14957\r\n=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n    \r\n"]
[1565.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1565.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1565.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1565.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n"]
[1570.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1570.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1570.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1570.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n"]
[1575.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1575.002, "i", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1575.004, "o", "sed -n '1600,2400p' sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1575.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n"]
[1580.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1580.002, "i", "cd /workspace/repo\r"]
[1580.004, "o", "cd /workspace/repo\r\n"]
[1580.006, "o", "\u001b[?2004l\r\n"]
[1585.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1585.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1585.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1585.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1590.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1590.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1590.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1590.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1595.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1595.002, "i", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r"]
[1595.004, "o", "sed -n '1,800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1595.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n"]
[1600.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1600.002, "i", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r"]
[1600.004, "o", "sed -n '800,1600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1600.006, "o", "\u001b[?2004l\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n"]
[1605.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1605.002, "i", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r"]
[1605.004, "o", "sed -n '1600,2600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1605.006, "o", "\u001b[?2004l\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1610.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1610.002, "i", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1610.004, "o", "grep -n \"def _minibatch_step\\|_minibatch_step\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1610.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1615.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1615.002, "i", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1615.004, "o", "grep -n \"_sparse_encode_precomputed\\|sparse_encode\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1615.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1620.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1620.002, "i", "sed -n '2200,3600p' sklearn/decomposition/_dict_learning.py\r"]
[1620.004, "o", "sed -n '2200,3600p' sklearn/decomposition/_dict_learning.py\r\n"]
[1620.006, "o", "\u001b[?2004l\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1625.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1625.002, "i", "sed -n '3600,5200p' sklearn/decomposition/_dict_learning.py\r"]
[1625.004, "o", "sed -n '3600,5200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1625.006, "o", "\u001b[?2004l\r\n"]
[1630.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1630.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1630.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1630.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1635.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1635.002, "i", "grep -n \"class _BaseSparseCoding\\|def transform\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r"]
[1635.004, "o", "grep -n \"class _BaseSparseCoding\\|def transform\\(|def partial_fit\\(\" sklearn/decomposition/_dict_learning.py\r\n"]
[1635.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1640.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1640.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1640.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1640.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1645.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1645.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1645.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1645.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[1650.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1650.002, "i", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r"]
[1650.004, "o", "sed -n '1200,2000p' sklearn/decomposition/_dict_learning.py\r\n"]
[1650.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n"]
[1655.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1655.002, "i", "sed -n '2000,2800p' sklearn/decomposition/_dict_learning.py\r"]
[1655.004, "o", "sed -n '2000,2800p' sklearn/decomposition/_dict_learning.py\r\n"]
[1655.006, "o", "\u001b[?2004l\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1660.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1660.002, "i", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r"]
[1660.004, "o", "grep -R \"def _minibatch_step\\|_minibatch_step\\(\" -n sklearn || true\r\n"]
[1660.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1665.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1665.002, "i", "grep -R \"self\\._A\\|self\\._B\\|_ewa_cost\\|_no_improvement\" -n sklearn/decomposition/_dict_learning.py || true\r"]
[1665.004, "o", "grep -R \"self\\._A\\|self\\._B\\|_ewa_cost\\|_no_improvement\" -n sklearn/decomposition/_dict_learning.py || true\r\n"]
[1665.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K688\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=10,\r\n\u001b[32m\u001b[K832\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K : int, default=10\r\n\u001b[32m\u001b[K838\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        `max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K` to None.\r\n\u001b[32m\u001b[K909\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K2010\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K    max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K : int, default=10\r\n\u001b[32m\u001b[K2016\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        `max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K` to None.\r\n\u001b[32m\u001b[K2111\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \"max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n\u001b[32m\u001b[K2137\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K=10,\r\n\u001b[32m\u001b[K2161\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\r\n\u001b[32m\u001b[K2214\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K *= beta\r\n\u001b[32m\u001b[K2215\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K += code.T @ code / batch_size\r\n\u001b[32m\u001b[K2216\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K *= beta\r\n\u001b[32m\u001b[K2217\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K += X.T @ code / batch_size\r\n\u001b[32m\u001b[K2248\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K2249\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K,\r\n\u001b[32m\u001b[K2267\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K          the max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K parameter.\r\n\u001b[32m\u001b[K2284\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K is None:\r\n\u001b[32m\u001b[K2285\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = batch_cost\r\n\u001b[32m\u001b[K2289\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K * (1 - alpha) + batch_cost * alpha\r\n\u001b[32m\u001b[K2294\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                f\"{batch_cost}, ewa cost: {self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K}\"\r\n\u001b[32m\u001b[K2306\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        if self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min is None or self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K < self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min:\r\n\u001b[32m\u001b[K2307\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = 0\r\n\u001b[32m\u001b[K2308\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min = self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K\r\n\u001b[32m\u001b[K2310\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K += 1\r\n\u001b[32m\u001b[K2313\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K is not None\r\n\u001b[32m\u001b[K2314\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            and self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K >= self.max\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K\r\n\u001b[32m\u001b[K2378\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K = np.zeros(\r\n\u001b[32m\u001b[K2381\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\u001b[32m\u001b[K2385\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K = None\r\n\u001b[32m\u001b[K2386\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_ewa_cost\u001b[m\u001b[K_min = None\r\n\u001b[32m\u001b[K2387\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            self.\u001b[01;31m\u001b[K_no_improvement\u001b[m\u001b[K = 0\r\n\u001b[32m\u001b[K2474\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._A\u001b[m\u001b[K = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n\u001b[32m\u001b[K2475\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K            \u001b[01;31m\u001b[Kself._B\u001b[m\u001b[K = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n"]
[1670.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1670.002, "i", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r"]
[1670.004, "o", "grep -R \"_sparse_encode_precomputed\\s*\\(|sparse_encode\\s*\\(\" -n sklearn/decomposition || true\r\n"]
[1670.006, "o", "\u001b[?2004l\r\ngrep: Unmatched ( or \\(\r\n"]
[1675.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1675.002, "i", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r"]
[1675.004, "o", "sed -n '1,1200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1675.006, "o", "\u001b[?2004l\r\n\"\"\" Dictionary learning.\r\n\"\"\"\r\n# Author: Vlad Niculae, Gael Varoquaux, Alexandre Gramfort\r\n# License: BSD 3 clause\r\n\r\nimport itertools\r\nimport sys\r\nimport time\r\nimport warnings\r\nfrom math import ceil\r\nfrom numbers import Integral, Real\r\n\r\nimport numpy as np\r\nfrom joblib import effective_n_jobs\r\nfrom scipy import linalg\r\n\r\nfrom ..base import (\r\n    BaseEstimator,\r\n    ClassNamePrefixFeaturesOutMixin,\r\n    TransformerMixin,\r\n    _fit_context,\r\n)\r\nfrom ..linear_model import Lars, Lasso, LassoLars, orthogonal_mp_gram\r\nfrom ..utils import check_array, check_random_state, gen_batches, gen_even_slices\r\nfrom ..utils._param_validation import Hidden, Interval, StrOptions, validate_params\r\nfrom ..utils.extmath import randomized_svd, row_norms, svd_flip\r\nfrom ..utils.parallel import Parallel, delayed\r\nfrom ..utils.validation import check_is_fitted\r\n\r\n\r\ndef _check_positive_coding(method, positive):\r\n    if positive and method in [\"omp\", \"lars\"]:\r\n        raise ValueError(\r\n            \"Positive constraint not supported for '{}' coding method.\".format(method)\r\n        )\r\n\r\n\r\ndef _sparse_encode_precomputed(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    regularization=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Generic sparse coding with precomputed Gram and/or covariance matrices.\r\n\r\n    Each row of the result is the solution to a Lasso problem.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows.\r\n\r\n    gram : ndarray of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`\r\n        gram can be `None` if method is 'threshold'.\r\n\r\n    cov : ndarray of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary * X'`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    regularization : int or float, default=None\r\n        The regularization parameter. It corresponds to alpha when\r\n        algorithm is `'lasso_lars'`, `'lasso_cd'` or `'threshold'`.\r\n        Otherwise it corresponds to `n_nonzero_coefs`.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse code. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive: bool, default=False\r\n        Whether to enforce a positivity constraint on the sparse code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_components, n_features)\r\n        The sparse codes.\r\n    \"\"\"\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm == \"lasso_lars\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lasso_lars = LassoLars(\r\n                alpha=alpha,\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                fit_path=False,\r\n                positive=positive,\r\n                max_iter=max_iter,\r\n            )\r\n            lasso_lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lasso_lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"lasso_cd\":\r\n        alpha = float(regularization) / n_features  # account for scaling\r\n\r\n        # TODO: Make verbosity argument for Lasso?\r\n        # sklearn.linear_model.coordinate_descent.enet_path has a verbosity\r\n        # argument that we could pass in from Lasso.\r\n        clf = Lasso(\r\n            alpha=alpha,\r\n            fit_intercept=False,\r\n            precompute=gram,\r\n            max_iter=max_iter,\r\n            warm_start=True,\r\n            positive=positive,\r\n        )\r\n\r\n        if init is not None:\r\n            # In some workflows using coordinate descent algorithms:\r\n            #  - users might provide NumPy arrays with read-only buffers\r\n            #  - `joblib` might memmap arrays making their buffer read-only\r\n            # TODO: move this handling (which is currently too broad)\r\n            # closer to the actual private function which need buffers to be writable.\r\n            if not init.flags[\"WRITEABLE\"]:\r\n                init = np.array(init)\r\n            clf.coef_ = init\r\n\r\n        clf.fit(dictionary.T, X.T, check_input=False)\r\n        new_code = clf.coef_\r\n\r\n    elif algorithm == \"lars\":\r\n        try:\r\n            err_mgt = np.seterr(all=\"ignore\")\r\n\r\n            # Not passing in verbose=max(0, verbose-1) because Lars.fit already\r\n            # corrects the verbosity level.\r\n            lars = Lars(\r\n                fit_intercept=False,\r\n                verbose=verbose,\r\n                precompute=gram,\r\n                n_nonzero_coefs=int(regularization),\r\n                fit_path=False,\r\n            )\r\n            lars.fit(dictionary.T, X.T, Xy=cov)\r\n            new_code = lars.coef_\r\n        finally:\r\n            np.seterr(**err_mgt)\r\n\r\n    elif algorithm == \"threshold\":\r\n        new_code = (np.sign(cov) * np.maximum(np.abs(cov) - regularization, 0)).T\r\n        if positive:\r\n            np.clip(new_code, 0, None, out=new_code)\r\n\r\n    elif algorithm == \"omp\":\r\n        new_code = orthogonal_mp_gram(\r\n            Gram=gram,\r\n            Xy=cov,\r\n            n_nonzero_coefs=int(regularization),\r\n            tol=None,\r\n            norms_squared=row_norms(X, squared=True),\r\n            copy_Xy=copy_cov,\r\n        ).T\r\n\r\n    return new_code.reshape(n_samples, n_components)\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"dictionary\": [\"array-like\"],\r\n        \"gram\": [\"array-like\", None],\r\n        \"cov\": [\"array-like\", None],\r\n        \"algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"copy_cov\": [\"boolean\"],\r\n        \"init\": [\"array-like\", None],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"n_jobs\": [Integral, None],\r\n        \"check_input\": [\"boolean\"],\r\n        \"verbose\": [\"verbose\"],\r\n        \"positive\": [\"boolean\"],\r\n    },\r\n    prefer_skip_nested_validation=True,\r\n)\r\n# XXX : could be moved to the linear_model module\r\ndef sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    check_input=True,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    dictionary : array-like of shape (n_components, n_features)\r\n        The dictionary matrix against which to solve the sparse coding of\r\n        the data. Some of the algorithms assume normalized rows for meaningful\r\n        output.\r\n\r\n    gram : array-like of shape (n_components, n_components), default=None\r\n        Precomputed Gram matrix, `dictionary * dictionary'`.\r\n\r\n    cov : array-like of shape (n_components, n_samples), default=None\r\n        Precomputed covariance, `dictionary' * X`.\r\n\r\n    algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'}, \\\r\n            default='lasso_lars'\r\n        The algorithm used:\r\n\r\n        * `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        * `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        * `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). lasso_lars will be faster if\r\n          the estimated components are sparse;\r\n        * `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        * `'threshold'`: squashes to zero all coefficients less than\r\n          regularization from the projection `dictionary * data'`.\r\n\r\n    n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    copy_cov : bool, default=True\r\n        Whether to copy the precomputed covariance matrix; if `False`, it may\r\n        be overwritten.\r\n\r\n    init : ndarray of shape (n_samples, n_components), default=None\r\n        Initialization value of the sparse codes. Only used if\r\n        `algorithm='lasso_cd'`.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    check_input : bool, default=True\r\n        If `False`, the input arrays X and dictionary will not be checked.\r\n\r\n    verbose : int, default=0\r\n        Controls the verbosity; the higher, the more messages.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the encoding.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse codes.\r\n\r\n    See Also\r\n    --------\r\n    sklearn.linear_model.lars_path : Compute Least Angle Regression or Lasso\r\n        path using LARS algorithm.\r\n    sklearn.linear_model.orthogonal_mp : Solves Orthogonal Matching Pursuit problems.\r\n    sklearn.linear_model.Lasso : Train Linear Model with L1 prior as regularizer.\r\n    SparseCoder : Find a sparse representation of data from a fixed precomputed\r\n        dictionary.\r\n    \"\"\"\r\n    if check_input:\r\n        if algorithm == \"lasso_cd\":\r\n            dictionary = check_array(\r\n                dictionary, order=\"C\", dtype=[np.float64, np.float32]\r\n            )\r\n            X = check_array(X, order=\"C\", dtype=[np.float64, np.float32])\r\n        else:\r\n            dictionary = check_array(dictionary)\r\n            X = check_array(X)\r\n\r\n    if dictionary.shape[1] != X.shape[1]:\r\n        raise ValueError(\r\n            \"Dictionary and X have different numbers of features:\"\r\n            \"dictionary.shape: {} X.shape{}\".format(dictionary.shape, X.shape)\r\n        )\r\n\r\n    _check_positive_coding(algorithm, positive)\r\n\r\n    return _sparse_encode(\r\n        X,\r\n        dictionary,\r\n        gram=gram,\r\n        cov=cov,\r\n        algorithm=algorithm,\r\n        n_nonzero_coefs=n_nonzero_coefs,\r\n        alpha=alpha,\r\n        copy_cov=copy_cov,\r\n        init=init,\r\n        max_iter=max_iter,\r\n        n_jobs=n_jobs,\r\n        verbose=verbose,\r\n        positive=positive,\r\n    )\r\n\r\n\r\ndef _sparse_encode(\r\n    X,\r\n    dictionary,\r\n    *,\r\n    gram=None,\r\n    cov=None,\r\n    algorithm=\"lasso_lars\",\r\n    n_nonzero_coefs=None,\r\n    alpha=None,\r\n    copy_cov=True,\r\n    init=None,\r\n    max_iter=1000,\r\n    n_jobs=None,\r\n    verbose=0,\r\n    positive=False,\r\n):\r\n    \"\"\"Sparse coding without input/parameter validation.\"\"\"\r\n\r\n    n_samples, n_features = X.shape\r\n    n_components = dictionary.shape[0]\r\n\r\n    if algorithm in (\"lars\", \"omp\"):\r\n        regularization = n_nonzero_coefs\r\n        if regularization is None:\r\n            regularization = min(max(n_features / 10, 1), n_components)\r\n    else:\r\n        regularization = alpha\r\n        if regularization is None:\r\n            regularization = 1.0\r\n\r\n    if gram is None and algorithm != \"threshold\":\r\n        gram = np.dot(dictionary, dictionary.T)\r\n\r\n    if cov is None and algorithm != \"lasso_cd\":\r\n        copy_cov = False\r\n        cov = np.dot(dictionary, X.T)\r\n\r\n    if effective_n_jobs(n_jobs) == 1 or algorithm == \"threshold\":\r\n        code = _sparse_encode_precomputed(\r\n            X,\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        return code\r\n\r\n    # Enter parallel code block\r\n    n_samples = X.shape[0]\r\n    n_components = dictionary.shape[0]\r\n    code = np.empty((n_samples, n_components))\r\n    slices = list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))\r\n\r\n    code_views = Parallel(n_jobs=n_jobs, verbose=verbose)(\r\n        delayed(_sparse_encode_precomputed)(\r\n            X[this_slice],\r\n            dictionary,\r\n            gram=gram,\r\n            cov=cov[:, this_slice] if cov is not None else None,\r\n            algorithm=algorithm,\r\n            regularization=regularization,\r\n            copy_cov=copy_cov,\r\n            init=init[this_slice] if init is not None else None,\r\n            max_iter=max_iter,\r\n            verbose=verbose,\r\n            positive=positive,\r\n        )\r\n        for this_slice in slices\r\n    )\r\n    for this_slice, this_view in zip(slices, code_views):\r\n        code[this_slice] = this_view\r\n    return code\r\n\r\n\r\ndef _update_dict(\r\n    dictionary,\r\n    Y,\r\n    code,\r\n    A=None,\r\n    B=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    positive=False,\r\n):\r\n    \"\"\"Update the dense dictionary factor in place.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        Value of the dictionary at the previous iteration.\r\n\r\n    Y : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    code : ndarray of shape (n_samples, n_components)\r\n        Sparse coding of the data against which to optimize the dictionary.\r\n\r\n    A : ndarray of shape (n_components, n_components), default=None\r\n        Together with `B`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    B : ndarray of shape (n_features, n_components), default=None\r\n        Together with `A`, sufficient stats of the online model to update the\r\n        dictionary.\r\n\r\n    verbose: bool, default=False\r\n        Degree of output the procedure will print.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n    \"\"\"\r\n    n_samples, n_components = code.shape\r\n    random_state = check_random_state(random_state)\r\n\r\n    if A is None:\r\n        A = code.T @ code\r\n    if B is None:\r\n        B = Y.T @ code\r\n\r\n    n_unused = 0\r\n\r\n    for k in range(n_components):\r\n        if A[k, k] > 1e-6:\r\n            # 1e-6 is arbitrary but consistent with the spams implementation\r\n            dictionary[k] += (B[:, k] - A[k] @ dictionary) / A[k, k]\r\n        else:\r\n            # kth atom is almost never used -> sample a new one from the data\r\n            newd = Y[random_state.choice(n_samples)]\r\n\r\n            # add small noise to avoid making the sparse coding ill conditioned\r\n            noise_level = 0.01 * (newd.std() or 1)  # avoid 0 std\r\n            noise = random_state.normal(0, noise_level, size=len(newd))\r\n\r\n            dictionary[k] = newd + noise\r\n            code[:, k] = 0\r\n            n_unused += 1\r\n\r\n        if positive:\r\n            np.clip(dictionary[k], 0, None, out=dictionary[k])\r\n\r\n        # Projection on the constraint set ||V_k|| <= 1\r\n        dictionary[k] /= max(linalg.norm(dictionary[k]), 1)\r\n\r\n    if verbose and n_unused > 0:\r\n        print(f\"{n_unused} unused atoms resampled.\")\r\n\r\n\r\ndef _dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter,\r\n    tol,\r\n    method,\r\n    n_jobs,\r\n    dict_init,\r\n    code_init,\r\n    callback,\r\n    verbose,\r\n    random_state,\r\n    return_n_iter,\r\n    positive_dict,\r\n    positive_code,\r\n    method_max_iter,\r\n):\r\n    \"\"\"Main dictionary learning algorithm\"\"\"\r\n    t0 = time.time()\r\n    # Init the code and the dictionary with SVD of Y\r\n    if code_init is not None and dict_init is not None:\r\n        code = np.array(code_init, order=\"F\")\r\n        # Don't copy V, it will happen below\r\n        dictionary = dict_init\r\n    else:\r\n        code, S, dictionary = linalg.svd(X, full_matrices=False)\r\n        # flip the initial code's sign to enforce deterministic output\r\n        code, dictionary = svd_flip(code, dictionary)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:  # True even if n_components=None\r\n        code = code[:, :n_components]\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        code = np.c_[code, np.zeros((len(code), n_components - r))]\r\n        dictionary = np.r_[\r\n            dictionary, np.zeros((n_components - r, dictionary.shape[1]))\r\n        ]\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = np.asfortranarray(dictionary)\r\n\r\n    errors = []\r\n    current_cost = np.nan\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    # If max_iter is 0, number of iterations returned should be zero\r\n    ii = -1\r\n\r\n    for ii in range(max_iter):\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            print(\r\n                \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn, current cost % 7.3f)\"\r\n                % (ii, dt, dt / 60, current_cost)\r\n            )\r\n\r\n        # Update code\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            init=code,\r\n            n_jobs=n_jobs,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Cost function\r\n        current_cost = 0.5 * np.sum((X - code @ dictionary) ** 2) + alpha * np.sum(\r\n            np.abs(code)\r\n        )\r\n        errors.append(current_cost)\r\n\r\n        if ii > 0:\r\n            dE = errors[-2] - errors[-1]\r\n            # assert(dE >= -tol * errors[-1])\r\n            if dE < tol * errors[-1]:\r\n                if verbose == 1:\r\n                    # A line return\r\n                    print(\"\")\r\n                elif verbose:\r\n                    print(\"--- Convergence reached after %d iterations\" % ii)\r\n                break\r\n        if ii % 5 == 0 and callback is not None:\r\n            callback(locals())\r\n\r\n    if return_n_iter:\r\n        return code, dictionary, errors, ii + 1\r\n    else:\r\n        return code, dictionary, errors\r\n\r\n\r\ndef _check_warn_deprecated(param, name, default, additional_message=None):\r\n    if param != \"deprecated\":\r\n        msg = (\r\n            f\"'{name}' is deprecated in version 1.1 and will be removed in version 1.4.\"\r\n        )\r\n        if additional_message:\r\n            msg += f\" {additional_message}\"\r\n        warnings.warn(msg, FutureWarning)\r\n        return param\r\n    else:\r\n        return default\r\n\r\n\r\ndef dict_learning_online(\r\n    X,\r\n    n_components=2,\r\n    *,\r\n    alpha=1,\r\n    n_iter=\"deprecated\",\r\n    max_iter=None,\r\n    return_code=True,\r\n    dict_init=None,\r\n    callback=None,\r\n    batch_size=256,\r\n    verbose=False,\r\n    shuffle=True,\r\n    n_jobs=None,\r\n    method=\"lars\",\r\n    iter_offset=\"deprecated\",\r\n    random_state=None,\r\n    return_inner_stats=\"deprecated\",\r\n    inner_stats=\"deprecated\",\r\n    return_n_iter=\"deprecated\",\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n    tol=1e-3,\r\n    max_no_improvement=10,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem online.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                     with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n    This is accomplished by repeatedly iterating over mini-batches by slicing\r\n    the input data.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : ndarray of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int or None, default=2\r\n        Number of dictionary atoms to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=100\r\n        Number of mini-batch iterations to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           `n_iter` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           `max_iter` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    return_code : bool, default=True\r\n        Whether to also return the code U or just the dictionary `V`.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary for warm restart scenarios.\r\n        If `None`, the initial values for the dictionary are created\r\n        with an SVD decomposition of the data via :func:`~sklearn.utils.randomized_svd`.\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n    batch_size : int, default=256\r\n        The number of samples to take in each batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the data before splitting it in batches.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    iter_offset : int, default=0\r\n        Number of previous iterations completed on the dictionary used for\r\n        initialization.\r\n\r\n        .. deprecated:: 1.1\r\n           `iter_offset` serves internal purpose only and will be removed in 1.4.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_inner_stats : bool, default=False\r\n        Return the inner statistics A (dictionary covariance) and B\r\n        (data approximation). Useful to restart the algorithm in an\r\n        online setting. If `return_inner_stats` is `True`, `return_code` is\r\n        ignored.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    inner_stats : tuple of (A, B) ndarrays, default=None\r\n        Inner sufficient statistics that are kept by the algorithm.\r\n        Passing them at initialization is useful in online settings, to\r\n        avoid losing the history of the evolution.\r\n        `A` `(n_components, n_components)` is the dictionary covariance matrix.\r\n        `B` `(n_features, n_components)` is the data approximation matrix.\r\n\r\n        .. deprecated:: 1.1\r\n           `inner_stats` serves internal purpose only and will be removed in 1.4.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n        .. deprecated:: 1.1\r\n           `return_n_iter` will be removed in 1.4 and n_iter will never be returned.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform when solving the lasso problem.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components),\r\n        The sparse code (only returned if `return_code=True`).\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The solutions to the dictionary learning problem.\r\n\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to `True`.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning : Solve a dictionary learning matrix factorization problem.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the dictionary\r\n        learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    deps = (return_n_iter, return_inner_stats, iter_offset, inner_stats)\r\n    if max_iter is not None and not all(arg == \"deprecated\" for arg in deps):\r\n        raise ValueError(\r\n            \"The following arguments are incompatible with 'max_iter': \"\r\n            \"return_n_iter, return_inner_stats, iter_offset, inner_stats\"\r\n        )\r\n\r\n    iter_offset = _check_warn_deprecated(iter_offset, \"iter_offset\", default=0)\r\n    return_inner_stats = _check_warn_deprecated(\r\n        return_inner_stats,\r\n        \"return_inner_stats\",\r\n        default=False,\r\n        additional_message=\"From 1.4 inner_stats will never be returned.\",\r\n    )\r\n    inner_stats = _check_warn_deprecated(inner_stats, \"inner_stats\", default=None)\r\n    return_n_iter = _check_warn_deprecated(\r\n        return_n_iter,\r\n        \"return_n_iter\",\r\n        default=False,\r\n        additional_message=(\r\n            \"From 1.4 'n_iter' will never be returned. Refer to the 'n_iter_' and \"\r\n            \"'n_steps_' attributes of the MiniBatchDictionaryLearning object instead.\"\r\n        ),\r\n    )\r\n\r\n    if max_iter is not None:\r\n        transform_algorithm = \"lasso_\" + method\r\n\r\n        est = MiniBatchDictionaryLearning(\r\n            n_components=n_components,\r\n            alpha=alpha,\r\n            n_iter=n_iter,\r\n            n_jobs=n_jobs,\r\n            fit_algorithm=method,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle,\r\n            dict_init=dict_init,\r\n            random_state=random_state,\r\n            transform_algorithm=transform_algorithm,\r\n            transform_alpha=alpha,\r\n            positive_code=positive_code,\r\n            positive_dict=positive_dict,\r\n            transform_max_iter=method_max_iter,\r\n            verbose=verbose,\r\n            callback=callback,\r\n            tol=tol,\r\n            max_no_improvement=max_no_improvement,\r\n        ).fit(X)\r\n\r\n        if not return_code:\r\n            return est.components_\r\n        else:\r\n            code = est.transform(X)\r\n            return code, est.components_\r\n\r\n    # TODO(1.4) remove the whole old behavior\r\n    # Fallback to old behavior\r\n\r\n    n_iter = _check_warn_deprecated(\r\n        n_iter, \"n_iter\", default=100, additional_message=\"Use 'max_iter' instead.\"\r\n    )\r\n\r\n    if n_components is None:\r\n        n_components = X.shape[1]\r\n\r\n    if method not in (\"lars\", \"cd\"):\r\n        raise ValueError(\"Coding method not supported as a fit algorithm.\")\r\n\r\n    _check_positive_coding(method, positive_code)\r\n\r\n    method = \"lasso_\" + method\r\n\r\n    t0 = time.time()\r\n    n_samples, n_features = X.shape\r\n    # Avoid integer division problems\r\n    alpha = float(alpha)\r\n    random_state = check_random_state(random_state)\r\n\r\n    # Init V with SVD of X\r\n    if dict_init is not None:\r\n        dictionary = dict_init\r\n    else:\r\n        _, S, dictionary = randomized_svd(X, n_components, random_state=random_state)\r\n        dictionary = S[:, np.newaxis] * dictionary\r\n    r = len(dictionary)\r\n    if n_components <= r:\r\n        dictionary = dictionary[:n_components, :]\r\n    else:\r\n        dictionary = np.r_[\r\n            dictionary,\r\n            np.zeros((n_components - r, dictionary.shape[1]), dtype=dictionary.dtype),\r\n        ]\r\n\r\n    if verbose == 1:\r\n        print(\"[dict_learning]\", end=\" \")\r\n\r\n    if shuffle:\r\n        X_train = X.copy()\r\n        random_state.shuffle(X_train)\r\n    else:\r\n        X_train = X\r\n\r\n    X_train = check_array(\r\n        X_train, order=\"C\", dtype=[np.float64, np.float32], copy=False\r\n    )\r\n\r\n    # Fortran-order dict better suited for the sparse coding which is the\r\n    # bottleneck of this algorithm.\r\n    dictionary = check_array(dictionary, order=\"F\", dtype=X_train.dtype, copy=False)\r\n    dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n    batches = gen_batches(n_samples, batch_size)\r\n    batches = itertools.cycle(batches)\r\n\r\n    # The covariance of the dictionary\r\n    if inner_stats is None:\r\n        A = np.zeros((n_components, n_components), dtype=X_train.dtype)\r\n        # The data approximation\r\n        B = np.zeros((n_features, n_components), dtype=X_train.dtype)\r\n    else:\r\n        A = inner_stats[0].copy()\r\n        B = inner_stats[1].copy()\r\n\r\n    # If n_iter is zero, we need to return zero.\r\n    ii = iter_offset - 1\r\n\r\n    for ii, batch in zip(range(iter_offset, iter_offset + n_iter), batches):\r\n        this_X = X_train[batch]\r\n        dt = time.time() - t0\r\n        if verbose == 1:\r\n            sys.stdout.write(\".\")\r\n            sys.stdout.flush()\r\n        elif verbose:\r\n            if verbose > 10 or ii % ceil(100.0 / verbose) == 0:\r\n                print(\r\n                    \"Iteration % 3i (elapsed time: % 3is, % 4.1fmn)\" % (ii, dt, dt / 60)\r\n                )\r\n\r\n        this_code = sparse_encode(\r\n            this_X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n\r\n        # Update the auxiliary variables\r\n        if ii < batch_size - 1:\r\n            theta = float((ii + 1) * batch_size)\r\n        else:\r\n            theta = float(batch_size**2 + ii + 1 - batch_size)\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        A *= beta\r\n        A += np.dot(this_code.T, this_code)\r\n        B *= beta\r\n        B += np.dot(this_X.T, this_code)\r\n\r\n        # Update dictionary in place\r\n        _update_dict(\r\n            dictionary,\r\n            this_X,\r\n            this_code,\r\n            A,\r\n            B,\r\n            verbose=verbose,\r\n            random_state=random_state,\r\n            positive=positive_dict,\r\n        )\r\n\r\n        # Maybe we need a stopping criteria based on the amount of\r\n        # modification in the dictionary\r\n        if callback is not None:\r\n            callback(locals())\r\n\r\n    if return_inner_stats:\r\n        if return_n_iter:\r\n            return dictionary, (A, B), ii - iter_offset + 1\r\n        else:\r\n            return dictionary, (A, B)\r\n    if return_code:\r\n        if verbose > 1:\r\n            print(\"Learning code...\", end=\" \")\r\n        elif verbose == 1:\r\n            print(\"|\", end=\" \")\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=method,\r\n            alpha=alpha,\r\n            n_jobs=n_jobs,\r\n            check_input=False,\r\n            positive=positive_code,\r\n            max_iter=method_max_iter,\r\n            verbose=verbose,\r\n        )\r\n        if verbose > 1:\r\n            dt = time.time() - t0\r\n            print(\"done (total time: % 3is, % 4.1fmn)\" % (dt, dt / 60))\r\n        if return_n_iter:\r\n            return code, dictionary, ii - iter_offset + 1\r\n        else:\r\n            return code, dictionary\r\n\r\n    if return_n_iter:\r\n        return dictionary, ii - iter_offset + 1\r\n    else:\r\n        return dictionary\r\n\r\n\r\n@validate_params(\r\n    {\r\n        \"X\": [\"array-like\"],\r\n        \"method\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"return_n_iter\": [\"boolean\"],\r\n        \"method_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    },\r\n    prefer_skip_nested_validation=False,\r\n)\r\ndef dict_learning(\r\n    X,\r\n    n_components,\r\n    *,\r\n    alpha,\r\n    max_iter=100,\r\n    tol=1e-8,\r\n    method=\"lars\",\r\n    n_jobs=None,\r\n    dict_init=None,\r\n    code_init=None,\r\n    callback=None,\r\n    verbose=False,\r\n    random_state=None,\r\n    return_n_iter=False,\r\n    positive_dict=False,\r\n    positive_code=False,\r\n    method_max_iter=1000,\r\n):\r\n    \"\"\"Solve a dictionary learning matrix factorization problem.\r\n\r\n    Finds the best dictionary and the corresponding sparse code for\r\n    approximating the data matrix X by solving::\r\n\r\n        (U^*, V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                     (U,V)\r\n                    with || V_k ||_2 = 1 for all  0 <= k < n_components\r\n\r\n    where V is the dictionary and U is the sparse code. ||.||_Fro stands for\r\n    the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm\r\n    which is the sum of the absolute values of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    X : array-like of shape (n_samples, n_features)\r\n        Data matrix.\r\n\r\n    n_components : int\r\n        Number of dictionary atoms to extract.\r\n\r\n    alpha : int or float\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=100\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for the stopping condition.\r\n\r\n    method : {'lars', 'cd'}, default='lars'\r\n        The method used:\r\n\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n           problem (`linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value for the dictionary for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the sparse code for warm restart scenarios. Only used\r\n        if `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for randomly initializing the dictionary. Pass an int for\r\n        reproducible results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    return_n_iter : bool, default=False\r\n        Whether or not to return the number of iterations.\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    method_max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Returns\r\n    -------\r\n    code : ndarray of shape (n_samples, n_components)\r\n        The sparse code factor in the matrix factorization.\r\n\r\n    dictionary : ndarray of shape (n_components, n_features),\r\n        The dictionary factor in the matrix factorization.\r\n\r\n    errors : array\r\n        Vector of errors at each iteration.\r\n\r\n    n_iter : int\r\n"]
[1680.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1680.002, "i", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r"]
[1680.004, "o", "sed -n '1200,2200p' sklearn/decomposition/_dict_learning.py\r\n"]
[1680.006, "o", "\u001b[?2004l\r\n    n_iter : int\r\n        Number of iterations run. Returned only if `return_n_iter` is\r\n        set to True.\r\n\r\n    See Also\r\n    --------\r\n    dict_learning_online : Solve a dictionary learning matrix factorization\r\n        problem online.\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate version\r\n        of the dictionary learning algorithm.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    \"\"\"\r\n    estimator = DictionaryLearning(\r\n        n_components=n_components,\r\n        alpha=alpha,\r\n        max_iter=max_iter,\r\n        tol=tol,\r\n        fit_algorithm=method,\r\n        n_jobs=n_jobs,\r\n        dict_init=dict_init,\r\n        callback=callback,\r\n        code_init=code_init,\r\n        verbose=verbose,\r\n        random_state=random_state,\r\n        positive_code=positive_code,\r\n        positive_dict=positive_dict,\r\n        transform_max_iter=method_max_iter,\r\n    )\r\n    code = estimator.fit_transform(X)\r\n    if return_n_iter:\r\n        return (\r\n            code,\r\n            estimator.components_,\r\n            estimator.error_,\r\n            estimator.n_iter_,\r\n        )\r\n    return code, estimator.components_, estimator.error_\r\n\r\n\r\nclass _BaseSparseCoding(ClassNamePrefixFeaturesOutMixin, TransformerMixin):\r\n    \"\"\"Base class from SparseCoder and DictionaryLearning algorithms.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        transform_algorithm,\r\n        transform_n_nonzero_coefs,\r\n        transform_alpha,\r\n        split_sign,\r\n        n_jobs,\r\n        positive_code,\r\n        transform_max_iter,\r\n    ):\r\n        self.transform_algorithm = transform_algorithm\r\n        self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\r\n        self.transform_alpha = transform_alpha\r\n        self.transform_max_iter = transform_max_iter\r\n        self.split_sign = split_sign\r\n        self.n_jobs = n_jobs\r\n        self.positive_code = positive_code\r\n\r\n    def _transform(self, X, dictionary):\r\n        \"\"\"Private method allowing to accommodate both DictionaryLearning and\r\n        SparseCoder.\"\"\"\r\n        X = self._validate_data(X, reset=False)\r\n\r\n        if hasattr(self, \"alpha\") and self.transform_alpha is None:\r\n            transform_alpha = self.alpha\r\n        else:\r\n            transform_alpha = self.transform_alpha\r\n\r\n        code = sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self.transform_algorithm,\r\n            n_nonzero_coefs=self.transform_n_nonzero_coefs,\r\n            alpha=transform_alpha,\r\n            max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n        )\r\n\r\n        if self.split_sign:\r\n            # feature vector is split into a positive and negative side\r\n            n_samples, n_features = code.shape\r\n            split_code = np.empty((n_samples, 2 * n_features))\r\n            split_code[:, :n_features] = np.maximum(code, 0)\r\n            split_code[:, n_features:] = -np.minimum(code, 0)\r\n            code = split_code\r\n\r\n        return code\r\n\r\n    def transform(self, X):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Test data to be transformed, must have the same number of\r\n            features as the data used to train the model.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        check_is_fitted(self)\r\n        return self._transform(X, self.components_)\r\n\r\n\r\nclass SparseCoder(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Sparse coding.\r\n\r\n    Finds a sparse representation of data against a fixed, precomputed\r\n    dictionary.\r\n\r\n    Each row of the result is the solution to a sparse coding problem.\r\n    The goal is to find a sparse array `code` such that::\r\n\r\n        X ~= code * dictionary\r\n\r\n    Read more in the :ref:`User Guide <SparseCoder>`.\r\n\r\n    Parameters\r\n    ----------\r\n    dictionary : ndarray of shape (n_components, n_features)\r\n        The dictionary atoms used for sparse coding. Lines are assumed to be\r\n        normalized to unit norm.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution;\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if\r\n          the estimated components are sparse;\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution;\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and `algorithm='omp'`\r\n        and is overridden by `alpha` in the `omp` case. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of\r\n        the reconstruction error targeted. In this case, it overrides\r\n        `n_nonzero_coefs`.\r\n        If `None`, default to 1.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `lasso_lars`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    n_components_ : int\r\n        Number of atoms.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchDictionaryLearning : A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n    sparse_encode : Sparse coding where each row of the result is the solution\r\n        to a sparse coding problem.\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.decomposition import SparseCoder\r\n    >>> X = np.array([[-1, -1, -1], [0, 0, 3]])\r\n    >>> dictionary = np.array(\r\n    ...     [[0, 1, 0],\r\n    ...      [-1, -1, 2],\r\n    ...      [1, 1, 1],\r\n    ...      [0, 1, 1],\r\n    ...      [0, 2, 1]],\r\n    ...    dtype=np.float64\r\n    ... )\r\n    >>> coder = SparseCoder(\r\n    ...     dictionary=dictionary, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=1e-10,\r\n    ... )\r\n    >>> coder.transform(X)\r\n    array([[ 0.,  0., -1.,  0.,  0.],\r\n           [ 0.,  1.,  1.,  0.,  0.]])\r\n    \"\"\"\r\n\r\n    _required_parameters = [\"dictionary\"]\r\n\r\n    def __init__(\r\n        self,\r\n        dictionary,\r\n        *,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        split_sign=False,\r\n        n_jobs=None,\r\n        positive_code=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.dictionary = dictionary\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Do nothing and return the estimator unchanged.\r\n\r\n        This method is just there to implement the usual API and hence\r\n        work in pipelines.\r\n\r\n        Parameters\r\n        ----------\r\n        X : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        \"\"\"Encode the data as a sparse combination of the dictionary atoms.\r\n\r\n        Coding method is determined by the object parameter\r\n        `transform_algorithm`.\r\n\r\n        Parameters\r\n        ----------\r\n        X : ndarray of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        X_new : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        return super()._transform(X, self.dictionary)\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"requires_fit\": False,\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n    @property\r\n    def n_components_(self):\r\n        \"\"\"Number of atoms.\"\"\"\r\n        return self.dictionary.shape[0]\r\n\r\n    @property\r\n    def n_features_in_(self):\r\n        \"\"\"Number of features seen during `fit`.\"\"\"\r\n        return self.dictionary.shape[1]\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.n_components_\r\n\r\n\r\nclass DictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n        (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract. If None, then ``n_components``\r\n        is set to ``n_features``.\r\n\r\n    alpha : float, default=1.0\r\n        Sparsity controlling parameter.\r\n\r\n    max_iter : int, default=1000\r\n        Maximum number of iterations to perform.\r\n\r\n    tol : float, default=1e-8\r\n        Tolerance for numerical error.\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        * `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (:func:`~sklearn.linear_model.lars_path`);\r\n        * `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be\r\n          faster if the estimated components are sparse.\r\n\r\n        .. versionadded:: 0.17\r\n           *cd* coordinate descent method to improve speed.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (:func:`~sklearn.linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`\r\n          will be faster if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n        .. versionadded:: 0.17\r\n           *lasso_cd* coordinate descent method to improve speed.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    n_jobs : int or None, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    code_init : ndarray of shape (n_samples, n_components), default=None\r\n        Initial value for the code, for warm restart. Only used if `code_init`\r\n        and `dict_init` are not None.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial values for the dictionary, for warm restart. Only used if\r\n        `code_init` and `dict_init` are not None.\r\n\r\n    callback : callable, default=None\r\n        Callable that gets invoked every five iterations.\r\n\r\n        .. versionadded:: 1.3\r\n\r\n    verbose : bool, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        dictionary atoms extracted from the data\r\n\r\n    error_ : array\r\n        vector of errors at each iteration\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations run.\r\n\r\n    See Also\r\n    --------\r\n    MiniBatchDictionaryLearning: A faster, less accurate, version of the\r\n        dictionary learning algorithm.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import DictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> dict_learner = DictionaryLearning(\r\n    ...     n_components=15, transform_algorithm='lasso_lars', transform_alpha=0.1,\r\n    ...     random_state=42,\r\n    ... )\r\n    >>> X_transformed = dict_learner.fit(X).transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0)\r\n    0.41...\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.07...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"fit_algorithm\": [StrOptions({\"lars\", \"cd\"})],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"n_jobs\": [Integral, None],\r\n        \"code_init\": [np.ndarray, None],\r\n        \"dict_init\": [np.ndarray, None],\r\n        \"callback\": [callable, None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        max_iter=1000,\r\n        tol=1e-8,\r\n        fit_algorithm=\"lars\",\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        n_jobs=None,\r\n        code_init=None,\r\n        dict_init=None,\r\n        callback=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.max_iter = max_iter\r\n        self.tol = tol\r\n        self.fit_algorithm = fit_algorithm\r\n        self.code_init = code_init\r\n        self.dict_init = dict_init\r\n        self.callback = callback\r\n        self.verbose = verbose\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        self.fit_transform(X)\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit_transform(self, X, y=None):\r\n        \"\"\"Fit the model from data in X and return the transformed data.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        V : ndarray of shape (n_samples, n_components)\r\n            Transformed data.\r\n        \"\"\"\r\n        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n\r\n        method = \"lasso_\" + self.fit_algorithm\r\n\r\n        random_state = check_random_state(self.random_state)\r\n        X = self._validate_data(X)\r\n\r\n        if self.n_components is None:\r\n            n_components = X.shape[1]\r\n        else:\r\n            n_components = self.n_components\r\n\r\n        V, U, E, self.n_iter_ = _dict_learning(\r\n            X,\r\n            n_components,\r\n            alpha=self.alpha,\r\n            tol=self.tol,\r\n            max_iter=self.max_iter,\r\n            method=method,\r\n            method_max_iter=self.transform_max_iter,\r\n            n_jobs=self.n_jobs,\r\n            code_init=self.code_init,\r\n            dict_init=self.dict_init,\r\n            callback=self.callback,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            return_n_iter=True,\r\n            positive_dict=self.positive_dict,\r\n            positive_code=self.positive_code,\r\n        )\r\n        self.components_ = U\r\n        self.error_ = E\r\n\r\n        return V\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n\r\n\r\nclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n    \"\"\"Mini-batch dictionary learning.\r\n\r\n    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n    encoding the fitted data.\r\n\r\n    Solves the optimization problem::\r\n\r\n       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n                    (U,V)\r\n                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n\r\n    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n    the entry-wise matrix norm which is the sum of the absolute values\r\n    of all the entries in the matrix.\r\n\r\n    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n\r\n    Parameters\r\n    ----------\r\n    n_components : int, default=None\r\n        Number of dictionary elements to extract.\r\n\r\n    alpha : float, default=1\r\n        Sparsity controlling parameter.\r\n\r\n    n_iter : int, default=1000\r\n        Total number of iterations over data batches to perform.\r\n\r\n        .. deprecated:: 1.1\r\n           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n           ``max_iter`` instead.\r\n\r\n    max_iter : int, default=None\r\n        Maximum number of iterations over the complete dataset before\r\n        stopping independently of any early stopping criterion heuristics.\r\n        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n        The algorithm used:\r\n\r\n        - `'lars'`: uses the least angle regression method to solve the lasso\r\n          problem (`linear_model.lars_path`)\r\n        - `'cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n          the estimated components are sparse.\r\n\r\n    n_jobs : int, default=None\r\n        Number of parallel jobs to run.\r\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n        for more details.\r\n\r\n    batch_size : int, default=256\r\n        Number of samples in each mini-batch.\r\n\r\n        .. versionchanged:: 1.3\r\n           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n\r\n    shuffle : bool, default=True\r\n        Whether to shuffle the samples before forming batches.\r\n\r\n    dict_init : ndarray of shape (n_components, n_features), default=None\r\n        Initial value of the dictionary for warm restart scenarios.\r\n\r\n    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n            'threshold'}, default='omp'\r\n        Algorithm used to transform the data:\r\n\r\n        - `'lars'`: uses the least angle regression method\r\n          (`linear_model.lars_path`);\r\n        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n          if the estimated components are sparse.\r\n        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n          solution.\r\n        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n          the projection ``dictionary * X'``.\r\n\r\n    transform_n_nonzero_coefs : int, default=None\r\n        Number of nonzero coefficients to target in each column of the\r\n        solution. This is only used by `algorithm='lars'` and\r\n        `algorithm='omp'`. If `None`, then\r\n        `transform_n_nonzero_coefs=int(n_features / 10)`.\r\n\r\n    transform_alpha : float, default=None\r\n        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the\r\n        penalty applied to the L1 norm.\r\n        If `algorithm='threshold'`, `alpha` is the absolute value of the\r\n        threshold below which coefficients will be squashed to zero.\r\n        If `None`, defaults to `alpha`.\r\n\r\n        .. versionchanged:: 1.2\r\n            When None, default value changed from 1.0 to `alpha`.\r\n\r\n    verbose : bool or int, default=False\r\n        To control the verbosity of the procedure.\r\n\r\n    split_sign : bool, default=False\r\n        Whether to split the sparse feature vector into the concatenation of\r\n        its negative part and its positive part. This can improve the\r\n        performance of downstream classifiers.\r\n\r\n    random_state : int, RandomState instance or None, default=None\r\n        Used for initializing the dictionary when ``dict_init`` is not\r\n        specified, randomly shuffling the data when ``shuffle`` is set to\r\n        ``True``, and updating the dictionary. Pass an int for reproducible\r\n        results across multiple function calls.\r\n        See :term:`Glossary <random_state>`.\r\n\r\n    positive_code : bool, default=False\r\n        Whether to enforce positivity when finding the code.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    positive_dict : bool, default=False\r\n        Whether to enforce positivity when finding the dictionary.\r\n\r\n        .. versionadded:: 0.20\r\n\r\n    transform_max_iter : int, default=1000\r\n        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\r\n        `'lasso_lars'`.\r\n\r\n        .. versionadded:: 0.22\r\n\r\n    callback : callable, default=None\r\n        A callable that gets invoked at the end of each iteration.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    tol : float, default=1e-3\r\n        Control early stopping based on the norm of the differences in the\r\n        dictionary between 2 steps. Used only if `max_iter` is not None.\r\n\r\n        To disable early stopping based on changes in the dictionary, set\r\n        `tol` to 0.0.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    max_no_improvement : int, default=10\r\n        Control early stopping based on the consecutive number of mini batches\r\n        that does not yield an improvement on the smoothed cost function. Used only if\r\n        `max_iter` is not None.\r\n\r\n        To disable convergence detection based on cost function, set\r\n        `max_no_improvement` to None.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    Attributes\r\n    ----------\r\n    components_ : ndarray of shape (n_components, n_features)\r\n        Components extracted from the data.\r\n\r\n    n_features_in_ : int\r\n        Number of features seen during :term:`fit`.\r\n\r\n        .. versionadded:: 0.24\r\n\r\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\r\n        Names of features seen during :term:`fit`. Defined only when `X`\r\n        has feature names that are all strings.\r\n\r\n        .. versionadded:: 1.0\r\n\r\n    n_iter_ : int\r\n        Number of iterations over the full dataset.\r\n\r\n    n_steps_ : int\r\n        Number of mini-batches processed.\r\n\r\n        .. versionadded:: 1.1\r\n\r\n    See Also\r\n    --------\r\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\r\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\r\n    SparseCoder : Find a sparse representation of data from a fixed,\r\n        precomputed dictionary.\r\n    SparsePCA : Sparse Principal Components Analysis.\r\n\r\n    References\r\n    ----------\r\n\r\n    J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning\r\n    for sparse coding (https://www.di.ens.fr/sierra/pdfs/icml09.pdf)\r\n\r\n    Examples\r\n    --------\r\n    >>> import numpy as np\r\n    >>> from sklearn.datasets import make_sparse_coded_signal\r\n    >>> from sklearn.decomposition import MiniBatchDictionaryLearning\r\n    >>> X, dictionary, code = make_sparse_coded_signal(\r\n    ...     n_samples=100, n_components=15, n_features=20, n_nonzero_coefs=10,\r\n    ...     random_state=42)\r\n    >>> dict_learner = MiniBatchDictionaryLearning(\r\n    ...     n_components=15, batch_size=3, transform_algorithm='lasso_lars',\r\n    ...     transform_alpha=0.1, random_state=42)\r\n    >>> X_transformed = dict_learner.fit_transform(X)\r\n\r\n    We can check the level of sparsity of `X_transformed`:\r\n\r\n    >>> np.mean(X_transformed == 0) < 0.5\r\n    True\r\n\r\n    We can compare the average squared euclidean norm of the reconstruction\r\n    error of the sparse coded signal relative to the squared euclidean norm of\r\n    the original signal:\r\n\r\n    >>> X_hat = X_transformed @ dict_learner.components_\r\n    >>> np.mean(np.sum((X_hat - X) ** 2, axis=1) / np.sum(X ** 2, axis=1))\r\n    0.057...\r\n    \"\"\"\r\n\r\n    _parameter_constraints: dict = {\r\n        \"n_components\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"alpha\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"n_iter\": [\r\n            Interval(Integral, 0, None, closed=\"left\"),\r\n            Hidden(StrOptions({\"deprecated\"})),\r\n        ],\r\n        \"max_iter\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n        \"fit_algorithm\": [StrOptions({\"cd\", \"lars\"})],\r\n        \"n_jobs\": [None, Integral],\r\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\r\n        \"shuffle\": [\"boolean\"],\r\n        \"dict_init\": [None, np.ndarray],\r\n        \"transform_algorithm\": [\r\n            StrOptions({\"lasso_lars\", \"lasso_cd\", \"lars\", \"omp\", \"threshold\"})\r\n        ],\r\n        \"transform_n_nonzero_coefs\": [Interval(Integral, 1, None, closed=\"left\"), None],\r\n        \"transform_alpha\": [Interval(Real, 0, None, closed=\"left\"), None],\r\n        \"verbose\": [\"verbose\"],\r\n        \"split_sign\": [\"boolean\"],\r\n        \"random_state\": [\"random_state\"],\r\n        \"positive_code\": [\"boolean\"],\r\n        \"positive_dict\": [\"boolean\"],\r\n        \"transform_max_iter\": [Interval(Integral, 0, None, closed=\"left\")],\r\n        \"callback\": [None, callable],\r\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\r\n        \"max_no_improvement\": [Interval(Integral, 0, None, closed=\"left\"), None],\r\n    }\r\n\r\n    def __init__(\r\n        self,\r\n        n_components=None,\r\n        *,\r\n        alpha=1,\r\n        n_iter=\"deprecated\",\r\n        max_iter=None,\r\n        fit_algorithm=\"lars\",\r\n        n_jobs=None,\r\n        batch_size=256,\r\n        shuffle=True,\r\n        dict_init=None,\r\n        transform_algorithm=\"omp\",\r\n        transform_n_nonzero_coefs=None,\r\n        transform_alpha=None,\r\n        verbose=False,\r\n        split_sign=False,\r\n        random_state=None,\r\n        positive_code=False,\r\n        positive_dict=False,\r\n        transform_max_iter=1000,\r\n        callback=None,\r\n        tol=1e-3,\r\n        max_no_improvement=10,\r\n    ):\r\n        super().__init__(\r\n            transform_algorithm,\r\n            transform_n_nonzero_coefs,\r\n            transform_alpha,\r\n            split_sign,\r\n            n_jobs,\r\n            positive_code,\r\n            transform_max_iter,\r\n        )\r\n        self.n_components = n_components\r\n        self.alpha = alpha\r\n        self.n_iter = n_iter\r\n        self.max_iter = max_iter\r\n        self.fit_algorithm = fit_algorithm\r\n        self.dict_init = dict_init\r\n        self.verbose = verbose\r\n        self.shuffle = shuffle\r\n        self.batch_size = batch_size\r\n        self.split_sign = split_sign\r\n        self.random_state = random_state\r\n        self.positive_dict = positive_dict\r\n        self.callback = callback\r\n        self.max_no_improvement = max_no_improvement\r\n        self.tol = tol\r\n\r\n    def _check_params(self, X):\r\n        # n_components\r\n        self._n_components = self.n_components\r\n        if self._n_components is None:\r\n            self._n_components = X.shape[1]\r\n\r\n        # fit_algorithm\r\n        _check_positive_coding(self.fit_algorithm, self.positive_code)\r\n        self._fit_algorithm = \"lasso_\" + self.fit_algorithm\r\n\r\n        # batch_size\r\n        self._batch_size = min(self.batch_size, X.shape[0])\r\n\r\n    def _initialize_dict(self, X, random_state):\r\n        \"\"\"Initialization of the dictionary.\"\"\"\r\n        if self.dict_init is not None:\r\n            dictionary = self.dict_init\r\n        else:\r\n            # Init V with SVD of X\r\n            _, S, dictionary = randomized_svd(\r\n                X, self._n_components, random_state=random_state\r\n            )\r\n            dictionary = S[:, np.newaxis] * dictionary\r\n\r\n        if self._n_components <= len(dictionary):\r\n            dictionary = dictionary[: self._n_components, :]\r\n        else:\r\n            dictionary = np.concatenate(\r\n                (\r\n                    dictionary,\r\n                    np.zeros(\r\n                        (self._n_components - len(dictionary), dictionary.shape[1]),\r\n                        dtype=dictionary.dtype,\r\n                    ),\r\n                )\r\n            )\r\n\r\n"]
[1685.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1685.002, "i", "sed -n '2200,3400p' sklearn/decomposition/_dict_learning.py\r"]
[1685.004, "o", "sed -n '2200,3400p' sklearn/decomposition/_dict_learning.py\r\n"]
[1685.006, "o", "\u001b[?2004l\r\n\r\n        dictionary = check_array(dictionary, order=\"F\", dtype=X.dtype, copy=False)\r\n        dictionary = np.require(dictionary, requirements=\"W\")\r\n\r\n        return dictionary\r\n\r\n    def _update_inner_stats(self, X, code, batch_size, step):\r\n        \"\"\"Update the inner stats inplace.\"\"\"\r\n        if step < batch_size - 1:\r\n            theta = (step + 1) * batch_size\r\n        else:\r\n            theta = batch_size**2 + step + 1 - batch_size\r\n        beta = (theta + 1 - batch_size) / (theta + 1)\r\n\r\n        self._A *= beta\r\n        self._A += code.T @ code / batch_size\r\n        self._B *= beta\r\n        self._B += X.T @ code / batch_size\r\n\r\n    def _minibatch_step(self, X, dictionary, random_state, step):\r\n        \"\"\"Perform the update on the dictionary for one minibatch.\"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # Compute code for this batch\r\n        code = _sparse_encode(\r\n            X,\r\n            dictionary,\r\n            algorithm=self._fit_algorithm,\r\n            alpha=self.alpha,\r\n            n_jobs=self.n_jobs,\r\n            positive=self.positive_code,\r\n            max_iter=self.transform_max_iter,\r\n            verbose=self.verbose,\r\n        )\r\n\r\n        batch_cost = (\r\n            0.5 * ((X - code @ dictionary) ** 2).sum()\r\n            + self.alpha * np.sum(np.abs(code))\r\n        ) / batch_size\r\n\r\n        # Update inner stats\r\n        self._update_inner_stats(X, code, batch_size, step)\r\n\r\n        # Update dictionary\r\n        _update_dict(\r\n            dictionary,\r\n            X,\r\n            code,\r\n            self._A,\r\n            self._B,\r\n            verbose=self.verbose,\r\n            random_state=random_state,\r\n            positive=self.positive_dict,\r\n        )\r\n\r\n        return batch_cost\r\n\r\n    def _check_convergence(\r\n        self, X, batch_cost, new_dict, old_dict, n_samples, step, n_steps\r\n    ):\r\n        \"\"\"Helper function to encapsulate the early stopping logic.\r\n\r\n        Early stopping is based on two factors:\r\n        - A small change of the dictionary between two minibatch updates. This is\r\n          controlled by the tol parameter.\r\n        - No more improvement on a smoothed estimate of the objective function for a\r\n          a certain number of consecutive minibatch updates. This is controlled by\r\n          the max_no_improvement parameter.\r\n        \"\"\"\r\n        batch_size = X.shape[0]\r\n\r\n        # counts steps starting from 1 for user friendly verbose mode.\r\n        step = step + 1\r\n\r\n        # Ignore 100 first steps or 1 epoch to avoid initializing the ewa_cost with a\r\n        # too bad value\r\n        if step <= min(100, n_samples / batch_size):\r\n            if self.verbose:\r\n                print(f\"Minibatch step {step}/{n_steps}: mean batch cost: {batch_cost}\")\r\n            return False\r\n\r\n        # Compute an Exponentially Weighted Average of the cost function to\r\n        # monitor the convergence while discarding minibatch-local stochastic\r\n        # variability: https://en.wikipedia.org/wiki/Moving_average\r\n        if self._ewa_cost is None:\r\n            self._ewa_cost = batch_cost\r\n        else:\r\n            alpha = batch_size / (n_samples + 1)\r\n            alpha = min(alpha, 1)\r\n            self._ewa_cost = self._ewa_cost * (1 - alpha) + batch_cost * alpha\r\n\r\n        if self.verbose:\r\n            print(\r\n                f\"Minibatch step {step}/{n_steps}: mean batch cost: \"\r\n                f\"{batch_cost}, ewa cost: {self._ewa_cost}\"\r\n            )\r\n\r\n        # Early stopping based on change of dictionary\r\n        dict_diff = linalg.norm(new_dict - old_dict) / self._n_components\r\n        if self.tol > 0 and dict_diff <= self.tol:\r\n            if self.verbose:\r\n                print(f\"Converged (small dictionary change) at step {step}/{n_steps}\")\r\n            return True\r\n\r\n        # Early stopping heuristic due to lack of improvement on smoothed\r\n        # cost function\r\n        if self._ewa_cost_min is None or self._ewa_cost < self._ewa_cost_min:\r\n            self._no_improvement = 0\r\n            self._ewa_cost_min = self._ewa_cost\r\n        else:\r\n            self._no_improvement += 1\r\n\r\n        if (\r\n            self.max_no_improvement is not None\r\n            and self._no_improvement >= self.max_no_improvement\r\n        ):\r\n            if self.verbose:\r\n                print(\r\n                    \"Converged (lack of improvement in objective function) \"\r\n                    f\"at step {step}/{n_steps}\"\r\n                )\r\n            return True\r\n\r\n        return False\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def fit(self, X, y=None):\r\n        \"\"\"Fit the model from data in X.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Returns the instance itself.\r\n        \"\"\"\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", copy=False\r\n        )\r\n\r\n        self._check_params(X)\r\n\r\n        if self.n_iter != \"deprecated\":\r\n            warnings.warn(\r\n                (\r\n                    \"'n_iter' is deprecated in version 1.1 and will be removed \"\r\n                    \"in version 1.4. Use 'max_iter' and let 'n_iter' to its default \"\r\n                    \"value instead. 'n_iter' is also ignored if 'max_iter' is \"\r\n                    \"specified.\"\r\n                ),\r\n                FutureWarning,\r\n            )\r\n            n_iter = self.n_iter\r\n\r\n        self._random_state = check_random_state(self.random_state)\r\n\r\n        dictionary = self._initialize_dict(X, self._random_state)\r\n        old_dict = dictionary.copy()\r\n\r\n        if self.shuffle:\r\n            X_train = X.copy()\r\n            self._random_state.shuffle(X_train)\r\n        else:\r\n            X_train = X\r\n\r\n        n_samples, n_features = X_train.shape\r\n\r\n        if self.verbose:\r\n            print(\"[dict_learning]\")\r\n\r\n        # Inner stats\r\n        self._A = np.zeros(\r\n            (self._n_components, self._n_components), dtype=X_train.dtype\r\n        )\r\n        self._B = np.zeros((n_features, self._n_components), dtype=X_train.dtype)\r\n\r\n        if self.max_iter is not None:\r\n            # Attributes to monitor the convergence\r\n            self._ewa_cost = None\r\n            self._ewa_cost_min = None\r\n            self._no_improvement = 0\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n            n_steps_per_iter = int(np.ceil(n_samples / self._batch_size))\r\n            n_steps = self.max_iter * n_steps_per_iter\r\n\r\n            i = -1  # to allow max_iter = 0\r\n\r\n            for i, batch in zip(range(n_steps), batches):\r\n                X_batch = X_train[batch]\r\n\r\n                batch_cost = self._minibatch_step(\r\n                    X_batch, dictionary, self._random_state, i\r\n                )\r\n\r\n                if self._check_convergence(\r\n                    X_batch, batch_cost, dictionary, old_dict, n_samples, i, n_steps\r\n                ):\r\n                    break\r\n\r\n                # XXX callback param added for backward compat in #18975 but a common\r\n                # unified callback API should be preferred\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n                old_dict[:] = dictionary\r\n\r\n            self.n_steps_ = i + 1\r\n            self.n_iter_ = np.ceil(self.n_steps_ / n_steps_per_iter)\r\n        else:\r\n            # TODO remove this branch in 1.4\r\n            n_iter = 1000 if self.n_iter == \"deprecated\" else self.n_iter\r\n\r\n            batches = gen_batches(n_samples, self._batch_size)\r\n            batches = itertools.cycle(batches)\r\n\r\n            for i, batch in zip(range(n_iter), batches):\r\n                self._minibatch_step(X_train[batch], dictionary, self._random_state, i)\r\n\r\n                trigger_verbose = self.verbose and i % ceil(100.0 / self.verbose) == 0\r\n                if self.verbose > 10 or trigger_verbose:\r\n                    print(f\"{i} batches processed.\")\r\n\r\n                if self.callback is not None:\r\n                    self.callback(locals())\r\n\r\n            self.n_steps_ = n_iter\r\n            self.n_iter_ = np.ceil(n_iter / int(np.ceil(n_samples / self._batch_size)))\r\n\r\n        self.components_ = dictionary\r\n\r\n        return self\r\n\r\n    @_fit_context(prefer_skip_nested_validation=True)\r\n    def partial_fit(self, X, y=None):\r\n        \"\"\"Update the model using the data in X as a mini-batch.\r\n\r\n        Parameters\r\n        ----------\r\n        X : array-like of shape (n_samples, n_features)\r\n            Training vector, where `n_samples` is the number of samples\r\n            and `n_features` is the number of features.\r\n\r\n        y : Ignored\r\n            Not used, present for API consistency by convention.\r\n\r\n        Returns\r\n        -------\r\n        self : object\r\n            Return the instance itself.\r\n        \"\"\"\r\n        has_components = hasattr(self, \"components_\")\r\n\r\n        X = self._validate_data(\r\n            X, dtype=[np.float64, np.float32], order=\"C\", reset=not has_components\r\n        )\r\n\r\n        if not has_components:\r\n            # This instance has not been fitted yet (fit or partial_fit)\r\n            self._check_params(X)\r\n            self._random_state = check_random_state(self.random_state)\r\n\r\n            dictionary = self._initialize_dict(X, self._random_state)\r\n\r\n            self.n_steps_ = 0\r\n\r\n            self._A = np.zeros((self._n_components, self._n_components), dtype=X.dtype)\r\n            self._B = np.zeros((X.shape[1], self._n_components), dtype=X.dtype)\r\n        else:\r\n            dictionary = self.components_\r\n\r\n        self._minibatch_step(X, dictionary, self._random_state, self.n_steps_)\r\n\r\n        self.components_ = dictionary\r\n        self.n_steps_ += 1\r\n\r\n        return self\r\n\r\n    @property\r\n    def _n_features_out(self):\r\n        \"\"\"Number of transformed output features.\"\"\"\r\n        return self.components_.shape[0]\r\n\r\n    def _more_tags(self):\r\n        return {\r\n            \"preserves_dtype\": [np.float64, np.float32],\r\n        }\r\n"]
[1690.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1690.002, "i", "grep -n \"old_dict\" sklearn/decomposition/_dict_learning.py\r"]
[1690.004, "o", "grep -n \"old_dict\" sklearn/decomposition/_dict_learning.py\r\n"]
[1690.006, "o", "\u001b[?2004l\r\n\u001b[32m\u001b[K2258\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        self, X, batch_cost, new_dict, \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K, n_samples, step, n_steps\r\n\u001b[32m\u001b[K2298\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        dict_diff = linalg.norm(new_dict - \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K) / self._n_components\r\n\u001b[32m\u001b[K2364\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K        \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K = dictionary.copy()\r\n\u001b[32m\u001b[K2404\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                    X_batch, batch_cost, dictionary, \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K, n_samples, i, n_steps\r\n\u001b[32m\u001b[K2413\u001b[m\u001b[K\u001b[36m\u001b[K:\u001b[m\u001b[K                \u001b[01;31m\u001b[Kold_dict\u001b[m\u001b[K[:] = dictionary\r\n"]
[1695.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1695.002, "i", "nl -ba sklearn/decomposition/_dict_learning.py | sed -n '1,2600p' | sed -n '1750,1950p'\r"]
[1695.004, "o", "nl -ba sklearn/decomposition/_dict_learning.py | sed -n '1,2600p' | sed -n '1750,1950p'\r\n"]
[1695.006, "o", "\u001b[?2004l\r\n  1750\t        transform_alpha=None,\r\n  1751\t        n_jobs=None,\r\n  1752\t        code_init=None,\r\n  1753\t        dict_init=None,\r\n  1754\t        callback=None,\r\n  1755\t        verbose=False,\r\n  1756\t        split_sign=False,\r\n  1757\t        random_state=None,\r\n  1758\t        positive_code=False,\r\n  1759\t        positive_dict=False,\r\n  1760\t        transform_max_iter=1000,\r\n  1761\t    ):\r\n  1762\t        super().__init__(\r\n  1763\t            transform_algorithm,\r\n  1764\t            transform_n_nonzero_coefs,\r\n  1765\t            transform_alpha,\r\n  1766\t            split_sign,\r\n  1767\t            n_jobs,\r\n  1768\t            positive_code,\r\n  1769\t            transform_max_iter,\r\n  1770\t        )\r\n  1771\t        self.n_components = n_components\r\n  1772\t        self.alpha = alpha\r\n  1773\t        self.max_iter = max_iter\r\n  1774\t        self.tol = tol\r\n  1775\t        self.fit_algorithm = fit_algorithm\r\n  1776\t        self.code_init = code_init\r\n  1777\t        self.dict_init = dict_init\r\n  1778\t        self.callback = callback\r\n  1779\t        self.verbose = verbose\r\n  1780\t        self.random_state = random_state\r\n  1781\t        self.positive_dict = positive_dict\r\n  1782\t\r\n  1783\t    def fit(self, X, y=None):\r\n  1784\t        \"\"\"Fit the model from data in X.\r\n  1785\t\r\n  1786\t        Parameters\r\n  1787\t        ----------\r\n  1788\t        X : array-like of shape (n_samples, n_features)\r\n  1789\t            Training vector, where `n_samples` is the number of samples\r\n  1790\t            and `n_features` is the number of features.\r\n  1791\t\r\n  1792\t        y : Ignored\r\n  1793\t            Not used, present for API consistency by convention.\r\n  1794\t\r\n  1795\t        Returns\r\n  1796\t        -------\r\n  1797\t        self : object\r\n  1798\t            Returns the instance itself.\r\n  1799\t        \"\"\"\r\n  1800\t        self.fit_transform(X)\r\n  1801\t        return self\r\n  1802\t\r\n  1803\t    @_fit_context(prefer_skip_nested_validation=True)\r\n  1804\t    def fit_transform(self, X, y=None):\r\n  1805\t        \"\"\"Fit the model from data in X and return the transformed data.\r\n  1806\t\r\n  1807\t        Parameters\r\n  1808\t        ----------\r\n  1809\t        X : array-like of shape (n_samples, n_features)\r\n  1810\t            Training vector, where `n_samples` is the number of samples\r\n  1811\t            and `n_features` is the number of features.\r\n  1812\t\r\n  1813\t        y : Ignored\r\n  1814\t            Not used, present for API consistency by convention.\r\n  1815\t\r\n  1816\t        Returns\r\n  1817\t        -------\r\n  1818\t        V : ndarray of shape (n_samples, n_components)\r\n  1819\t            Transformed data.\r\n  1820\t        \"\"\"\r\n  1821\t        _check_positive_coding(method=self.fit_algorithm, positive=self.positive_code)\r\n  1822\t\r\n  1823\t        method = \"lasso_\" + self.fit_algorithm\r\n  1824\t\r\n  1825\t        random_state = check_random_state(self.random_state)\r\n  1826\t        X = self._validate_data(X)\r\n  1827\t\r\n  1828\t        if self.n_components is None:\r\n  1829\t            n_components = X.shape[1]\r\n  1830\t        else:\r\n  1831\t            n_components = self.n_components\r\n  1832\t\r\n  1833\t        V, U, E, self.n_iter_ = _dict_learning(\r\n  1834\t            X,\r\n  1835\t            n_components,\r\n  1836\t            alpha=self.alpha,\r\n  1837\t            tol=self.tol,\r\n  1838\t            max_iter=self.max_iter,\r\n  1839\t            method=method,\r\n  1840\t            method_max_iter=self.transform_max_iter,\r\n  1841\t            n_jobs=self.n_jobs,\r\n  1842\t            code_init=self.code_init,\r\n  1843\t            dict_init=self.dict_init,\r\n  1844\t            callback=self.callback,\r\n  1845\t            verbose=self.verbose,\r\n  1846\t            random_state=random_state,\r\n  1847\t            return_n_iter=True,\r\n  1848\t            positive_dict=self.positive_dict,\r\n  1849\t            positive_code=self.positive_code,\r\n  1850\t        )\r\n  1851\t        self.components_ = U\r\n  1852\t        self.error_ = E\r\n  1853\t\r\n  1854\t        return V\r\n  1855\t\r\n  1856\t    @property\r\n  1857\t    def _n_features_out(self):\r\n  1858\t        \"\"\"Number of transformed output features.\"\"\"\r\n  1859\t        return self.components_.shape[0]\r\n  1860\t\r\n  1861\t    def _more_tags(self):\r\n  1862\t        return {\r\n  1863\t            \"preserves_dtype\": [np.float64, np.float32],\r\n  1864\t        }\r\n  1865\t\r\n  1866\t\r\n  1867\tclass MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n  1868\t    \"\"\"Mini-batch dictionary learning.\r\n  1869\t\r\n  1870\t    Finds a dictionary (a set of atoms) that performs well at sparsely\r\n  1871\t    encoding the fitted data.\r\n  1872\t\r\n  1873\t    Solves the optimization problem::\r\n  1874\t\r\n  1875\t       (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1\r\n  1876\t                    (U,V)\r\n  1877\t                    with || V_k ||_2 <= 1 for all  0 <= k < n_components\r\n  1878\t\r\n  1879\t    ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for\r\n  1880\t    the entry-wise matrix norm which is the sum of the absolute values\r\n  1881\t    of all the entries in the matrix.\r\n  1882\t\r\n  1883\t    Read more in the :ref:`User Guide <DictionaryLearning>`.\r\n  1884\t\r\n  1885\t    Parameters\r\n  1886\t    ----------\r\n  1887\t    n_components : int, default=None\r\n  1888\t        Number of dictionary elements to extract.\r\n  1889\t\r\n  1890\t    alpha : float, default=1\r\n  1891\t        Sparsity controlling parameter.\r\n  1892\t\r\n  1893\t    n_iter : int, default=1000\r\n  1894\t        Total number of iterations over data batches to perform.\r\n  1895\t\r\n  1896\t        .. deprecated:: 1.1\r\n  1897\t           ``n_iter`` is deprecated in 1.1 and will be removed in 1.4. Use\r\n  1898\t           ``max_iter`` instead.\r\n  1899\t\r\n  1900\t    max_iter : int, default=None\r\n  1901\t        Maximum number of iterations over the complete dataset before\r\n  1902\t        stopping independently of any early stopping criterion heuristics.\r\n  1903\t        If ``max_iter`` is not None, ``n_iter`` is ignored.\r\n  1904\t\r\n  1905\t        .. versionadded:: 1.1\r\n  1906\t\r\n  1907\t    fit_algorithm : {'lars', 'cd'}, default='lars'\r\n  1908\t        The algorithm used:\r\n  1909\t\r\n  1910\t        - `'lars'`: uses the least angle regression method to solve the lasso\r\n  1911\t          problem (`linear_model.lars_path`)\r\n  1912\t        - `'cd'`: uses the coordinate descent method to compute the\r\n  1913\t          Lasso solution (`linear_model.Lasso`). Lars will be faster if\r\n  1914\t          the estimated components are sparse.\r\n  1915\t\r\n  1916\t    n_jobs : int, default=None\r\n  1917\t        Number of parallel jobs to run.\r\n  1918\t        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\r\n  1919\t        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\r\n  1920\t        for more details.\r\n  1921\t\r\n  1922\t    batch_size : int, default=256\r\n  1923\t        Number of samples in each mini-batch.\r\n  1924\t\r\n  1925\t        .. versionchanged:: 1.3\r\n  1926\t           The default value of `batch_size` changed from 3 to 256 in version 1.3.\r\n  1927\t\r\n  1928\t    shuffle : bool, default=True\r\n  1929\t        Whether to shuffle the samples before forming batches.\r\n  1930\t\r\n  1931\t    dict_init : ndarray of shape (n_components, n_features), default=None\r\n  1932\t        Initial value of the dictionary for warm restart scenarios.\r\n  1933\t\r\n  1934\t    transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp', \\\r\n  1935\t            'threshold'}, default='omp'\r\n  1936\t        Algorithm used to transform the data:\r\n  1937\t\r\n  1938\t        - `'lars'`: uses the least angle regression method\r\n  1939\t          (`linear_model.lars_path`);\r\n  1940\t        - `'lasso_lars'`: uses Lars to compute the Lasso solution.\r\n  1941\t        - `'lasso_cd'`: uses the coordinate descent method to compute the\r\n  1942\t          Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster\r\n  1943\t          if the estimated components are sparse.\r\n  1944\t        - `'omp'`: uses orthogonal matching pursuit to estimate the sparse\r\n  1945\t          solution.\r\n  1946\t        - `'threshold'`: squashes to zero all coefficients less than alpha from\r\n  1947\t          the projection ``dictionary * X'``.\r\n  1948\t\r\n  1949\t    transform_n_nonzero_coefs : int, default=None\r\n  1950\t        Number of nonzero coefficients to target in each column of the\r\n"]
[1700.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1700.002, "i", "python - << 'PY'\r"]
[1700.004, "o", "python - << 'PY'\r\n"]
[1700.006, "o", "\u001b[?2004l\r\n\u001b[?2004h> from pathlib import Path\r\n\u001b[?2004l\r\n\u001b[?2004h> p=Path('sklearn/decomposition/_dict_learning.py')\r\n\u001b[?2004l\r\n\u001b[?2004h> s=p.read_text()\r\n\u001b[?2004l\r\n\u001b[?2004h> # Move unconditional old_dict copy into the max_iter branch to avoid unnecessary work when max_iter is None\r\n\u001b[?2004l\r\n\u001b[?2004h> old_line = '        old_dict = dictionary.copy()\\n'\r\n\u001b[?2004l\r\n\u001b[?2004h> if old_line in s:\r\n\u001b[?2004l\r\n\u001b[?2004h>     s = s.replace(old_line, '')\r\n\u001b[?2004l\r\n\u001b[?2004h>     # Insert a guarded copy inside the max_iter branch after setting monitoring attrs\r\n\u001b[?2004l\r\n\u001b[?2004h>     anchor = '            self._no_improvement = 0\\n\\n            batches = gen_batches'\r\n\u001b[?2004l\r\n\u001b[?2004h>     insert = '            old_dict = dictionary.copy()\\n\\n            batches = gen_batches'\r\n\u001b[?2004l\r\n\u001b[?2004h>     if anchor in s:\r\n\u001b[?2004l\r\n\u001b[?2004h>         s = s.replace(anchor, insert)\r\n\u001b[?2004l\r\n\u001b[?2004h>         p.write_text(s)\r\n\u001b[?2004l\r\n\u001b[?2004h>         print('Patched: moved old_dict copy inside max_iter branch')\r\n\u001b[?2004l\r\n\u001b[?2004h>     else:\r\n\u001b[?2004l\r\n\u001b[?2004h>         print('Anchor not found; no patch applied')\r\n\u001b[?2004l\r\n\u001b[?2004h> else:\r\n\u001b[?2004l\r\n\u001b[?2004h>     print('old_dict copy line not found or previously patched')\r\n\u001b[?2004l\r\n\u001b[?2004h> PY\r\n\u001b[?2004l\r\nPatched: moved old_dict copy inside max_iter branch\r\n"]
[1705.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1705.002, "i", "git --no-pager diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r"]
[1705.004, "o", "git --no-pager diff -- sklearn/decomposition/_dict_learning.py | sed -n '1,200p'\r\n"]
[1705.006, "o", "\u001b[?2004l\r\ndiff --git a/sklearn/decomposition/_dict_learning.py b/sklearn/decomposition/_dict_learning.py\r\nindex 48beac450..ddfda0392 100644\r\n--- a/sklearn/decomposition/_dict_learning.py\r\n+++ b/sklearn/decomposition/_dict_learning.py\r\n@@ -2361,7 +2361,6 @@ class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n         self._random_state = check_random_state(self.random_state)\r\n \r\n         dictionary = self._initialize_dict(X, self._random_state)\r\n-        old_dict = dictionary.copy()\r\n \r\n         if self.shuffle:\r\n             X_train = X.copy()\r\n@@ -2384,7 +2383,7 @@ class MiniBatchDictionaryLearning(_BaseSparseCoding, BaseEstimator):\r\n             # Attributes to monitor the convergence\r\n             self._ewa_cost = None\r\n             self._ewa_cost_min = None\r\n-            self._no_improvement = 0\r\n+            old_dict = dictionary.copy()\r\n \r\n             batches = gen_batches(n_samples, self._batch_size)\r\n             batches = itertools.cycle(batches)\r\n"]
[1710.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1710.002, "i", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r"]
[1710.004, "o", "pytest -q sklearn/decomposition/tests/test_dict_learning.py -k minibatch -q\r\n"]
[1710.006, "o", "\u001b[?2004l\r\n\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                                [100%]\u001b[0m\r\n"]
[1715.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1715.002, "i", "cd asv_benchmarks\r"]
[1715.004, "o", "cd asv_benchmarks\r\n"]
[1715.006, "o", "\u001b[?2004l\r\n"]
[1720.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1720.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1720.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1720.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0m                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.70\u00b10.02s \r\n                     cd        1.55\u00b10.04s \r\n              =============== ============\r\n\r\n"]
[1725.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1725.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1725.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1725.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[1730.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1730.002, "i", "cd ..\r"]
[1730.004, "o", "cd ..\r\n"]
[1730.006, "o", "\u001b[?2004l\r\n"]
[1735.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1735.002, "i", "git add sklearn/decomposition/_dict_learning.py\r"]
[1735.004, "o", "git add sklearn/decomposition/_dict_learning.py\r\n"]
[1735.006, "o", "\u001b[?2004l\r\n"]
[1740.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1740.002, "i", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None. Move o\r"]
[1740.004, "o", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None. Move o\r\n"]
[1740.006, "o", "old_dict copy inside max_iter branch to reduce allocations in common ASV path. All minibatch dict learning tests pass.\"\r\n\u001b[?2004l\r\nAuthor identity unknown\r\n\r\n*** Please tell me who you are.\r\n\r\nRun\r\n\r\n  git config --global user.email \"you@example.com\"\r\n  git config --global user.name \"Your Name\"\r\n\r\nto set your account's default identity.\r\nOmit --global to set the identity only in this repository.\r\n\r\nfatal: unable to auto-detect email address (got 'root@53ee3e831d17.(none)')\r\n"]
[1745.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1745.002, "i", "cd asv_benchmarks\r"]
[1745.004, "o", "cd asv_benchmarks\r\n"]
[1745.006, "o", "\u001b[?2004l\r\n"]
[1750.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1750.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1750.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1750.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.80\u00b10.02s \r\n                     cd        1.59\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1755.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1755.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1755.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1755.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[1760.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1760.002, "i", "cd /workspace/repo\r"]
[1760.004, "o", "cd /workspace/repo\r\n"]
[1760.006, "o", "\u001b[?2004l\r\n"]
[1765.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1765.002, "i", "git config user.email \"devnull@example.com\"\r"]
[1765.004, "o", "git config user.email \"devnull@example.com\"\r\n"]
[1765.006, "o", "\u001b[?2004l\r\n"]
[1770.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1770.002, "i", "git config user.name \"ASV Optimizer\"\r"]
[1770.004, "o", "git config user.name \"ASV Optimizer\"\r\n"]
[1770.006, "o", "\u001b[?2004l\r\n"]
[1775.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1775.002, "i", "git add sklearn/decomposition/_dict_learning.py\r"]
[1775.004, "o", "git add sklearn/decomposition/_dict_learning.py\r\n"]
[1775.006, "o", "\u001b[?2004l\r\n"]
[1780.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1780.002, "i", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by movi\r"]
[1780.004, "o", "git commit -m \"Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by movi\r\n"]
[1780.006, "o", "ing old_dict copy inside the max_iter branch. Reduces allocations in common path; tests pass.\"\r\n\u001b[?2004l\r\n[main 8250665ac] Perf: avoid unnecessary dictionary copy in MiniBatchDictionaryLearning when max_iter is None by moving old_dict copy inside the max_iter branch. Reduces allocations in common path; tests pass.\r\n 1 file changed, 1 insertion(+), 2 deletions(-)\r\n"]
[1785.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1785.002, "i", "cd asv_benchmarks\r"]
[1785.004, "o", "cd asv_benchmarks\r\n"]
[1785.006, "o", "\u001b[?2004l\r\n"]
[1790.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1790.002, "i", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r"]
[1790.004, "o", "asv run --python=same --bench=\"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\" --show-s\r\n"]
[1790.006, "o", "stderr\r\n\u001b[?2004l\r\n\u00b7 \u001b[0;32mDiscovering benchmarks\u001b[0m\r\n\u00b7 \u001b[0;32mRunning 1 total benchmarks (1 commits * 1 environments * 1 benchmarks)\u001b[0m\r\n[ 0.00%] \u00b7\u00b7 \u001b[0;34mBenchmarking existing-py_opt_conda_envs_asv_3.10_bin_python3.10\u001b[0m\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mSetting up decomposition:75\u001b[0m                                                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39mdecomposition.MiniBatchDictionaryLearningBenchmark.time_fit\u001b[0masv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n                                                                                     ok\r\n[100.00%] \u00b7\u00b7\u00b7 \u001b[0;39m=============== ============\u001b[0m\r\n              --                 n_jobs   \r\n              --------------- ------------\r\n               fit_algorithm       1      \r\n              =============== ============\r\n                    lars       6.72\u00b10.03s \r\n                     cd        1.58\u00b10.02s \r\n              =============== ============\r\n\r\n"]
[1795.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1795.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1795.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1795.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[1800.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1800.002, "i", "grep -n \"self\\._A\\|self\\._B\" sklearn/decomposition/_dict_learning.py\r"]
[1800.004, "o", "grep -n \"self\\._A\\|self\\._B\" sklearn/decomposition/_dict_learning.py\r\n"]
[1800.006, "o", "\u001b[?2004l\r\ngrep: sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1805.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1805.002, "i", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '2200,2600p'\r"]
[1805.004, "o", "sed -n '1,2600p' sklearn/decomposition/_dict_learning.py | nl -ba | sed -n '2200,2600p'\r\n"]
[1805.006, "o", "\u001b[?2004l\r\nsed: can't read sklearn/decomposition/_dict_learning.py: No such file or directory\r\n"]
[1810.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1810.002, "i", "cd /workspace/repo\r"]
[1810.004, "o", "cd /workspace/repo\r\n"]
[1810.006, "o", "\u001b[?2004l\r\n"]
[1815.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1815.002, "i", "git show --stat -1\r"]
[1815.004, "o", "git show --stat -1\r\n"]
[1815.006, "o", "\u001b[?2004l\r\nerror: cannot run less: No such file or directory\r\nfatal: unable to execute pager 'less'\r\n"]
[1820.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo# "]
[1820.002, "i", "cd asv_benchmarks\r"]
[1820.004, "o", "cd asv_benchmarks\r\n"]
[1820.006, "o", "\u001b[?2004l\r\n"]
[1825.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1825.002, "i", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r"]
[1825.004, "o", "asv show --python=same --bench \"decomposition.MiniBatchDictionaryLearningBenchmark.time_fit\"\r\n"]
[1825.006, "o", "\u001b[?2004l\r\nCommits with results:\r\n\r\n"]
[1830.0, "o", "(asv_3.10) root@53ee3e831d17:/workspace/repo/asv_benchmarks# "]
[1830.002, "i", "\u001b[?2004l\r"]
[1830.004, "o", "\u001b[?2004l\r\n"]
[1830.006, "o", "\r\nlogout\r\n"]
